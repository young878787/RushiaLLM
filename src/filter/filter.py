"""
å›æ‡‰å¾Œè™•ç†éæ¿¾å™¨æ¨¡çµ„
è² è²¬æ¸…ç†å’Œå„ªåŒ– LLM ç”Ÿæˆçš„å›æ‡‰å…§å®¹
"""

import re
import logging
import os
from typing import List, Optional
from collections import Counter
from pathlib import Path
from datetime import datetime

# OpenCC ç°¡ç¹è½‰æ›
try:
    import opencc
    OPENCC_AVAILABLE = True
except ImportError:
    OPENCC_AVAILABLE = False
    logging.warning("OpenCC æœªå®‰è£ï¼Œç°¡ç¹è½‰æ›åŠŸèƒ½å°‡è¢«ç¦ç”¨ã€‚è«‹é‹è¡Œ: pip install opencc-python-reimplemented")


class ResponseFilter:
    """å›æ‡‰éæ¿¾å™¨é¡"""
    
    def __init__(self, config: dict):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # éæ¿¾è¨­ç½®
        self.filter_config = config.get('vtuber', {}).get('response', {})
        self.max_sentence_repeat = self.filter_config.get('max_sentence_repeat', 1)
        self.filter_repetition = self.filter_config.get('filter_repetition', True)
        self.filter_incomplete = self.filter_config.get('filter_incomplete', True)
        
        # ç°¡ç¹è½‰æ›è¨­ç½®
        self.enable_traditional_chinese = self.filter_config.get('enable_traditional_chinese', True)
        self.opencc_config = self.filter_config.get('opencc_config', 's2twp.json')
        self.opencc_converter = None
        self._setup_opencc_converter()
        
        # VTuber è§’è‰²åç¨±
        self.character_name = 'éœ²è¥¿å©­'  # é»˜èªè§’è‰²åç¨±ï¼Œå°‡ç”± LLM ç®¡ç†å™¨å‹•æ…‹è¨­ç½®
        
        # è¨­ç½®èª¿è©¦æ—¥èªŒ
        self._setup_debug_logging(config)
        
        # åœæ­¢è©å½™å’Œç‰¹æ®Šæ¨™è¨˜
        self.stop_patterns = [
            f"{self.character_name}ï¼š",
            f"{self.character_name}:",
            "ç”¨æˆ¶ï¼š",
            "ç”¨æˆ¶:",
            "Human:",
            "Assistant:",
            "åƒè€ƒè³‡è¨Šï¼š",
            "åƒè€ƒè³‡è¨Š:",
        ]
        
        # éœ€è¦å®Œå…¨ç§»é™¤çš„ç‰¹æ®Šæ¨™è¨˜
        self.remove_tokens = [
            "<|endoftext|>",
            "<|system|>",
            "<|user|>", 
            "<|assistant|>",
            "<|context|>",
            "<|end_context|>",
            # Qwen3 å°ˆç”¨æ¨™è¨˜
            "<|startofcontext|>",
            "<|session|>",
            "<|modeloutput|>",
            "<|m|>",
            "<|thinking|>",
            "<|startofmidnight|>",
            "<|beginofresponse|>",
            "<|endpoint|>",
            "<|startofthinking|>",
            "<|end|",
        ]
        
        # è¼¸å‡ºé‚Šç•Œæ¨™è¨˜ï¼ˆåªä¿ç•™æ­¤æ¨™è¨˜å…§çš„å…§å®¹ï¼‰
        self.boundary_token = "<|end|>"
        
        # å¥å­çµæŸæ¨™è¨˜
        self.sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', '~', 'â™ª', 'â™¡']
        
        # å…è²¬è²æ˜æ¨¡å¼
        self.disclaimer_patterns = [
            # å®Œæ•´çš„å…è²¬è²æ˜
            r"ï¼ˆæ³¨ï¼šä»¥ä¸Šç‚ºè§’è‰²æƒ…æ™¯æ¼”ç¹¹ï¼Œä¸¦ç„¡å¯¦éš›æ„ç¾©ï¼‰",
            r"ï¼ˆæ³¨ï¼šæ­¤ç‚ºè§’è‰²æ‰®æ¼”æƒ…æ™¯åŠ‡æƒ…ç™¼å±•ï¼Œéç¾å¯¦äº‹ä»¶ï¼‰",
            r"\(æ³¨ï¼šé€™è£¡å±•ç¾äº†éœ²è¥¿å©­ä½œç‚ºæ­»éˆé­”æ³•ä½¿çš„æ—¥å¸¸ï¼ŒåŒæ™‚ä¹Ÿé€éœ²å‡ºå¥¹å–œæ­¡èˆ‡ä»–äººç›¸è™•çš„æ€§æ ¼ç‰¹é»\ï¼‰",
            
            # è®Šé«”æ¨¡å¼
            r"\ï¼ˆæ³¨ï¼š.*?æ¼”ç¹¹.*?å¯¦éš›æ„ç¾©.*?\ï¼‰",
            r"\ï¼ˆæ³¨ï¼š.*?æ‰®æ¼”.*?éç¾å¯¦.*?\ï¼‰",
            r"\(æ³¨ï¼š.*?æ¼”ç¹¹.*?å¯¦éš›æ„ç¾©.*?\)",
            r"\(æ³¨ï¼š.*?æ‰®æ¼”.*?éç¾å¯¦.*?\)",
            
            # å…¶ä»–å¯èƒ½çš„å…è²¬è²æ˜
            r"æ³¨ï¼š.*?ç‚ºè™›æ§‹.*?å…§å®¹",
            r"è²æ˜ï¼š.*?è§’è‰²æ‰®æ¼”.*?",
            r"æé†’ï¼š.*?åƒ…ç‚º.*?å¨›æ¨‚",
            r"ã€.*?å…è²¬.*?è²æ˜.*?ã€‘",
            r"\*.*?è§’è‰²æ‰®æ¼”.*?æƒ…ç¯€.*?\*",
            
            # æ¸…ç†æœ«å°¾çš„æ³¨é‡‹
            r"\n\s*æ³¨ï¼š.*?$",
            r"\n\s*ï¼ˆæ³¨ï¼š.*?ï¼‰\s*$",
            r"\n\s*\(æ³¨ï¼š.*?\)\s*$",
            
            # é¡å¤–çš„è§’è‰²æ‰®æ¼”æç¤º
            r"ä»¥ä¸Š.*?ç‚ºè§’è‰²æ‰®æ¼”.*?",
            r"æ­¤.*?ç‚ºè™›æ§‹.*?å°è©±",
            r"åƒ…ç‚º.*?å¨›æ¨‚.*?ç›®çš„",
            r"éçœŸå¯¦.*?äº‹ä»¶",
        ]
    
    def _setup_debug_logging(self, config: dict):
        """è¨­ç½®èª¿è©¦æ—¥èªŒ"""
        try:
            # ğŸ”¥ çµ±ä¸€ï¼šç²å–æ—¥èªŒç›®éŒ„ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„çµ±ä¸€è·¯å¾‘ï¼‰
            log_dir = Path(config.get('system', {}).get('log_dir', './scriptV2/LLM/logs'))
            log_dir.mkdir(parents=True, exist_ok=True)
            
            # èª¿è©¦æ—¥èªŒæ–‡ä»¶è·¯å¾‘
            self.debug_log_path = log_dir / 'response_raw.log'
            
            # æ¯æ¬¡å•Ÿå‹•æ™‚æ¸…é™¤èˆŠæ—¥èªŒ
            if self.debug_log_path.exists():
                self.debug_log_path.unlink()
            
            self.logger.info(f"èª¿è©¦æ—¥èªŒå·²è¨­ç½®: {self.debug_log_path}")
            
        except Exception as e:
            self.logger.error(f"è¨­ç½®èª¿è©¦æ—¥èªŒå¤±æ•—: {e}")
            self.debug_log_path = None
    
    def _setup_opencc_converter(self):
        """è¨­ç½® OpenCC ç°¡ç¹è½‰æ›å™¨"""
        if not OPENCC_AVAILABLE or not self.enable_traditional_chinese:
            self.logger.info("OpenCC ç°¡ç¹è½‰æ›åŠŸèƒ½å·²ç¦ç”¨")
            return
        
        try:
            # åˆå§‹åŒ–ç°¡é«”è½‰ç¹é«”è½‰æ›å™¨ï¼Œä½¿ç”¨æŒ‡å®šçš„é…ç½®æ–‡ä»¶
            self.opencc_converter = opencc.OpenCC(self.opencc_config)
            self.logger.info(f"âœ… OpenCC ç°¡ç¹è½‰æ›å™¨åˆå§‹åŒ–æˆåŠŸï¼Œé…ç½®: {self.opencc_config}")
            
            # æ¸¬è©¦è½‰æ›åŠŸèƒ½
            test_text = "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•"
            converted = self.opencc_converter.convert(test_text)
            self.logger.info(f"ğŸ”„ è½‰æ›æ¸¬è©¦: '{test_text}' â†’ '{converted}'")
            
        except Exception as e:
            self.logger.error(f"âŒ OpenCC åˆå§‹åŒ–å¤±æ•—: {e}")
            self.opencc_converter = None
            self.enable_traditional_chinese = False
    
    def _write_debug_log(self, stage: str, content: str):
        """å¯«å…¥èª¿è©¦æ—¥èªŒ"""
        if not self.debug_log_path:
            return
        
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            log_entry = f"\n[{timestamp}] {stage}:\n{content}\n{'-'*50}\n"
            
            with open(self.debug_log_path, 'a', encoding='utf-8') as f:
                f.write(log_entry)
                
        except Exception as e:
            self.logger.error(f"å¯«å…¥èª¿è©¦æ—¥èªŒå¤±æ•—: {e}")
    
    def _remove_disclaimers(self, text: str) -> str:
        """ç§»é™¤æ¨¡å‹è‡ªå‹•æ·»åŠ çš„å…è²¬è²æ˜"""
        if not text:
            return text
            
        original_text = text
        cleaned_text = text
        
        # é€ä¸€ç§»é™¤åŒ¹é…çš„å…è²¬è²æ˜æ¨¡å¼
        removed_disclaimers = []
        for pattern in self.disclaimer_patterns:
            matches = re.findall(pattern, cleaned_text, flags=re.IGNORECASE | re.MULTILINE)
            if matches:
                removed_disclaimers.extend(matches)
                cleaned_text = re.sub(pattern, "", cleaned_text, flags=re.IGNORECASE | re.MULTILINE)
        
        # è¨˜éŒ„ç§»é™¤çš„å…è²¬è²æ˜
        if removed_disclaimers:
            self._write_debug_log("ç§»é™¤å…è²¬è²æ˜ (REMOVE DISCLAIMERS)", 
                                f"ç§»é™¤çš„å…§å®¹: {removed_disclaimers}\nåŸæ–‡: {original_text}\næ¸…ç†å¾Œ: {cleaned_text}")
            self.logger.debug(f"ğŸš« ç§»é™¤å…è²¬è²æ˜: {len(removed_disclaimers)} å€‹")
        
        # æ¸…ç†å¤šé¤˜çš„ç©ºè¡Œå’Œç©ºç™½
        cleaned_text = re.sub(r'\n\s*\n+', '\n', cleaned_text)
        cleaned_text = re.sub(r'^\s+|\s+$', '', cleaned_text, flags=re.MULTILINE)
        cleaned_text = cleaned_text.strip()
        
        return cleaned_text

    def convert_to_traditional_chinese(self, text: str) -> str:
        """å°‡ç°¡é«”ä¸­æ–‡è½‰æ›ç‚ºç¹é«”ä¸­æ–‡"""
        if not self.opencc_converter or not self.enable_traditional_chinese:
            return text
        
        if not text or not text.strip():
            return text
        
        try:
            # åŸ·è¡Œç°¡ç¹è½‰æ›
            converted_text = self.opencc_converter.convert(text)
            
            # è¨˜éŒ„è½‰æ›çµæœï¼ˆåƒ…åœ¨æœ‰è®ŠåŒ–æ™‚ï¼‰
            if converted_text != text:
                self._write_debug_log("ç°¡ç¹è½‰æ› (OPENCC CONVERSION)", 
                                    f"åŸæ–‡: {text}\nè½‰æ›å¾Œ: {converted_text}\né…ç½®: {self.opencc_config}")
                self.logger.debug(f"ğŸ”„ ç°¡ç¹è½‰æ› ({self.opencc_config}): '{text[:30]}...' â†’ '{converted_text[:30]}...'")
            
            return converted_text
            
        except Exception as e:
            self.logger.error(f"ç°¡ç¹è½‰æ›å¤±æ•—: {e}")
            return text
    
    def toggle_traditional_chinese(self, enable: bool = None) -> bool:
        """åˆ‡æ›ç°¡ç¹è½‰æ›åŠŸèƒ½"""
        if enable is None:
            self.enable_traditional_chinese = not self.enable_traditional_chinese
        else:
            self.enable_traditional_chinese = enable
        
        status = "å•Ÿç”¨" if self.enable_traditional_chinese else "ç¦ç”¨"
        self.logger.info(f"ğŸ”„ OpenCC ç°¡ç¹è½‰æ›åŠŸèƒ½å·²{status}")
        
        return self.enable_traditional_chinese
        
    def set_character_name(self, character_name: str):
        """è¨­ç½®è§’è‰²åç¨±"""
        self.character_name = character_name
        # æ›´æ–°éæ¿¾æ¨¡å¼
        self.filter_patterns = [
            f"{self.character_name}ï¼š",
            f"{self.character_name}:",
            f"{self.character_name} :",
            f"{self.character_name}èªªï¼š",
            f"{self.character_name}èªª:",
        ]
    
    def filter_response(self, response: str) -> str:
        """ä¸»è¦éæ¿¾æ–¹æ³•"""
        if not response or not response.strip():
            return ""
        
        try:
            # è¨˜éŒ„åŸå§‹å›æ‡‰
            self._write_debug_log("åŸå§‹å›æ‡‰", response)
            
            # æˆªå–ç¬¬ä¸€å€‹æ¨™è¨˜ä¹‹å‰çš„å…§å®¹
            filtered = self._extract_before_tokens(response)
            
            # è¨˜éŒ„è¢«éæ¿¾æ‰çš„å…§å®¹
            filtered_out = response.replace(filtered, "").strip() if filtered else response
            if filtered_out:
                self._write_debug_log("éæ¿¾æ‰çš„æ–‡å­—", filtered_out)
            
            # æœ€çµ‚æ¸…ç†
            if filtered:
                filtered = self._final_cleanup(filtered)
            
            # ğŸ”¥ æ–°å¢ï¼šç§»é™¤å…è²¬è²æ˜
            if filtered:
                filtered = self._remove_disclaimers(filtered)
            
            # ğŸ”¥ æ–°å¢ï¼šç°¡ç¹è½‰æ›è™•ç†
            if filtered and self.enable_traditional_chinese:
                filtered = self.convert_to_traditional_chinese(filtered)
            
            # è¨˜éŒ„æœ€çµ‚è¼¸å‡º
            self._write_debug_log("æœ€çµ‚è¼¸å‡º", filtered)
            
            return filtered.strip() if filtered else ""
            
        except Exception as e:
            self.logger.error(f"éæ¿¾å›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            self._write_debug_log("éŒ¯èª¤", f"éæ¿¾å¤±æ•—: {e}")
            return response.strip()
    
    def _extract_before_tokens(self, text: str) -> str:
        """æå–ç¬¬ä¸€å€‹ç‰¹æ®Šæ¨™è¨˜ä¹‹å‰çš„å…§å®¹"""
        # æ–°å¢ï¼šå®šç¾©æˆªæ–·æ¨™è¨˜æ¨¡å¼
        truncate_patterns = [
            r'<\|[^|]*\|>',     # åŸæœ¬çš„ <|...| > æ ¼å¼
            r'<<\|[^|]*',       # æ–°å¢ï¼š<<|?? æ ¼å¼
            r'<</[^>]*>',       # æ–°å¢ï¼š<</start> æ ¼å¼
            r'<<[^>]*>',        # æ–°å¢ï¼šå…¶ä»– << é–‹é ­çš„æ¨™è¨˜
            r'<\|[^|]*\|',
            r'<|end|>',
            r'è¨»é‡‹',
            r'ï¼ˆæ³¨ï¼š.*?ï¼‰',      # æ–°å¢ï¼šæ‹¬è™Ÿå…§çš„æ³¨é‡‹
            r'æ³¨é‡‹',
        ]
        
        # å°‹æ‰¾ç¬¬ä¸€å€‹åŒ¹é…çš„æˆªæ–·æ¨™è¨˜
        earliest_pos = len(text)
        matched_pattern = None
        
        for pattern in truncate_patterns:
            match = re.search(pattern, text)
            if match and match.start() < earliest_pos:
                earliest_pos = match.start()
                matched_pattern = pattern
        
        # å¦‚æœæ‰¾åˆ°ä»»ä½•æˆªæ–·æ¨™è¨˜ï¼Œåœ¨è©²ä½ç½®æˆªæ–·
        if earliest_pos < len(text):
            content = text[:earliest_pos]
            self._write_debug_log("æˆªæ–·æ¨™è¨˜æª¢æ¸¬", f"ä½¿ç”¨æ¨¡å¼ '{matched_pattern}' åœ¨ä½ç½® {earliest_pos} æˆªæ–·")
            return content.strip()
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°æˆªæ–·æ¨™è¨˜ï¼Œæª¢æŸ¥æ˜¯å¦æœ‰å…¶ä»–åœæ­¢æ¨™è¨˜
        for token in self.remove_tokens:
            if token in text:
                content = text.split(token)[0]
                self._write_debug_log("åœæ­¢æ¨™è¨˜æˆªæ–·", f"é‡åˆ°åœæ­¢æ¨™è¨˜ '{token}' é€²è¡Œæˆªæ–·")
                return content.strip()
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ä»»ä½•æ¨™è¨˜ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
        return text.strip()
    
    def _basic_cleanup(self, text: str) -> str:
        """åŸºæœ¬æ¸…ç†"""
        original_text = text
        
        # 1. è™•ç†é‚Šç•Œæ¨™è¨˜ - åœ¨ <|end|> è™•æˆªæ–·
        if self.boundary_token in text:
            text = text.split(self.boundary_token)[0]
            self._write_debug_log("é‚Šç•Œæ¨™è¨˜æˆªæ–· (BOUNDARY CUT)", f"æ‰¾åˆ° {self.boundary_token}ï¼Œæˆªæ–·å¾Œ: {text}")
        
        # 2. ç§»é™¤æ‰€æœ‰ç‰¹æ®Šæ¨™è¨˜
        for token in self.remove_tokens:
            if token in text:
                text = text.replace(token, '')
                self._write_debug_log(f"ç§»é™¤æ¨™è¨˜ (REMOVE TOKEN)", f"ç§»é™¤ '{token}' å¾Œ: {text}")
        
        # 3. é€šç”¨éæ¿¾ï¼šç§»é™¤æ‰€æœ‰ <|...| > æ ¼å¼çš„æ¨™è¨˜
        matches = re.findall(r'<\|[^|]*\|>', text)
        if matches:
            self._write_debug_log("ç™¼ç¾é€šç”¨æ¨™è¨˜ (FOUND GENERIC TOKENS)", f"ç™¼ç¾æ¨™è¨˜: {matches}")
            text = re.sub(r'<\|[^|]*\|>', '', text)
            self._write_debug_log("é€šç”¨æ¨™è¨˜éæ¿¾å¾Œ (GENERIC FILTER)", text)
        
        # 4. ç§»é™¤æ¨¡å‹æ€è€ƒéç¨‹ï¼ˆé€šå¸¸åœ¨æ‹¬è™Ÿæˆ–ç‰¹æ®Šæ ¼å¼ä¸­ï¼‰
        text = self._remove_thinking_process(text)
        
        # 5. ç§»é™¤å¤šé¤˜çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        
        # 6. ç§»é™¤åœæ­¢è©å½™
        for pattern in self.stop_patterns:
            if pattern in text:
                text = text.replace(pattern, '')
                self._write_debug_log(f"ç§»é™¤åœæ­¢è© (REMOVE STOP WORD)", f"ç§»é™¤ '{pattern}' å¾Œ: {text}")
        
        # 7. ç§»é™¤é–‹é ­çš„æ¨™é»ç¬¦è™Ÿ
        text = re.sub(r'^[ï¼š:]+', '', text)
        
        return text.strip()
    
    def _remove_thinking_process(self, text: str) -> str:
        """ç§»é™¤æ¨¡å‹çš„æ€è€ƒéç¨‹"""
        # ç§»é™¤ <|thinking|> æ¨™è¨˜å…§çš„æ‰€æœ‰å…§å®¹
        text = re.sub(r'<\|thinking\|>.*?(?=<\||\Z)', '', text, flags=re.DOTALL)
        
        # ç§»é™¤æ‹¬è™Ÿå…§çš„æ€è€ƒå…§å®¹ï¼ˆå¦‚æœå¾ˆé•·çš„è©±ï¼‰
        text = re.sub(r'\([^)]{20,}\)', '', text)
        text = re.sub(r'ï¼ˆ[^ï¼‰]{20,}ï¼‰', '', text)
        
        # ç§»é™¤å¸¸è¦‹çš„æ€è€ƒæ¨™è¨˜
        thinking_patterns = [
            r'è®“æˆ‘æƒ³æƒ³[^ã€‚]*ã€‚',
            r'æˆ‘è¦ºå¾—[^ã€‚]*ã€‚',
            r'æ ¹æ“š[^ã€‚]*ï¼Œ',
            r'åŸºæ–¼[^ã€‚]*ï¼Œ',
            r'è€ƒæ…®åˆ°[^ã€‚]*ï¼Œ',
        ]
        
        for pattern in thinking_patterns:
            text = re.sub(pattern, '', text)
        
        return text
    
    def _remove_repetitions(self, text: str) -> str:
        """ç§»é™¤é‡è¤‡å…§å®¹"""
        # åˆ†å‰²æˆå¥å­
        sentences = self._split_sentences(text)
        
        if not sentences:
            return text
        
        # ç§»é™¤é€£çºŒé‡è¤‡çš„å¥å­
        filtered_sentences = []
        prev_sentence = ""
        repeat_count = 0
        
        for sentence in sentences:
            sentence_clean = re.sub(r'[^\w\s]', '', sentence).strip()
            prev_clean = re.sub(r'[^\w\s]', '', prev_sentence).strip()
            
            if sentence_clean == prev_clean and sentence_clean:
                repeat_count += 1
                if repeat_count <= self.max_sentence_repeat:
                    filtered_sentences.append(sentence)
            else:
                repeat_count = 0
                filtered_sentences.append(sentence)
                prev_sentence = sentence
        
        # ç§»é™¤ N-gram é‡è¤‡
        result = ''.join(filtered_sentences)
        result = self._remove_ngram_repetitions(result)
        
        return result
    
    def _remove_ngram_repetitions(self, text: str, n: int = 3) -> str:
        """ç§»é™¤ N-gram é‡è¤‡"""
        words = text.split()
        if len(words) < n * 2:
            return text
        
        # æª¢æ¸¬é‡è¤‡çš„ n-gram
        filtered_words = []
        i = 0
        
        while i < len(words):
            if i + n * 2 <= len(words):
                # æª¢æŸ¥ç•¶å‰ n-gram æ˜¯å¦èˆ‡ä¸‹ä¸€å€‹ n-gram é‡è¤‡
                current_ngram = words[i:i+n]
                next_ngram = words[i+n:i+n*2]
                
                if current_ngram == next_ngram:
                    # è·³éé‡è¤‡çš„ n-gram
                    filtered_words.extend(current_ngram)
                    i += n * 2
                else:
                    filtered_words.append(words[i])
                    i += 1
            else:
                filtered_words.append(words[i])
                i += 1
        
        return ' '.join(filtered_words)
    
    def _split_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²å¥å­"""
        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åˆ†å‰²å¥å­
        pattern = r'([ã€‚ï¼ï¼Ÿ.!?~â™ªâ™¡]+)'
        parts = re.split(pattern, text)
        
        sentences = []
        current_sentence = ""
        
        for part in parts:
            current_sentence += part
            if any(ending in part for ending in self.sentence_endings):
                if current_sentence.strip():
                    sentences.append(current_sentence.strip())
                current_sentence = ""
        
        # æ·»åŠ å‰©é¤˜éƒ¨åˆ†
        if current_sentence.strip():
            sentences.append(current_sentence.strip())
        
        return sentences
    
    def _remove_incomplete_sentences(self, text: str) -> str:
        """ç§»é™¤ä¸å®Œæ•´çš„å¥å­"""
        sentences = self._split_sentences(text)
        
        if not sentences:
            return text
        
        # ä¿ç•™å®Œæ•´çš„å¥å­
        complete_sentences = []
        
        for sentence in sentences:
            # æª¢æŸ¥å¥å­æ˜¯å¦ä»¥æ¨™é»ç¬¦è™Ÿçµå°¾
            if any(sentence.rstrip().endswith(ending) for ending in self.sentence_endings):
                complete_sentences.append(sentence)
            elif len(sentence.strip()) > 20:  # å¦‚æœå¥å­è¼ƒé•·ï¼Œå¯èƒ½æ˜¯å®Œæ•´çš„
                # æ·»åŠ é©ç•¶çš„çµå°¾æ¨™é»
                sentence = sentence.rstrip()
                if not any(sentence.endswith(ending) for ending in self.sentence_endings):
                    sentence += 'ã€‚'
                complete_sentences.append(sentence)
        
        # å¦‚æœæ²’æœ‰å®Œæ•´å¥å­ï¼Œè¿”å›åŸæ–‡
        if not complete_sentences:
            return text
        
        return ''.join(complete_sentences)
    
    def _final_cleanup(self, text: str) -> str:
        """æœ€çµ‚æ¸…ç†"""
        # ç§»é™¤å¤šé¤˜çš„æ¨™é»ç¬¦è™Ÿ
        text = re.sub(r'([ã€‚ï¼ï¼Ÿ.!?]){2,}', r'\1', text)
        
        # ç§»é™¤å¤šé¤˜çš„ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤é–‹é ­å’Œçµå°¾çš„ç©ºç™½
        text = text.strip()
        
        # ç¢ºä¿ä»¥é©ç•¶çš„æ¨™é»ç¬¦è™Ÿçµå°¾
        if text and not any(text.endswith(ending) for ending in self.sentence_endings):
            # å¦‚æœæ˜¯å•å¥ï¼Œæ·»åŠ å•è™Ÿ
            if any(word in text for word in ['å—', 'å‘¢', 'å§', 'ä»€éº¼', 'æ€éº¼', 'ç‚ºä»€éº¼', 'å“ªè£¡']):
                text += 'ï¼Ÿ'
            else:
                text += 'ã€‚'
        
        return text
    
    def validate_response(self, response: str) -> bool:
        """é©—è­‰å›æ‡‰æ˜¯å¦æœ‰æ•ˆ"""
        if not response or not response.strip():
            return False
        
        # æª¢æŸ¥æœ€å°é•·åº¦
        if len(response.strip()) < 2:
            return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æœ‰æ„ç¾©çš„å…§å®¹
        meaningful_chars = re.sub(r'[^\w]', '', response)
        if len(meaningful_chars) < 2:
            return False
        
        return True
    
    def get_filter_stats(self, original: str, filtered: str) -> dict:
        """ç²å–éæ¿¾çµ±è¨ˆä¿¡æ¯"""
        # æª¢æŸ¥æ˜¯å¦é€²è¡Œäº†ç°¡ç¹è½‰æ›
        conversion_applied = False
        if self.enable_traditional_chinese and self.opencc_converter:
            # ç°¡å–®æª¢æ¸¬ï¼šå¦‚æœåŒ…å«ç°¡é«”å­—ç¬¦ï¼Œå¯èƒ½é€²è¡Œäº†è½‰æ›
            simplified_chars = set('è¿™ä¸ªä½ ä»¬æ¥è¯´è¯')
            traditional_chars = set('é€™å€‹ä½ å€‘ä¾†èªªè©±')
            
            has_traditional = any(char in filtered for char in traditional_chars)
            has_simplified = any(char in original for char in simplified_chars)
            conversion_applied = has_simplified and has_traditional
        
        # æª¢æŸ¥æ˜¯å¦ç§»é™¤äº†å…è²¬è²æ˜
        disclaimers_removed = False
        for pattern in self.disclaimer_patterns:
            if re.search(pattern, original, flags=re.IGNORECASE):
                disclaimers_removed = True
                break
        
        return {
            'original_length': len(original),
            'filtered_length': len(filtered),
            'reduction_ratio': 1 - (len(filtered) / len(original)) if original else 0,
            'original_sentences': len(self._split_sentences(original)),
            'filtered_sentences': len(self._split_sentences(filtered)),
            'traditional_chinese_enabled': self.enable_traditional_chinese,
            'conversion_applied': conversion_applied,
            'disclaimers_removed': disclaimers_removed,
            'opencc_available': OPENCC_AVAILABLE and self.opencc_converter is not None,
            'opencc_config': self.opencc_config if self.opencc_converter else None
        }
    
    def add_disclaimer_pattern(self, pattern: str):
        """æ·»åŠ è‡ªå®šç¾©å…è²¬è²æ˜æ¨¡å¼"""
        if pattern not in self.disclaimer_patterns:
            self.disclaimer_patterns.append(pattern)
            self.logger.info(f"âœ… æ·»åŠ å…è²¬è²æ˜æ¨¡å¼: {pattern}")
    
    def remove_disclaimer_pattern(self, pattern: str):
        """ç§»é™¤å…è²¬è²æ˜æ¨¡å¼"""
        if pattern in self.disclaimer_patterns:
            self.disclaimer_patterns.remove(pattern)
            self.logger.info(f"ğŸ—‘ï¸ ç§»é™¤å…è²¬è²æ˜æ¨¡å¼: {pattern}")
    
    def get_disclaimer_patterns(self) -> list:
        """ç²å–ç•¶å‰çš„å…è²¬è²æ˜æ¨¡å¼"""
        return self.disclaimer_patterns.copy()

    def get_conversion_status(self) -> dict:
        """ç²å–ç°¡ç¹è½‰æ›ç‹€æ…‹"""
        return {
            'opencc_available': OPENCC_AVAILABLE,
            'converter_initialized': self.opencc_converter is not None,
            'conversion_enabled': self.enable_traditional_chinese,
            'opencc_config': self.opencc_config if self.opencc_converter else None,
            'supported_configs': ['s2t.json', 's2tw.json', 's2twp.json', 's2hk.json'] if OPENCC_AVAILABLE else []
        }