"""
LLM æ¨¡å‹ç®¡ç†å™¨
è² è²¬è¼‰å…¥å’Œç®¡ç† Qwen-8B ä¸»æ¨¡å‹å’Œ Qwen3-Embedding åµŒå…¥æ¨¡å‹
"""

import asyncio
import logging
import torch
import numpy as np
from typing import Optional, List, Dict, Any
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    BitsAndBytesConfig, AutoModel
)
from sentence_transformers import SentenceTransformer
import gc
from .filter.filter import ResponseFilter
from .core import RushiaPersonalityCore


class OptimizedEmbeddingModel:
    """å„ªåŒ–çš„åµŒå…¥æ¨¡å‹åŒ…è£å™¨ï¼Œæ”¯æŒ8bité‡åŒ–"""
    
    def __init__(self, model, tokenizer, device, max_length=512, batch_size=32):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.max_length = max_length
        self.batch_size = batch_size
        
    def encode(self, texts, batch_size=None, convert_to_tensor=True, device=None, **kwargs):
        """ç·¨ç¢¼æ–‡æœ¬ç‚ºåµŒå…¥å‘é‡"""
        if isinstance(texts, str):
            texts = [texts]
        
        if batch_size is None:
            batch_size = self.batch_size
        
        all_embeddings = []
        
        # åˆ†æ‰¹è™•ç†
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            # Tokenize
            inputs = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt"
            ).to(self.device)
            
            # ç²å–åµŒå…¥
            with torch.no_grad():
                outputs = self.model(**inputs)
                
                # ä½¿ç”¨ [CLS] token æˆ–å¹³å‡æ± åŒ–
                if hasattr(outputs, 'last_hidden_state'):
                    # å¹³å‡æ± åŒ–
                    attention_mask = inputs['attention_mask']
                    token_embeddings = outputs.last_hidden_state
                    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
                    embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
                else:
                    # ä½¿ç”¨ pooler_output å¦‚æœå¯ç”¨
                    embeddings = outputs.pooler_output if hasattr(outputs, 'pooler_output') else outputs[0][:, 0]
                
                # æ­£è¦åŒ–
                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
                
                all_embeddings.append(embeddings)
        
        # åˆä½µæ‰€æœ‰æ‰¹æ¬¡
        final_embeddings = torch.cat(all_embeddings, dim=0)
        
        if convert_to_tensor:
            return final_embeddings
        else:
            return final_embeddings.cpu().numpy()


class LLMManager:
    def __init__(self, config: dict):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # æ¨¡å‹çµ„ä»¶
        self.llm_model: Optional[AutoModelForCausalLM] = None
        self.llm_tokenizer: Optional[AutoTokenizer] = None
        self.embedding_model: Optional[SentenceTransformer] = None
        
        # å›æ‡‰éæ¿¾å™¨
        self.response_filter = ResponseFilter(config)
        
        # æ ¸å¿ƒäººæ ¼æ¨¡çµ„
        self.personality_core = RushiaPersonalityCore()
        
        # ğŸ”¥ æ–°å¢ï¼šå°è©±è¨ˆæ•¸å™¨ï¼ˆç”¨æ–¼å‹•æ…‹ç³»çµ±æç¤ºè©ï¼‰
        self.conversation_count = 0
        
        # è¨­å‚™é…ç½®
        self.device = config['models']['llm']['device']
        if self.device == "cuda" and not torch.cuda.is_available():
            self.logger.warning("CUDA ä¸å¯ç”¨ï¼Œåˆ‡æ›åˆ° CPU")
            self.device = "cpu"
    
    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹"""
        await self._load_llm_model()
        await self._load_embedding_model()
        await self._setup_vtuber_personality()
    
    async def _load_llm_model(self):
        """è¼‰å…¥ä¸»è¦çš„ LLM æ¨¡å‹ (Qwen-8B)"""
        self.logger.info("è¼‰å…¥ Qwen-8B ä¸»æ¨¡å‹...")
        
        model_path = self.config['models']['llm']['model_path']
        
        # 4bit é‡åŒ–é…ç½® (å›ºå®šä½¿ç”¨4bitä»¥ç²å¾—æœ€ä½³è¨˜æ†¶é«”æ•ˆç‡)
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        ) if self.device == "cuda" else None
        
        try:
            # è¼‰å…¥ tokenizer
            self.llm_tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                padding_side="left"
            )
            
            # è¨­ç½® pad_token
            if self.llm_tokenizer.pad_token is None:
                self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token
            
            # è¼‰å…¥æ¨¡å‹
            self.llm_model = AutoModelForCausalLM.from_pretrained(
                model_path,
                quantization_config=quantization_config,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                low_cpu_mem_usage=True
            )
            
            if self.device == "cpu":
                self.llm_model = self.llm_model.to(self.device)
            
            self.logger.info("âœ… Qwen-8B æ¨¡å‹è¼‰å…¥æˆåŠŸ (4bité‡åŒ–)")
            
        except Exception as e:
            self.logger.error(f"âŒ Qwen-8B æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise
    
    async def _load_embedding_model(self):
        """è¼‰å…¥åµŒå…¥æ¨¡å‹ (Qwen3-Embedding-0.6B) ä½¿ç”¨8bité‡åŒ–"""
        self.logger.info("è¼‰å…¥ Qwen3-Embedding-0.6B åµŒå…¥æ¨¡å‹ (8bité‡åŒ–)...")
        
        model_path = self.config['models']['embedding']['model_path']
        
        try:
            # 8bit é‡åŒ–é…ç½®
            embedding_quantization_config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_enable_fp32_cpu_offload=True,
                llm_int8_has_fp16_weight=False,
                llm_int8_threshold=6.0
            )
            
            # å…ˆè¼‰å…¥åŸå§‹æ¨¡å‹é€²è¡Œé‡åŒ–
            from transformers import AutoModel
            
            # è¼‰å…¥ tokenizer
            embedding_tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            
            # è¼‰å…¥æ¨¡å‹ä¸¦æ‡‰ç”¨8bité‡åŒ–
            embedding_model = AutoModel.from_pretrained(
                model_path,
                quantization_config=embedding_quantization_config if self.device == "cuda" else None,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                low_cpu_mem_usage=True
            )
            
            if self.device == "cpu":
                embedding_model = embedding_model.to(self.device)
            
            # å‰µå»ºè‡ªå®šç¾©çš„åµŒå…¥æ¨¡å‹åŒ…è£å™¨
            self.embedding_model = OptimizedEmbeddingModel(
                model=embedding_model,
                tokenizer=embedding_tokenizer,
                device=self.device,
                max_length=self.config['models']['embedding']['max_length'],
                batch_size=self.config['models']['embedding']['batch_size']
            )
            
            self.logger.info("âœ… Qwen3-Embedding æ¨¡å‹è¼‰å…¥æˆåŠŸ (8bité‡åŒ–)")
            
        except Exception as e:
            self.logger.error(f"âŒ Qwen3-Embedding æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            self.logger.info("å›é€€åˆ°æ¨™æº–è¼‰å…¥æ–¹æ³•...")
            try:
                self.embedding_model = SentenceTransformer(
                    model_path,
                    device=self.device
                )
                self.logger.info("âœ… Qwen3-Embedding æ¨¡å‹è¼‰å…¥æˆåŠŸ (æ¨™æº–æ¨¡å¼)")
            except Exception as fallback_error:
                self.logger.error(f"âŒ æ¨™æº–è¼‰å…¥ä¹Ÿå¤±æ•—: {fallback_error}")
                raise
    
    async def _setup_vtuber_personality(self):
        """è¨­ç½® VTuber è§’è‰²äººæ ¼ - å®Œå…¨ä¾è³´ core.json"""
        # è¼‰å…¥æ ¸å¿ƒäººæ ¼æ•¸æ“š
        if not self.personality_core.load_core_personality():
            self.logger.error("âŒ ç„¡æ³•è¼‰å…¥æ ¸å¿ƒäººæ ¼æ•¸æ“šï¼Œç³»çµ±ç„¡æ³•é‹è¡Œ")
            raise RuntimeError("æ ¸å¿ƒäººæ ¼æ•¸æ“šè¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥ rushia_wiki/core.json æ–‡ä»¶")
        
        # å®Œå…¨ä½¿ç”¨æ ¸å¿ƒäººæ ¼ç”Ÿæˆç³»çµ±æç¤ºè©
        self.system_prompt = self.personality_core.generate_system_prompt()
        
        # è¨˜éŒ„è¼‰å…¥çš„è§’è‰²ä¿¡æ¯
        identity = self.personality_core.get_character_identity()
        personality = self.personality_core.get_personality_traits()
        
        # è¨­ç½®éæ¿¾å™¨çš„è§’è‰²åç¨±
        character_name = identity['name'].get('zh', 'éœ²è¥¿å©­')
        self.response_filter.set_character_name(character_name)
        
        self.logger.info("âœ… VTuber è§’è‰²äººæ ¼è¨­ç½®å®Œæˆ")
        self.logger.info(f"   è§’è‰²åç¨±: {character_name}")
        self.logger.info(f"   æ€§æ ¼ç‰¹å¾µ: {', '.join(personality['primary_traits'])}")
        self.logger.info(f"   ç•¶å‰æƒ…ç·’: {self.personality_core.current_mood}")
    
    async def generate_response(
        self, 
        prompt: str, 
        context: Optional[str] = None,
        conversation_history: Optional[List[tuple]] = None,  # ğŸ”¥ æ–°å¢ï¼šå°è©±æ­·å²
        stream: bool = False,
        rag_enabled: bool = True
    ) -> str:
        """ç”Ÿæˆå›æ‡‰ - æ”¯æŒå‹•æ…‹ç³»çµ±æç¤ºè©å’Œå°è©±æ­·å²çš„èªç¾©åˆ†æå¢å¼·ç‰ˆ"""
        try:
            # ğŸ”¥ å¢åŠ å°è©±è¨ˆæ•¸
            self.conversation_count += 1
            
            # ğŸ”¥ ä¿®å¾©ï¼šå‚³éå°è©±æ­·å²åˆ°æƒ…æ„Ÿåˆ†æ
            if hasattr(self.personality_core, 'analyze_emotional_triggers_enhanced'):
                emotional_analysis = self.personality_core.analyze_emotional_triggers_enhanced(
                    prompt, conversation_history  # ğŸ”¥ å‚³éå°è©±æ­·å²
                )
            else:
                emotional_analysis = self.personality_core.analyze_emotional_triggers(prompt)
            
            # æ ¹æ“šæƒ…ç·’åˆ†æèª¿æ•´å¿ƒæƒ…
            if emotional_analysis['suggested_mood'] != self.personality_core.current_mood:
                self.personality_core.update_mood(emotional_analysis['suggested_mood'])
            
            # ğŸ”¥ æ–°å¢ï¼šå‰µå»ºä¸Šä¸‹æ–‡æç¤ºï¼ˆç”¨æ–¼å‹•æ…‹ç³»çµ±æç¤ºè©ï¼‰
            context_hints = self._create_context_hints(emotional_analysis, conversation_history)
            
            # ğŸ”¥ ä½¿ç”¨å‹•æ…‹ç³»çµ±æç¤ºè©ç”Ÿæˆï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(self.personality_core, 'generate_dynamic_system_prompt'):
                self.system_prompt = self.personality_core.generate_dynamic_system_prompt(
                    conversation_count=self.conversation_count,
                    context_hints=context_hints
                )
                self.logger.info(f"ğŸ“ ä½¿ç”¨å‹•æ…‹ç³»çµ±æç¤ºè© (ç¬¬ {self.conversation_count} æ¬¡å°è©±)")
            elif hasattr(self.personality_core, 'generate_system_prompt_enhanced'):
                self.system_prompt = self.personality_core.generate_system_prompt_enhanced()
            else:
                self.system_prompt = self.personality_core.generate_system_prompt()
            
            # ç²å–å›æ‡‰æç¤º
            response_hints = self.personality_core.get_contextual_response_hints(emotional_analysis)
            
            # èªç¾©å‘é‡å¢å¼·çš„ä¸Šä¸‹æ–‡æª¢ç´¢ (åªåœ¨RAGå•Ÿç”¨æ™‚)
            enhanced_context = context
            if rag_enabled and not context:
                enhanced_context = await self._get_semantic_vector_enhanced_context(prompt, emotional_analysis)
            
            # æ§‹å»ºå®Œæ•´çš„æç¤ºï¼ˆåŒ…å«å°è©±æ­·å²ï¼‰
            full_prompt = self._build_enhanced_prompt_with_history(
                prompt, enhanced_context, conversation_history, emotional_analysis, response_hints
            )
            
            # ç²å–å‹•æ…‹ç”Ÿæˆåƒæ•¸
            generation_config = self._get_adaptive_generation_config(emotional_analysis)
            
            # Tokenize
            inputs = self.llm_tokenizer(
                full_prompt,
                return_tensors="pt",
                truncation=True,
                max_length=self.config['models']['llm']['max_length'] - 512
            ).to(self.device)
            
            # ç”Ÿæˆå›æ‡‰ - ğŸ”¥ åœ¨åŸ·è¡Œå™¨ä¸­é‹è¡Œï¼Œé¿å…é˜»å¡
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                self._generate_model_response,
                inputs,
                generation_config
            )
            
            # æ‡‰ç”¨éæ¿¾å™¨è™•ç†å›æ‡‰
            filtered_response = self.response_filter.filter_response(response)
            
            # é©—è­‰å›æ‡‰æœ‰æ•ˆæ€§
            if not self.response_filter.validate_response(filtered_response):
                self.logger.warning("ç”Ÿæˆçš„å›æ‡‰ç„¡æ•ˆï¼Œä½¿ç”¨å‚™ç”¨å›æ‡‰")
                return "å—¯å—¯ï¼Œæˆ‘éœ€è¦æƒ³æƒ³å‘¢ï½"
            
            return filtered_response
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘ç¾åœ¨ç„¡æ³•å›æ‡‰ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
    
    def _generate_model_response(self, inputs, generation_config) -> str:
        """åŒæ­¥ç”Ÿæˆæ¨¡å‹å›æ‡‰ï¼ˆåœ¨åŸ·è¡Œå™¨ä¸­é‹è¡Œï¼‰"""
        try:
            with torch.no_grad():
                outputs = self.llm_model.generate(
                    **inputs,
                    **generation_config
                )
            
            # è§£ç¢¼å›æ‡‰
            response = self.llm_tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            return response
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹ç”Ÿæˆå¤±æ•—: {e}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆéç¨‹ä¸­å‡ºç¾å•é¡Œã€‚"
    
    def _build_prompt(
        self, 
        user_input: str, 
        context: Optional[str] = None,
        emotional_analysis: Optional[Dict[str, Any]] = None,
        response_hints: Optional[Dict[str, Any]] = None
    ) -> str:
        """æ§‹å»ºå®Œæ•´çš„æç¤º"""
        prompt_parts = [self.system_prompt]
        
        # æ·»åŠ RAGä¸Šä¸‹æ–‡
        if context:
            prompt_parts.append(f"\n<|context|>\n{context}\n<|end_context|>")
        
        # æ·»åŠ æƒ…ç·’åˆ†æä¿¡æ¯ï¼ˆå¦‚æœæœ‰é¡¯è‘—è§¸ç™¼ï¼‰
        if emotional_analysis and emotional_analysis.get('trigger_strength', 0) >= 2:
            emotion_info = f"æª¢æ¸¬åˆ°æƒ…ç·’è§¸ç™¼ï¼š{emotional_analysis['emotional_category']}"
            if emotional_analysis['detected_triggers']:
                trigger_words = [t['word'] for t in emotional_analysis['detected_triggers']]
                emotion_info += f"ï¼Œè§¸ç™¼è©ï¼š{', '.join(trigger_words)}"
            prompt_parts.append(f"\n<|emotion|>{emotion_info}<|end_emotion|>")
        
        # æ·»åŠ å›æ‡‰é¢¨æ ¼æç¤º
        if response_hints and response_hints.get('response_style') != 'normal':
            style_hint = ""
            if response_hints['response_style'] == 'intense':
                style_hint = "è«‹ä»¥å¼·çƒˆçš„æƒ…ç·’å›æ‡‰ï¼Œå¯èƒ½åŒ…å«æ†¤æ€’æˆ–æ¿€å‹•çš„èªæ°£"
            elif response_hints['response_style'] == 'moderate':
                style_hint = "è«‹ä»¥é©åº¦çš„æƒ…ç·’å›æ‡‰ï¼Œè¡¨ç¾å‡ºç›¸æ‡‰çš„æ„Ÿæƒ…"
            
            if response_hints.get('special_phrases'):
                style_hint += f"ï¼Œå¯ä»¥ä½¿ç”¨é€™äº›è¡¨é”ï¼š{', '.join(response_hints['special_phrases'])}"
            
            if style_hint:
                prompt_parts.append(f"\n<|style|>{style_hint}<|end_style|>")
        
        prompt_parts.append(f"\n<|user|>{user_input}<|end|>")
        
        # ç²å–è§’è‰²åç¨± - å®Œå…¨å¾ core.json ç²å–
        character_name = "AIåŠ©æ‰‹"  # é»˜èªå›é€€åç¨±
        if hasattr(self.personality_core, 'core_data') and self.personality_core.core_data:
            character_name = self.personality_core.get_character_identity()['name'].get('zh', 'éœ²è¥¿å©­')
        
        prompt_parts.append(f"\n<|assistant|>{character_name}ï¼š")
        
        return "\n".join(prompt_parts)
    
    async def _get_semantic_vector_enhanced_context(self, user_input: str, emotional_analysis: Dict[str, Any]) -> Optional[str]:
        """åŸºæ–¼èªç¾©å‘é‡å¢å¼·çš„æ™ºèƒ½æª¢ç´¢"""
        try:
            if not hasattr(self, '_rag_system_ref'):
                return None
            
            # ä½¿ç”¨æ–°çš„èªç¾©å‘é‡å¢å¼·æœç´¢
            search_results = await self._rag_system_ref.semantic_enhanced_search(
                user_input, 
                emotional_analysis, 
                top_k=8  # å¢åŠ æª¢ç´¢æ•¸é‡ä»¥ç²å¾—æ›´å¤šæ¨£æ€§
            )
            
            if not search_results:
                self.logger.info("ğŸ” èªç¾©å‘é‡æœç´¢æœªæ‰¾åˆ°çµæœï¼Œå˜—è©¦æ¨™æº–æœç´¢")
                # å›é€€åˆ°æ¨™æº–æœç´¢
                search_results = await self._rag_system_ref.search(user_input, top_k=5)
            
            if search_results:
                return self._build_diverse_context(search_results, emotional_analysis)
            
            return None
            
        except Exception as e:
            self.logger.error(f"èªç¾©å‘é‡å¢å¼·æª¢ç´¢å¤±æ•—: {e}")
            # å›é€€åˆ°æ¨™æº–æª¢ç´¢
            return await self._get_enhanced_context(user_input, emotional_analysis)

    async def _get_enhanced_context(self, user_input: str, emotional_analysis: Dict[str, Any]) -> Optional[str]:
        """æ ¹æ“šæƒ…ç·’åˆ†æç²å–å¢å¼·ç‰ˆä¸Šä¸‹æ–‡ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        try:
            
            # æ ¹æ“šæƒ…ç·’ç‹€æ…‹èª¿æ•´æª¢ç´¢ç­–ç•¥
            current_mood = self.personality_core.current_mood
            
            # ç¢ºå®šæª¢ç´¢é¡åˆ¥å„ªå…ˆç´š
            category_priority = self._determine_search_category(emotional_analysis, current_mood)
            
            # ä½¿ç”¨å¢å¼·ç‰ˆRAGç³»çµ±æª¢ç´¢
            if hasattr(self, '_rag_system_ref'):
                # ç›´æ¥èª¿ç”¨searchæ–¹æ³•ï¼Œä¿ç•™æœç´¢æ—¥èªŒ
                search_results = await self._rag_system_ref.search(
                    user_input, 
                    top_k=5, 
                    category_filter=category_priority
                )
                
                if search_results:
                    # æ‰‹å‹•æ§‹å»ºä¸Šä¸‹æ–‡ï¼Œä¸å†é¡¯ç¤ºé¡å¤–çš„LLMæ—¥èªŒ
                    context_parts = []
                    for result in search_results:
                        content = result['content'].strip()
                        metadata = result['metadata']
                        
                        # æ§‹å»ºä¾†æºä¿¡æ¯
                        source_info = []
                        if metadata.get('category'):
                            source_info.append(f"é¡åˆ¥: {metadata['category']}")
                        if metadata.get('section_title'):
                            source_info.append(f"ç« ç¯€: {metadata['section_title']}")
                        if metadata.get('filename'):
                            source_info.append(f"æ–‡ä»¶: {metadata['filename']}")
                        
                        source_str = " | ".join(source_info) if source_info else "æœªçŸ¥ä¾†æº"
                        similarity = result['similarity']
                        
                        context_parts.append(f"[{source_str} | ç›¸é—œåº¦: {similarity:.3f}]\n{content}")
                    
                    return "\n\n---\n\n".join(context_parts)
                
                return None
            
            return None
            
        except Exception as e:
            self.logger.error(f"ç²å–å¢å¼·ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            return None
    
    def _determine_search_category(self, emotional_analysis: Dict[str, Any], current_mood: str) -> Optional[str]:
        """æ ¹æ“šæƒ…ç·’åˆ†æç¢ºå®šæœç´¢é¡åˆ¥å„ªå…ˆç´š"""
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æƒ…ç·’è§¸ç™¼
        if emotional_analysis.get('trigger_strength', 0) >= 2:
            emotional_category = emotional_analysis.get('emotional_category', 'neutral')
            
            if emotional_category == 'angry':
                # æ†¤æ€’æƒ…ç·’ - å„ªå…ˆæœç´¢è©å½™å’Œèº«ä»½ç›¸é—œ
                return 'vocabulary'
            elif emotional_category == 'happy':
                # é–‹å¿ƒæƒ…ç·’ - å„ªå…ˆæœç´¢äº’å‹•å’Œå…§å®¹ç›¸é—œ
                return 'relationships'
        
        # æ ¹æ“šç•¶å‰å¿ƒæƒ…ç¢ºå®šé¡åˆ¥
        if current_mood == 'gaming':
            return 'content'  # éŠæˆ²æ¨¡å¼å„ªå…ˆæœç´¢å…§å®¹ç›¸é—œ
        elif current_mood == 'embarrassed':
            return 'core_identity'  # æ•æ„Ÿæ¨¡å¼å„ªå…ˆæœç´¢èº«ä»½ç›¸é—œ
        elif current_mood == 'protective':
            return 'relationships'  # ä¿è­·æ¨¡å¼å„ªå…ˆæœç´¢é—œä¿‚ç›¸é—œ
        
        # é»˜èªä¸é™åˆ¶é¡åˆ¥
        return None
    
    def set_rag_system_reference(self, rag_system):
        """è¨­ç½®RAGç³»çµ±å¼•ç”¨ï¼ˆç”¨æ–¼å¢å¼·æª¢ç´¢ï¼‰"""
        self._rag_system_ref = rag_system
    
    def get_embeddings(self, texts: List[str]) -> torch.Tensor:
        """ç²å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        try:
            embeddings = self.embedding_model.encode(
                texts,
                batch_size=self.config['models']['embedding']['batch_size'],
                convert_to_tensor=True,
                device=self.device
            )
            return embeddings
        except Exception as e:
            self.logger.error(f"ç”ŸæˆåµŒå…¥å‘é‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise
    
    def get_model_info(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹ä¿¡æ¯"""
        try:
            llm_info = {
                "model_type": "Qwen-8B",
                "quantization": "4bit",
                "device": self.device,
                "memory_usage": "Unknown"
            }
            
            embedding_info = {
                "model_type": "Qwen3-Embedding-0.6B", 
                "quantization": "8bit",
                "device": self.device,
                "memory_usage": "Unknown"
            }
            
            # å˜—è©¦ç²å–è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
            if torch.cuda.is_available() and self.device == "cuda":
                gpu_memory = torch.cuda.get_device_properties(0).total_memory
                gpu_allocated = torch.cuda.memory_allocated(0)
                gpu_cached = torch.cuda.memory_reserved(0)
                
                llm_info["gpu_total"] = f"{gpu_memory // (1024**3)}GB"
                llm_info["gpu_allocated"] = f"{gpu_allocated // (1024**3)}GB"
                llm_info["gpu_cached"] = f"{gpu_cached // (1024**3)}GB"
                
                embedding_info.update(llm_info)
            
            # æ·»åŠ æ ¸å¿ƒäººæ ¼ä¿¡æ¯
            personality_info = {}
            if hasattr(self.personality_core, 'core_data') and self.personality_core.core_data:
                personality_info = {
                    "core_loaded": True,
                    "current_mood": self.personality_core.current_mood,
                    "character_name": self.personality_core.get_character_identity()['name'].get('zh', 'éœ²è¥¿å©­'),
                    "personality_traits": len(self.personality_core.get_personality_traits()['primary_traits'])
                }
            else:
                personality_info = {"core_loaded": False}
            
            return {
                "llm_model": llm_info,
                "embedding_model": embedding_info,
                "personality_core": personality_info
            }
            
        except Exception as e:
            self.logger.error(f"ç²å–æ¨¡å‹ä¿¡æ¯å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def cleanup(self):
        """æ¸…ç†æ¨¡å‹è³‡æº"""
        if self.llm_model:
            del self.llm_model
        if self.embedding_model:
            del self.embedding_model
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        gc.collect()
        self.logger.info("æ¨¡å‹è³‡æºå·²æ¸…ç†")
    
    def _determine_search_focus(self, intent_category: str) -> str:
        """ç¢ºå®šæª¢ç´¢é‡é»"""
        focus_mapping = {
            'companionship_request': 'companionship_responses',  # æª¢ç´¢é™ªä¼´ç›¸é—œçš„å›æ‡‰
            'emotional_support': 'comfort_expressions',         # æª¢ç´¢å®‰æ…°è¡¨é”
            'question_asking': 'helpful_responses',             # æª¢ç´¢æ¨‚æ–¼åŠ©äººçš„å›æ‡‰
            'casual_chat': 'casual_interactions',               # æª¢ç´¢æ—¥å¸¸äº’å‹•
            'intimate_expression': 'intimate_responses',        # æª¢ç´¢è¦ªå¯†å›æ‡‰
            'unknown': 'general_responses'                      # æª¢ç´¢é€šç”¨å›æ‡‰
        }
        return focus_mapping.get(intent_category, 'general_responses')

    def _get_emotional_search_priority(self, emotional_state: str) -> Dict[str, Any]:
        """æ ¹æ“šæƒ…æ„Ÿç‹€æ…‹ç¢ºå®šæª¢ç´¢å„ªå…ˆç´š"""
        priority_map = {
            'very_strong': {
                'top_k': 8,
                'category': 'content',
                'keywords': ['å¼·çƒˆè² é¢æƒ…ç·’', 'æº«æŸ”å®‰æ…°', 'å¿ƒç–¼é—œå¿ƒ']
            },
            'moderate': {
                'top_k': 6,
                'category': 'content', 
                'keywords': ['ä¸­ç­‰è² é¢æƒ…ç·’', 'æº«æŸ”å›æ‡‰', 'é—œå¿ƒç†è§£']
            },
            'mild': {
                'top_k': 5,
                'category': 'content',
                'keywords': ['å¹³éœæƒ…ç·’', 'è‡ªç„¶è¦ªåˆ‡', 'å¯æ„›é¢¨æ ¼']
            }
        }
        return priority_map.get(emotional_state, priority_map['mild'])

    def _get_intimacy_style_filter(self, intimacy_score: float) -> Dict[str, Any]:
        """æ ¹æ“šè¦ªå¯†åº¦ç²å–é¢¨æ ¼éæ¿¾å™¨"""
        if intimacy_score > 2.0:
            return {
                'style': 'very_intimate',
                'keywords': ['è¦ªæ„›çš„', 'å¯¶è²', 'æ„›ä½ ', 'æˆ€äºº', 'æ’’å¬Œ', 'ç”œèœœ']
            }
        elif intimacy_score > 1.0:
            return {
                'style': 'warm_close',
                'keywords': ['è¦ªè¿‘', 'æº«æš–', 'é»äºº', 'å¯æ„›', 'é—œå¿ƒ']
            }
        else:
            return {
                'style': 'friendly',
                'keywords': ['å‹å–„', 'è¦ªåˆ‡', 'ç¦®è²Œ', 'è‡ªç„¶']
            }

    def _build_enhanced_query(self, user_input: str, search_focus: str, emotional_state: str) -> str:
        """æ§‹å»ºå¢å¼·æª¢ç´¢æŸ¥è©¢"""
        
        # åŸºæ–¼æª¢ç´¢é‡é»æ·»åŠ é—œéµè©
        focus_keywords = {
            'companionship_responses': 'é™ªä¼´ ä¸€èµ· æº«æš–',
            'comfort_expressions': 'å®‰æ…° æº«æŸ” é—œå¿ƒ',
            'helpful_responses': 'å¹«åŠ© è§£ç­” æ¨‚æ„',
            'casual_interactions': 'èŠå¤© è¼•é¬† å¯æ„›',
            'intimate_responses': 'è¦ªå¯† æ’’å¬Œ ç”œèœœ',
            'general_responses': 'å›æ‡‰ äº’å‹•'
        }
        
        # åŸºæ–¼æƒ…æ„Ÿç‹€æ…‹æ·»åŠ ä¿®é£¾è©
        emotional_modifiers = {
            'very_strong': 'ç‰¹åˆ¥æº«æŸ” æ·±åº¦é—œæ‡·',
            'moderate': 'æº«æŸ” é—œå¿ƒ',
            'mild': 'è¦ªåˆ‡ è‡ªç„¶'
        }
        
        enhanced_query = f"{user_input} {focus_keywords.get(search_focus, '')} {emotional_modifiers.get(emotional_state, '')}"
        return enhanced_query.strip()

    def _filter_by_response_style(self, search_results: List, response_style_filter: Dict[str, Any]) -> List:
        """æ ¹æ“šå›æ‡‰é¢¨æ ¼éæ¿¾æª¢ç´¢çµæœ"""
        if not search_results:
            return []
        
        filtered_results = []
        style_keywords = response_style_filter.get('keywords', [])
        
        for result in search_results:
            content = result['content'].lower()
            
            # è¨ˆç®—é¢¨æ ¼åŒ¹é…åº¦
            style_match_count = sum(1 for keyword in style_keywords if keyword in content)
            
            # æ·»åŠ é¢¨æ ¼åŒ¹é…åˆ†æ•¸
            result['style_score'] = style_match_count / len(style_keywords) if style_keywords else 0.5
            
            # åªä¿ç•™æœ‰ä¸€å®šåŒ¹é…åº¦çš„çµæœ
            if result['style_score'] > 0.1 or result['similarity'] > 0.7:
                filtered_results.append(result)
        
        # æŒ‰é¢¨æ ¼åŒ¹é…åº¦å’Œç›¸ä¼¼åº¦æ’åº
        filtered_results.sort(key=lambda x: (x['style_score'], x['similarity']), reverse=True)
        
        return filtered_results[:5]

    def _build_diverse_context(self, search_results: List, emotional_analysis: Dict) -> str:
        """æ§‹å»ºå¤šæ¨£åŒ–ä¸Šä¸‹æ–‡"""
        if not search_results:
            return ""
        
        context_parts = []
        
        # çµ±è¨ˆçµæœä¾†æº
        file_sources = {}
        for result in search_results:
            filename = result['metadata'].get('filename', 'unknown')
            if filename not in file_sources:
                file_sources[filename] = []
            file_sources[filename].append(result)
        
        # è¨˜éŒ„å¤šæ¨£æ€§ä¿¡æ¯
        self.logger.info(f"ğŸ¯ ä¸Šä¸‹æ–‡ä¾†æºå¤šæ¨£æ€§: {len(file_sources)} å€‹æ–‡ä»¶")
        for filename, results in file_sources.items():
            self.logger.info(f"   ğŸ“„ {filename}: {len(results)} å€‹ç‰‡æ®µ")
        
        # æ§‹å»ºåˆ†é¡ä¸Šä¸‹æ–‡
        for result in search_results[:6]:  # æœ€å¤š6å€‹çµæœ
            content = result['content'].strip()
            metadata = result['metadata']
            
            # æ§‹å»ºä¾†æºä¿¡æ¯
            source_info = []
            if metadata.get('category'):
                source_info.append(f"é¡åˆ¥: {metadata['category']}")
            if metadata.get('section_title'):
                source_info.append(f"ç« ç¯€: {metadata['section_title']}")
            if metadata.get('filename'):
                source_info.append(f"æ–‡ä»¶: {metadata['filename']}")
            
            source_str = " | ".join(source_info) if source_info else "æœªçŸ¥ä¾†æº"
            similarity = result['similarity']
            
            # æ¨™è¨˜æ˜¯å¦ç‚ºè£œå……çµæœ
            result_type = "è£œå……" if result.get('supplementary', False) else "ä¸»è¦"
            
            context_parts.append(f"[{result_type}] [{source_str} | ç›¸é—œåº¦: {similarity:.3f}]\n{content}")
        
        return "\n\n---\n\n".join(context_parts)
    
    def _build_enhanced_prompt_with_history(
        self, 
        user_input: str, 
        context: Optional[str] = None,
        conversation_history: Optional[List[tuple]] = None,
        emotional_analysis: Optional[Dict[str, Any]] = None,
        response_hints: Optional[Dict[str, Any]] = None
    ) -> str:
        """æ§‹å»ºåŒ…å«å°è©±æ­·å²çš„å¢å¼·ç‰ˆæç¤ºè©"""
        prompt_parts = [self.system_prompt]
        
        # ğŸ”¥ æ–°å¢ï¼šæ·»åŠ å°è©±æ­·å²ä¸Šä¸‹æ–‡
        if conversation_history and len(conversation_history) > 0:
            history_context = self._build_conversation_context(conversation_history)
            prompt_parts.append(f"\n<|conversation_history|>\n{history_context}\n<|end_history|>")
        
        # æ·»åŠ èªç¾©åˆ†æä¸Šä¸‹æ–‡
        if context:
            prompt_parts.append(f"\n<|context|>\n{context}\n<|end_context|>")
        
        # æ·»åŠ æƒ…æ„Ÿç†è§£ä¿¡æ¯ï¼ˆä½¿ç”¨äººæ€§åŒ–èªè¨€ï¼‰
        if emotional_analysis and emotional_analysis.get('emotional_intensity') != 'mild':
            emotion_guidance = emotional_analysis.get('response_guidance', '')
            intimacy_guidance = emotional_analysis.get('intimacy_guidance', '')
            intent_guidance = emotional_analysis.get('intent_guidance', '')
            
            human_guidance = []
            if emotion_guidance:
                human_guidance.append(f"æƒ…æ„Ÿç†è§£ï¼š{emotion_guidance}")
            if intimacy_guidance:
                human_guidance.append(f"äº’å‹•æ–¹å¼ï¼š{intimacy_guidance}")
            if intent_guidance:
                human_guidance.append(f"å›æ‡‰é‡é»ï¼š{intent_guidance}")
            
            if human_guidance:
                guidance_text = "ã€".join(human_guidance)
                prompt_parts.append(f"\n<|guidance|>{guidance_text}<|end_guidance|>")
        
        # æ·»åŠ å›æ‡‰é¢¨æ ¼æç¤ºï¼ˆäººæ€§åŒ–è¡¨é”ï¼‰
        if response_hints and response_hints.get('response_style') != 'normal':
            style_hint = ""
            if response_hints['response_style'] == 'intense':
                style_hint = "éœ²è¥¿äºç¾åœ¨éœ€è¦è¡¨ç¾å¾—ç‰¹åˆ¥æº«æŸ”é«”è²¼ï¼Œç”¨æœ€é—œå¿ƒçš„èªæ°£å›æ‡‰"
            elif response_hints['response_style'] == 'moderate':
                style_hint = "éœ²è¥¿äºè¦æ¯”å¹³å¸¸æ›´æº«æš–ä¸€äº›ï¼Œè¡¨ç¾å‡ºé—œå¿ƒå’Œç†è§£"
            
            if response_hints.get('special_phrases'):
                style_hint += f"ï¼Œå¯ä»¥ä½¿ç”¨é€™äº›è¡¨é”æ–¹å¼ï¼š{', '.join(response_hints['special_phrases'])}"
            
            if style_hint:
                prompt_parts.append(f"\n<|style|>{style_hint}<|end_style|>")
        
        prompt_parts.append(f"\n<|user|>{user_input}<|end|>")
        
        # ç²å–è§’è‰²åç¨± - å®Œå…¨å¾ core.json ç²å–
        character_name = "AIåŠ©æ‰‹"  # é»˜èªå›é€€åç¨±
        if hasattr(self.personality_core, 'core_data') and self.personality_core.core_data:
            character_name = self.personality_core.get_character_identity()['name'].get('zh', 'éœ²è¥¿å©­')
        
        prompt_parts.append(f"\n<|assistant|>{character_name}ï¼š")
        
        return "\n".join(prompt_parts)
    
    def _build_conversation_context(self, conversation_history: List[tuple]) -> str:
        """æ§‹å»ºå°è©±æ­·å²ä¸Šä¸‹æ–‡ï¼ˆä¿ç•™æœ€è¿‘7è¼ªï¼‰"""
        if not conversation_history:
            return ""
        
        # åªä½¿ç”¨æœ€è¿‘çš„7è¼ªå°è©±
        recent_history = conversation_history[-7:]
        
        context_parts = []
        for i, (user_msg, bot_response) in enumerate(recent_history, 1):
            # é™åˆ¶æ¯æ¢æ¶ˆæ¯çš„é•·åº¦ï¼Œé¿å…æç¤ºè©éé•·
            user_msg_short = user_msg[:100] + "..." if len(user_msg) > 100 else user_msg
            bot_response_short = bot_response[:150] + "..." if len(bot_response) > 150 else bot_response
            
            context_parts.append(f"ç¬¬{i}è¼ªå°è©±:")
            context_parts.append(f"ç”¨æˆ¶: {user_msg_short}")
            context_parts.append(f"éœ²è¥¿äº: {bot_response_short}")
            context_parts.append("")  # ç©ºè¡Œåˆ†éš”
        
        return "\n".join(context_parts).strip()
    
    def _get_adaptive_generation_config(self, emotional_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºæ–¼æƒ…æ„Ÿç†è§£å‹•æ…‹èª¿æ•´ç”Ÿæˆåƒæ•¸"""
        
        base_config = {
            "max_new_tokens": self.config['vtuber']['response']['max_tokens'],
            "min_new_tokens": self.config['vtuber']['response'].get('min_tokens', 10),
            "temperature": self.config['models']['llm']['temperature'],
            "top_p": self.config['models']['llm']['top_p'],
            "top_k": self.config['models']['llm']['top_k'],
            "repetition_penalty": self.config['models']['llm'].get('repetition_penalty', 1.15),
            "no_repeat_ngram_size": self.config['models']['llm'].get('no_repeat_ngram_size', 3),
            "length_penalty": self.config['models']['llm'].get('length_penalty', 1.0),
            "early_stopping": self.config['models']['llm'].get('early_stopping', True),
            "do_sample": True,
        }
        
        # åªæœ‰åœ¨tokenizerå·²åˆå§‹åŒ–æ™‚æ‰æ·»åŠ tokenç›¸é—œé…ç½®
        if self.llm_tokenizer is not None:
            base_config["pad_token_id"] = self.llm_tokenizer.pad_token_id
            base_config["eos_token_id"] = self.llm_tokenizer.eos_token_id
        
        # æ ¹æ“šæƒ…æ„Ÿå¼·åº¦èª¿æ•´å‰µé€ æ€§
        emotional_intensity = emotional_analysis.get('emotional_intensity', 'mild')
        
        if emotional_intensity == 'very_strong':
            # å¼·çƒˆæƒ…æ„Ÿéœ€è¦æ›´æœ‰å‰µé€ æ€§å’Œè¡¨é”åŠ›çš„å›æ‡‰
            base_config["temperature"] = min(0.8, base_config["temperature"] + 0.2)
            base_config["top_p"] = min(0.9, base_config["top_p"] + 0.1)
            base_config["max_new_tokens"] = min(200, base_config["max_new_tokens"] + 50)  # 150+50=200 â‰ˆ 100å­—
            
        elif emotional_intensity == 'moderate':
            # ä¸­ç­‰æƒ…æ„Ÿéœ€è¦é©åº¦çš„è¡¨é”åŠ›
            base_config["temperature"] = min(0.7, base_config["temperature"] + 0.1)
            base_config["max_new_tokens"] = min(180, base_config["max_new_tokens"] + 30)  # 150+30=180 â‰ˆ 90å­—  
        
        # æ ¹æ“šè¦ªå¯†åº¦èª¿æ•´å›æ‡‰é•·åº¦å’Œç´°è†©åº¦
        intimacy_score = emotional_analysis.get('intimacy_score', 0.0)
        
        if intimacy_score > 2.0:
            # é«˜è¦ªå¯†åº¦éœ€è¦æ›´é•·æ›´ç´°è†©çš„å›æ‡‰ï¼Œä½†æ§åˆ¶åœ¨100å­—å…§
            base_config["max_new_tokens"] = min(200, base_config["max_new_tokens"] + 30)  # æœ€å¤š200 â‰ˆ 100å­—
            base_config["min_new_tokens"] = max(25, base_config["min_new_tokens"] + 5)   
            base_config["repetition_penalty"] = max(1.05, base_config["repetition_penalty"] - 0.1)
        
        # æ ¹æ“šæ„åœ–é¡å‹èª¿æ•´æº–ç¢ºæ€§
        detected_intent = emotional_analysis.get('detected_intent', '')
        
        if 'question' in detected_intent or detected_intent == 'asking_info':
            # å•é¡Œé¡éœ€è¦è©³ç´°ä½†æº–ç¢ºçš„å›æ‡‰ï¼Œæ§åˆ¶åœ¨100å­—å…§
            base_config["temperature"] = max(0.5, base_config["temperature"] - 0.1)  # è¼•å¾®é™ä½å‰µé€ æ€§
            base_config["top_k"] = max(30, base_config["top_k"] - 10)                # è¼•å¾®æ¸›å°‘å€™é¸è©
            
            # å¢åŠ å•é¡Œé¡å›æ‡‰çš„tokenæ•¸é‡ä½†ä¸è¶…éé™åˆ¶
            base_config["max_new_tokens"] = min(200, base_config["max_new_tokens"] + 30)  # æœ€å¤š200 â‰ˆ 100å­—
            base_config["min_new_tokens"] = max(30, base_config["min_new_tokens"] + 20)   # 15+20=35
            base_config["repetition_penalty"] = max(1.05, base_config["repetition_penalty"] - 0.1)  # æ¸›å°‘é‡è¤‡æ‡²ç½°
            
        elif detected_intent == 'intimate_expression':
            # è¦ªå¯†è¡¨é”éœ€è¦æ›´æœ‰æƒ…æ„Ÿçš„å›æ‡‰
            base_config["temperature"] = min(0.75, base_config["temperature"] + 0.15)
            base_config["repetition_penalty"] = max(1.1, base_config["repetition_penalty"] - 0.05)
        
        # ğŸ”¥ æ–°å¢ï¼šåŸºæ–¼å›æ‡‰æœŸæœ›çš„å‹•æ…‹èª¿æ•´
        response_expectation = emotional_analysis.get('response_expectation', 'normal')
        
        if response_expectation == 'detailed':
            # è©³ç´°å›æ‡‰ï¼šå•é¡Œã€å®‰æ…°ã€è¤‡é›œè©±é¡Œï¼Œä½†æ§åˆ¶åœ¨100å­—å…§
            base_config["max_new_tokens"] = min(200, base_config["max_new_tokens"] + 40)  # æœ€å¤š200 â‰ˆ 100å­—
            base_config["min_new_tokens"] = max(30, base_config["min_new_tokens"] + 10)
            base_config["temperature"] = min(0.75, base_config["temperature"] + 0.05)  # ç¨å¾®å¢åŠ è¡¨é”åŠ›
            
        elif response_expectation == 'short':
            # ç°¡çŸ­å›æ‡‰ï¼šç¢ºèªã€ç°¡å–®å•å€™
            base_config["max_new_tokens"] = max(200, base_config["max_new_tokens"] - 50)
            base_config["min_new_tokens"] = max(10, base_config["min_new_tokens"] - 5)
        
        # normal ä¿æŒåŸºç¤é…ç½®ä¸è®Š
        
        return base_config
    
    def _create_context_hints(self, emotional_analysis: Dict[str, Any], conversation_history: Optional[List[tuple]] = None) -> Dict[str, Any]:
        """å‰µå»ºä¸Šä¸‹æ–‡æç¤ºï¼ˆç”¨æ–¼å‹•æ…‹ç³»çµ±æç¤ºè©ï¼‰"""
        context_hints = {}
        
        # 1. æƒ…æ„Ÿç‹€æ…‹æç¤º
        emotional_intensity = emotional_analysis.get('emotional_intensity', 'mild')
        if emotional_intensity == 'very_strong':
            context_hints['emotional_state'] = 'sad'  # éœ€è¦å®‰æ…°
        elif emotional_intensity == 'moderate':
            context_hints['emotional_state'] = 'excited'  # æœ‰æƒ…ç·’æ³¢å‹•
        else:
            context_hints['emotional_state'] = 'calm'  # å¹³éœ
        
        # 2. å°è©±ä¸»é¡Œé¡å‹æ¨æ¸¬
        detected_intent = emotional_analysis.get('detected_intent', 'unknown')
        if detected_intent in ['question_asking', 'asking_info']:
            context_hints['topic_type'] = 'casual'
        elif detected_intent == 'intimate_expression':
            context_hints['topic_type'] = 'emotional'
        elif detected_intent == 'companionship_request':
            context_hints['topic_type'] = 'daily_life'
        elif self.personality_core.current_mood == 'gaming':
            context_hints['topic_type'] = 'gaming'
        else:
            context_hints['topic_type'] = 'casual'
        
        # 3. å°è©±æ­·å²åˆ†æ
        if conversation_history and len(conversation_history) > 0:
            # åˆ†æå°è©±é•·åº¦å’Œé »ç‡
            if len(conversation_history) > 5:
                context_hints['conversation_depth'] = 'deep'
            elif len(conversation_history) > 2:
                context_hints['conversation_depth'] = 'moderate'
            else:
                context_hints['conversation_depth'] = 'new'
            
            # åˆ†ææœ€è¿‘çš„äº¤äº’æ¨¡å¼
            recent_user_messages = [msg[0] for msg in conversation_history[-3:]]
            total_length = sum(len(msg) for msg in recent_user_messages)
            avg_length = total_length / len(recent_user_messages) if recent_user_messages else 0
            
            if avg_length > 50:
                context_hints['interaction_style'] = 'detailed'
            elif avg_length > 20:
                context_hints['interaction_style'] = 'normal'
            else:
                context_hints['interaction_style'] = 'brief'
        
        # 4. æ™‚é–“ä¸Šä¸‹æ–‡
        import datetime
        current_hour = datetime.datetime.now().hour
        
        if 6 <= current_hour <= 11:
            context_hints['time_period'] = 'morning'
        elif 12 <= current_hour <= 17:
            context_hints['time_period'] = 'afternoon'
        elif 18 <= current_hour <= 22:
            context_hints['time_period'] = 'evening'
        else:
            context_hints['time_period'] = 'night'
        
        return context_hints
    
    def reset_conversation_count(self):
        """é‡ç½®å°è©±è¨ˆæ•¸å™¨ï¼ˆç”¨æ–¼æ–°å°è©±é–‹å§‹ï¼‰"""
        self.conversation_count = 0
        self.logger.info("å°è©±è¨ˆæ•¸å™¨å·²é‡ç½®")
    
    def get_conversation_stats(self) -> Dict[str, Any]:
        """ç²å–å°è©±çµ±è¨ˆä¿¡æ¯"""
        return {
            'conversation_count': self.conversation_count,
            'current_mood': self.personality_core.current_mood,
            'intimacy_level': getattr(self.personality_core, 'intimacy_level', 0.0),
            'semantic_enabled': getattr(self.personality_core, '_semantic_enabled', False)
        }