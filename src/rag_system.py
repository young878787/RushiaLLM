"""
RAG (æª¢ç´¢å¢å¼·ç”Ÿæˆ) ç³»çµ±
è² è²¬æ–‡æª”è™•ç†ã€å‘é‡å­˜å„²å’Œæª¢ç´¢
"""

import logging
import os
from typing import List, Dict, Any, Optional
from pathlib import Path
import asyncio

import chromadb
from chromadb.config import Settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    Docx2txtLoader,
)
import numpy as np


class RAGSystem:
    def __init__(self, config: dict, embedding_model):
        self.config = config
        self.embedding_model = embedding_model
        self.logger = logging.getLogger(__name__)
        
        # RAG é…ç½®
        self.rag_config = config['rag']
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.rag_config['retrieval']['chunk_size'],
            chunk_overlap=self.rag_config['retrieval']['chunk_overlap'],
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " "]
        )
        
        # å‘é‡æ•¸æ“šåº«
        self.chroma_client = None
        self.collection = None
        
        # èªç¾©å‘é‡å¢å¼·ç³»çµ±
        self.semantic_vectors = {}
        self.vector_cache_initialized = False
        
        # å¤šæ¨£æ€§æ§åˆ¶é…ç½®
        self.diversity_config = {
            'max_per_file': 2,  # æ¯å€‹æ–‡ä»¶æœ€å¤š2å€‹çµæœ
            'min_files': 3,     # è‡³å°‘ä¾†è‡ª3å€‹ä¸åŒæ–‡ä»¶
            'semantic_analysis_limit': 1  # semantic_analysis.mdæœ€å¤š1å€‹çµæœ
        }
        
        # Rushia Knowledge åˆ†é¡ç³»çµ± (å¹³è¡¡æ¬Šé‡)
        self.knowledge_categories = {
            'core_identity': {'priority': 'high', 'weight': 1.4},
            'vocabulary': {'priority': 'high', 'weight': 1.3},
            'content': {'priority': 'high', 'weight': 1.4},  # é™ä½contentå„ªå…ˆç´š
            'relationships': {'priority': 'medium', 'weight': 1.2},
            'timeline': {'priority': 'low', 'weight': 1.0}
        }
    
    async def initialize(self):
        """åˆå§‹åŒ– RAG ç³»çµ±"""
        await self._setup_vector_db()
        await self._load_existing_documents()
        await self._load_rushia_knowledge()
        await self._initialize_semantic_vectors()
    
    async def _setup_vector_db(self):
        """è¨­ç½®å‘é‡æ•¸æ“šåº«"""
        self.logger.info("åˆå§‹åŒ–å‘é‡æ•¸æ“šåº«...")

        # å–å¾— LLM ç›®éŒ„ï¼ˆrag_system.py çš„ä¸Šå…©å±¤ï¼‰
        llm_dir = Path(__file__).parent.parent.resolve()
        persist_dir = llm_dir / Path(self.rag_config['vector_db']['persist_directory'])
        Path(persist_dir).mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ– ChromaDB
        self.chroma_client = chromadb.PersistentClient(
            path=persist_dir,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # ç²å–æˆ–å‰µå»ºé›†åˆ
        collection_name = self.rag_config['vector_db']['collection_name']
        try:
            self.collection = self.chroma_client.get_collection(collection_name)
            self.logger.info(f"è¼‰å…¥ç¾æœ‰é›†åˆ: {collection_name}")
        except:
            self.collection = self.chroma_client.create_collection(
                name=collection_name,
                metadata={"description": "VTuber AI çŸ¥è­˜åº« + Rushia Knowledge"}
            )
            self.logger.info(f"å‰µå»ºæ–°é›†åˆ: {collection_name}")
    
    async def _load_existing_documents(self):
        """è¼‰å…¥ç¾æœ‰æ–‡æª”"""
        docs_dir = Path("data/documents")
        if not docs_dir.exists():
            docs_dir.mkdir(parents=True, exist_ok=True)
            return
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æ–°æ–‡æª”éœ€è¦è™•ç†
        doc_files = list(docs_dir.glob("**/*"))
        doc_files = [f for f in doc_files if f.is_file() and f.suffix.lower() in ['.txt', '.pdf', '.docx']]
        
        if doc_files:
            self.logger.info(f"ç™¼ç¾ {len(doc_files)} å€‹æ–‡æª”ï¼Œé–‹å§‹è™•ç†...")
            for doc_file in doc_files:
                await self.add_document(str(doc_file))
    
    async def _load_rushia_knowledge(self):
        """è¼‰å…¥Rushia Knowledge markdownæ–‡ä»¶"""
        knowledge_dir = Path("rushia_wiki/rushia_knowledge")
        if not knowledge_dir.exists():
            self.logger.warning(f"Rushia Knowledgeç›®éŒ„ä¸å­˜åœ¨: {knowledge_dir}")
            return
        
        # ç²å–æ‰€æœ‰markdownæ–‡ä»¶
        md_files = list(knowledge_dir.glob("**/*.md"))
        
        if md_files:
            self.logger.info(f"ğŸ§  ç™¼ç¾ {len(md_files)} å€‹Rushia Knowledgeæ–‡ä»¶ï¼Œé–‹å§‹æ•´åˆ...")
            
            for md_file in md_files:
                await self._add_knowledge_file(md_file)
            
            self.logger.info("âœ… Rushia Knowledgeæ•´åˆå®Œæˆ")
        else:
            self.logger.warning("æœªæ‰¾åˆ°Rushia Knowledge markdownæ–‡ä»¶")
    
    async def _add_knowledge_file(self, file_path: Path):
        """æ·»åŠ çŸ¥è­˜æ–‡ä»¶åˆ°RAGç³»çµ±"""
        try:
            self.logger.info(f"è™•ç†çŸ¥è­˜æ–‡ä»¶: {file_path.name}")
            
            # è®€å–markdownå…§å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if not content.strip():
                self.logger.warning(f"æ–‡ä»¶å…§å®¹ç‚ºç©º: {file_path}")
                return False
            
            # åˆ†ææ–‡ä»¶é¡å‹å’Œå„ªå…ˆç´š
            category = self._categorize_knowledge_file(file_path)
            priority_info = self.knowledge_categories.get(category, {'priority': 'medium', 'weight': 1.0})
            
            # æ™ºèƒ½åˆ†å¡Š
            chunks = self._smart_chunk_markdown(content, file_path)
            
            if not chunks:
                return False
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            texts = [chunk['content'] for chunk in chunks]
            embeddings = self.embedding_model.encode(texts).tolist()
            
            # æº–å‚™æ•¸æ“š
            ids = [f"rushia_{file_path.stem}_{i}" for i in range(len(chunks))]
            metadatas = []
            
            for i, chunk in enumerate(chunks):
                metadata = {
                    'source': str(file_path),
                    'filename': file_path.name,
                    'file_type': 'markdown',
                    'category': category,
                    'priority': priority_info['priority'],
                    'weight': priority_info['weight'],
                    'chunk_id': i,
                    'total_chunks': len(chunks),
                    'knowledge_type': 'rushia_knowledge',
                    **chunk.get('metadata', {})
                }
                metadatas.append(metadata)
            
            # æ·»åŠ åˆ°å‘é‡æ•¸æ“šåº«
            self.collection.add(
                embeddings=embeddings,
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
            
            self.logger.info(f"âœ… æˆåŠŸæ·»åŠ  {len(chunks)} å€‹çŸ¥è­˜å¡Š: {file_path.name} ({category})")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ·»åŠ çŸ¥è­˜æ–‡ä»¶å¤±æ•— {file_path}: {e}")
            return False
    
    def _categorize_knowledge_file(self, file_path: Path) -> str:
        """æ ¹æ“šæ–‡ä»¶è·¯å¾‘åˆ†é¡çŸ¥è­˜æ–‡ä»¶"""
        path_parts = file_path.parts
        
        for part in path_parts:
            if part in self.knowledge_categories:
                return part
        
        return 'general'
    
    def _smart_chunk_markdown(self, content: str, file_path: Path) -> List[Dict[str, Any]]:
        """æ™ºèƒ½åˆ†å¡Šmarkdownå…§å®¹"""
        chunks = []
        
        # æŒ‰æ¨™é¡Œåˆ†å‰²
        sections = self._split_by_headers(content)
        
        for section in sections:
            if len(section['content']) > self.rag_config['retrieval']['chunk_size']:
                # å¤§æ®µè½éœ€è¦é€²ä¸€æ­¥åˆ†å‰²
                sub_chunks = self.text_splitter.split_text(section['content'])
                for i, sub_chunk in enumerate(sub_chunks):
                    chunks.append({
                        'content': sub_chunk,
                        'metadata': {
                            'section_title': section['title'],
                            'section_level': section['level'],
                            'sub_chunk': i
                        }
                    })
            else:
                # å°æ®µè½ä¿æŒå®Œæ•´
                chunks.append({
                    'content': section['content'],
                    'metadata': {
                        'section_title': section['title'],
                        'section_level': section['level']
                    }
                })
        
        return chunks
    
    def _split_by_headers(self, content: str) -> List[Dict[str, Any]]:
        """æŒ‰markdownæ¨™é¡Œåˆ†å‰²å…§å®¹"""
        lines = content.split('\n')
        sections = []
        current_section = {'title': '', 'level': 0, 'content': ''}
        
        for line in lines:
            if line.startswith('#'):
                # ä¿å­˜å‰ä¸€å€‹section
                if current_section['content'].strip():
                    sections.append(current_section.copy())
                
                # é–‹å§‹æ–°section
                level = len(line) - len(line.lstrip('#'))
                title = line.lstrip('#').strip()
                current_section = {
                    'title': title,
                    'level': level,
                    'content': line + '\n'
                }
            else:
                current_section['content'] += line + '\n'
        
        # æ·»åŠ æœ€å¾Œä¸€å€‹section
        if current_section['content'].strip():
            sections.append(current_section)
        
        return sections
    
    async def add_document(self, file_path: str) -> bool:
        """æ·»åŠ æ–‡æª”åˆ°çŸ¥è­˜åº«"""
        try:
            self.logger.info(f"è™•ç†æ–‡æª”: {file_path}")
            
            # è¼‰å…¥æ–‡æª”
            documents = await self._load_document(file_path)
            if not documents:
                return False
            
            # åˆ†å‰²æ–‡æœ¬
            chunks = []
            for doc in documents:
                doc_chunks = self.text_splitter.split_text(doc['content'])
                for i, chunk in enumerate(doc_chunks):
                    chunks.append({
                        'content': chunk,
                        'metadata': {
                            **doc['metadata'],
                            'chunk_id': i,
                            'total_chunks': len(doc_chunks)
                        }
                    })
            
            if not chunks:
                return False
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            texts = [chunk['content'] for chunk in chunks]
            embeddings = self.embedding_model.encode(texts).tolist()
            
            # æº–å‚™æ•¸æ“š
            ids = [f"{Path(file_path).stem}_{i}" for i in range(len(chunks))]
            metadatas = [chunk['metadata'] for chunk in chunks]
            
            # æ·»åŠ åˆ°å‘é‡æ•¸æ“šåº«
            self.collection.add(
                embeddings=embeddings,
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
            
            self.logger.info(f"âœ… æˆåŠŸæ·»åŠ  {len(chunks)} å€‹æ–‡æœ¬å¡Šåˆ°çŸ¥è­˜åº«")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ·»åŠ æ–‡æª”å¤±æ•—: {e}")
            return False
    
    async def _load_document(self, file_path: str) -> List[Dict[str, Any]]:
        """è¼‰å…¥æ–‡æª”å…§å®¹"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            self.logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return []
        
        try:
            if file_path.suffix.lower() == '.pdf':
                loader = PyPDFLoader(str(file_path))
            elif file_path.suffix.lower() == '.docx':
                loader = Docx2txtLoader(str(file_path))
            elif file_path.suffix.lower() == '.txt':
                loader = TextLoader(str(file_path), encoding='utf-8')
            else:
                self.logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
                return []
            
            docs = loader.load()
            
            result = []
            for doc in docs:
                result.append({
                    'content': doc.page_content,
                    'metadata': {
                        'source': str(file_path),
                        'filename': file_path.name,
                        'file_type': file_path.suffix.lower(),
                        'category': 'traditional_document',
                        'priority': 'medium',
                        'weight': 1.0,
                        'knowledge_type': 'traditional_document',
                        **doc.metadata
                    }
                })
            
            return result
            
        except Exception as e:
            self.logger.error(f"è¼‰å…¥æ–‡æª”å¤±æ•— {file_path}: {e}")
            return []
    
    async def semantic_enhanced_search(self, query: str, emotional_analysis: Dict[str, Any] = None, top_k: Optional[int] = None) -> List[Dict[str, Any]]:
        """èªç¾©å‘é‡å¢å¼·æœç´¢ - åŸºæ–¼å‘é‡èª¿æ•´çš„æ™ºèƒ½æª¢ç´¢"""
        if top_k is None:
            top_k = self.rag_config['retrieval']['top_k']
        
        try:
            # ç¢ºä¿èªç¾©å‘é‡å·²åˆå§‹åŒ–
            if not self.vector_cache_initialized:
                await self._initialize_semantic_vectors()
            
            # ç²å–åŸå§‹æŸ¥è©¢å‘é‡
            query_vector = self.embedding_model.encode([query])[0]
            
            # æ ¹æ“šæƒ…æ„Ÿåˆ†æèª¿æ•´æŸ¥è©¢å‘é‡
            adjusted_vector = self._adjust_query_vector(query_vector, emotional_analysis)
            
            # åŸ·è¡Œå¤šå‘é‡æœç´¢
            results = await self._multi_vector_search(adjusted_vector, emotional_analysis, top_k)
            
            # ç¢ºä¿çµæœå¤šæ¨£æ€§
            diverse_results = self._ensure_result_diversity(results)
            
            # è¨˜éŒ„èªç¾©æœç´¢æ—¥èªŒ
            self._log_semantic_search_results(query, emotional_analysis, diverse_results)
            
            return diverse_results[:top_k]
            
        except Exception as e:
            self.logger.error(f"èªç¾©å¢å¼·æœç´¢å¤±æ•—: {e}")
            # å›é€€åˆ°æ¨™æº–æœç´¢
            return await self.search(query, top_k)

    async def search(self, query: str, top_k: Optional[int] = None, category_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """å¢å¼·ç‰ˆæœç´¢ - æ”¯æ´åˆ†é¡éæ¿¾å’Œå„ªå…ˆç´šæ¬Šé‡"""
        if top_k is None:
            top_k = self.rag_config['retrieval']['top_k']
        
        try:
            # ç”ŸæˆæŸ¥è©¢åµŒå…¥
            query_embedding = self.embedding_model.encode([query]).tolist()[0]
            
            # æ§‹å»ºéæ¿¾æ¢ä»¶
            where_filter = {}
            if category_filter:
                where_filter['category'] = category_filter
            
            # æœç´¢ (ç²å–æ›´å¤šçµæœç”¨æ–¼é‡æ–°æ’åº)
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k * 2,
                include=['documents', 'metadatas', 'distances'],
                where=where_filter if where_filter else None
            )
            
            # æ ¼å¼åŒ–å’Œé‡æ–°æ’åºçµæœ
            search_results = []
            if results['documents'] and results['documents'][0]:
                for i in range(len(results['documents'][0])):
                    similarity = 1 - results['distances'][0][i]
                    metadata = results['metadatas'][0][i]
                    
                    # éæ¿¾ä½ç›¸ä¼¼åº¦çµæœ
                    if similarity >= self.rag_config['retrieval']['similarity_threshold']:
                        # è¨ˆç®—åŠ æ¬Šåˆ†æ•¸
                        weight = metadata.get('weight', 1.0)
                        weighted_score = similarity * weight
                        
                        search_results.append({
                            'content': results['documents'][0][i],
                            'metadata': metadata,
                            'similarity': similarity,
                            'weighted_score': weighted_score
                        })
            
            # æŒ‰åŠ æ¬Šåˆ†æ•¸æ’åº
            search_results.sort(key=lambda x: x['weighted_score'], reverse=True)
            
            # è¿”å›top_kçµæœ
            final_results = search_results[:top_k]
            
            # è©³ç´°çš„æœç´¢çµæœæ—¥èªŒ
            if final_results:
                sources = list(set([r['metadata'].get('filename', 'unknown') for r in final_results]))
                categories = list(set([r['metadata'].get('category', 'unknown') for r in final_results]))
                similarities = [r['similarity'] for r in final_results]
                
                self.logger.info(f"ğŸ” å¢å¼·æœç´¢å®Œæˆ: {len(final_results)} å€‹çµæœ")
                self.logger.info(f"   ğŸ“š ä¾†æºæ–‡ä»¶: {', '.join(sources)}")
                self.logger.info(f"   ğŸ“‚ æ¶‰åŠé¡åˆ¥: {', '.join(categories)}")
                self.logger.info(f"   ğŸ“Š ç›¸ä¼¼åº¦ç¯„åœ: {min(similarities):.3f} - {max(similarities):.3f}")
                if category_filter:
                    self.logger.info(f"   ğŸ¯ é¡åˆ¥éæ¿¾: {category_filter}")
            else:
                self.logger.info(f"ğŸ” å¢å¼·æœç´¢å®Œæˆ: 0 å€‹çµæœ (æŸ¥è©¢: '{query}')")
                self.logger.info(f"   âš ï¸ å¯èƒ½åŸå› : ç›¸ä¼¼åº¦ä½æ–¼é–¾å€¼ {self.rag_config['retrieval']['similarity_threshold']}")
            
            return final_results
            
        except Exception as e:
            self.logger.error(f"æœç´¢å¤±æ•—: {e}")
            return []
    
    async def get_context_for_query(self, query: str) -> str:
        """ç‚ºæŸ¥è©¢ç²å–å¢å¼·ç‰ˆä¸Šä¸‹æ–‡ (ä¸è¨˜éŒ„æ—¥èªŒï¼Œé¿å…é‡è¤‡)"""
        try:
            # ç”ŸæˆæŸ¥è©¢åµŒå…¥
            query_embedding = self.embedding_model.encode([query]).tolist()[0]
            
            # æœç´¢ (ä¸ä½¿ç”¨ search æ–¹æ³•ï¼Œé¿å…é‡è¤‡æ—¥èªŒ)
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=5 * 2,
                include=['documents', 'metadatas', 'distances']
            )
            
            # æ ¼å¼åŒ–çµæœ (ç°¡åŒ–ç‰ˆï¼Œä¸è¨˜éŒ„æ—¥èªŒ)
            search_results = []
            if results['documents'] and results['documents'][0]:
                for i in range(len(results['documents'][0])):
                    similarity = 1 - results['distances'][0][i]
                    metadata = results['metadatas'][0][i]
                    
                    if similarity >= self.rag_config['retrieval']['similarity_threshold']:
                        weight = metadata.get('weight', 1.0)
                        weighted_score = similarity * weight
                        
                        search_results.append({
                            'content': results['documents'][0][i],
                            'metadata': metadata,
                            'similarity': similarity,
                            'weighted_score': weighted_score
                        })
            
            # æ’åºä¸¦å–å‰5å€‹
            search_results.sort(key=lambda x: x['weighted_score'], reverse=True)
            final_results = search_results[:5]
            
            if not final_results:
                return ""
            
            # çµ„åˆä¸Šä¸‹æ–‡
            context_parts = []
            for result in final_results:
                content = result['content'].strip()
                metadata = result['metadata']
                
                # æ§‹å»ºä¾†æºä¿¡æ¯
                source_info = []
                if metadata.get('category'):
                    source_info.append(f"é¡åˆ¥: {metadata['category']}")
                if metadata.get('section_title'):
                    source_info.append(f"ç« ç¯€: {metadata['section_title']}")
                if metadata.get('filename'):
                    source_info.append(f"æ–‡ä»¶: {metadata['filename']}")
                
                source_str = " | ".join(source_info) if source_info else "æœªçŸ¥ä¾†æº"
                similarity = result['similarity']
                
                context_parts.append(f"[{source_str} | ç›¸é—œåº¦: {similarity:.2f}]\n{content}")
            
            return "\n\n---\n\n".join(context_parts)
            
        except Exception as e:
            self.logger.error(f"ç²å–ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            return ""
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çŸ¥è­˜åº«çµ±è¨ˆä¿¡æ¯"""
        try:
            count = self.collection.count()
            
            # çµ±è¨ˆä¸åŒé¡å‹çš„æ–‡æª”æ•¸é‡
            try:
                all_results = self.collection.get(include=['metadatas'])
                category_counts = {}
                knowledge_type_counts = {}
                
                if all_results['metadatas']:
                    for metadata in all_results['metadatas']:
                        category = metadata.get('category', 'unknown')
                        knowledge_type = metadata.get('knowledge_type', 'unknown')
                        
                        category_counts[category] = category_counts.get(category, 0) + 1
                        knowledge_type_counts[knowledge_type] = knowledge_type_counts.get(knowledge_type, 0) + 1
                
                return {
                    "total_documents": count,
                    "collection_name": self.rag_config['vector_db']['collection_name'],
                    "category_breakdown": category_counts,
                    "knowledge_type_breakdown": knowledge_type_counts,
                    "knowledge_categories": list(self.knowledge_categories.keys()),
                    "enhanced_features": ["markdown_support", "category_filtering", "priority_weighting"]
                }
            except:
                return {
                    "total_documents": count,
                    "collection_name": self.rag_config['vector_db']['collection_name']
                }
        except Exception as e:
            self.logger.error(f"ç²å–çµ±è¨ˆä¿¡æ¯å¤±æ•—: {e}")
            return {"total_documents": 0, "collection_name": "unknown"}
    
    async def clear_knowledge_base(self):
        """æ¸…ç©ºçŸ¥è­˜åº«"""
        try:
            # åˆªé™¤ç¾æœ‰é›†åˆ
            collection_name = self.rag_config['vector_db']['collection_name']
            self.chroma_client.delete_collection(collection_name)
            
            # é‡æ–°å‰µå»ºé›†åˆ
            self.collection = self.chroma_client.create_collection(
                name=collection_name,
                metadata={"description": "VTuber AI çŸ¥è­˜åº«"}
            )
            
            self.logger.info("çŸ¥è­˜åº«å·²æ¸…ç©º")
            return True
            
        except Exception as e:
            self.logger.error(f"æ¸…ç©ºçŸ¥è­˜åº«å¤±æ•—: {e}")
            return False
    
    def _build_semantic_query(self, query: str, emotional_analysis: Dict[str, Any] = None) -> str:
        """æ§‹å»ºèªç¾©å¢å¼·æŸ¥è©¢"""
        if not emotional_analysis:
            return query
        
        query_parts = [query]
        
        # æ·»åŠ æƒ…æ„Ÿç›¸é—œé—œéµè©
        emotional_intensity = emotional_analysis.get('emotional_intensity', 'mild')
        if emotional_intensity == 'very_strong':
            query_parts.append('å¼·çƒˆè² é¢æƒ…ç·’ æº«æŸ”å®‰æ…° å¿ƒç–¼é—œå¿ƒ')
        elif emotional_intensity == 'moderate':
            query_parts.append('ä¸­ç­‰è² é¢æƒ…ç·’ æº«æŸ”å›æ‡‰ é—œå¿ƒç†è§£')
        elif emotional_intensity == 'mild':
            query_parts.append('å¹³éœæƒ…ç·’ è‡ªç„¶è¦ªåˆ‡ å¯æ„›é¢¨æ ¼')
        
        # æ·»åŠ è¦ªå¯†åº¦ç›¸é—œé—œéµè©
        intimacy_score = emotional_analysis.get('intimacy_score', 0.0)
        if intimacy_score > 2.5:
            query_parts.append('æ·±åº¦è¦ªå¯† æˆ€äººç”œèœœ è¦ªå¯†ç¨±å‘¼')
        elif intimacy_score > 1.5:
            query_parts.append('è¦ªå¯†éšæ®µ æ’’å¬Œé»äºº æº«æš–äº’å‹•')
        elif intimacy_score > 0.5:
            query_parts.append('ç†Ÿæ‚‰éšæ®µ è¦ªåˆ‡å‹å–„ æº«æš–é—œä¿‚')
        else:
            query_parts.append('åˆå§‹éšæ®µ å‹å–„è·é›¢ å»ºç«‹é—œä¿‚')
        
        # æ·»åŠ æ„åœ–ç›¸é—œé—œéµè©
        detected_intent = emotional_analysis.get('detected_intent', '')
        intent_keywords = {
            'companionship_request': 'å°‹æ±‚é™ªä¼´ é–‹å¿ƒé¡˜æ„ ä¸€èµ·äº’å‹•',
            'emotional_support': 'å°‹æ±‚å®‰æ…° æº«æŸ”é«”è²¼ é—œæ‡·ç…§é¡§',
            'intimate_expression': 'è¡¨é”æ„›æ„ å®³ç¾é–‹å¿ƒ ç”œèœœå›æ‡‰',
            'question_asking': 'è©¢å•å•é¡Œ è€å¿ƒè§£ç­” æ¨‚æ–¼åŠ©äºº',
            'casual_chat': 'æ—¥å¸¸é–’èŠ æ´»æ½‘è‡ªç„¶ è¼•é¬†äº’å‹•'
        }
        
        if detected_intent in intent_keywords:
            query_parts.append(intent_keywords[detected_intent])
        
        return ' '.join(query_parts)
    
    def _determine_semantic_search_strategy(self, emotional_analysis: Dict[str, Any] = None) -> Dict[str, Any]:
        """ç¢ºå®šèªç¾©æœç´¢ç­–ç•¥"""
        if not emotional_analysis:
            return {
                'primary_category': 'content',
                'boost_keywords': [],
                'priority_weight': 1.0
            }
        
        # åŸºæ–¼æƒ…æ„Ÿå¼·åº¦ç¢ºå®šç­–ç•¥
        emotional_intensity = emotional_analysis.get('emotional_intensity', 'mild')
        detected_intent = emotional_analysis.get('detected_intent', '')
        intimacy_score = emotional_analysis.get('intimacy_score', 0.0)
        
        strategy = {
            'primary_category': 'content',  # semantic_analysis.md åœ¨ content ç›®éŒ„
            'boost_keywords': [],
            'priority_weight': 1.6  # content é¡åˆ¥çš„æ¬Šé‡
        }
        
        # æ ¹æ“šæƒ…æ„Ÿå¼·åº¦èª¿æ•´
        if emotional_intensity == 'very_strong':
            strategy['boost_keywords'].extend(['å¼·çƒˆè² é¢æƒ…ç·’', 'æº«æŸ”å®‰æ…°', 'æ²’é—œä¿‚', 'æˆ‘åœ¨é€™è£¡'])
            strategy['priority_weight'] = 1.8
        elif emotional_intensity == 'moderate':
            strategy['boost_keywords'].extend(['ä¸­ç­‰è² é¢æƒ…ç·’', 'æº«æŸ”å›æ‡‰', 'é—œå¿ƒ'])
            strategy['priority_weight'] = 1.7
        
        # æ ¹æ“šæ„åœ–èª¿æ•´
        if detected_intent == 'emotional_support':
            strategy['boost_keywords'].extend(['å®‰æ…°', 'é«”è²¼', 'é—œæ‡·'])
        elif detected_intent == 'intimate_expression':
            strategy['boost_keywords'].extend(['æ„›æ„', 'å®³ç¾', 'ç”œèœœ'])
        elif detected_intent == 'companionship_request':
            strategy['boost_keywords'].extend(['é™ªä¼´', 'ä¸€èµ·', 'é–‹å¿ƒ'])
        
        # æ ¹æ“šè¦ªå¯†åº¦èª¿æ•´
        if intimacy_score > 2.0:
            strategy['boost_keywords'].extend(['è¦ªæ„›çš„', 'å¯¶è²', 'æ’’å¬Œ'])
        
        return strategy
    
    async def _multi_tier_search(self, enhanced_query: str, search_strategy: Dict[str, Any], top_k: int) -> List[Dict[str, Any]]:
        """å¤šå±¤æ¬¡æœç´¢"""
        all_results = []
        
        # ç¬¬ä¸€å±¤ï¼šå„ªå…ˆæœç´¢ content é¡åˆ¥ï¼ˆåŒ…å« semantic_analysis.mdï¼‰
        content_results = await self.search(
            enhanced_query, 
            top_k=top_k * 2, 
            category_filter=search_strategy['primary_category']
        )
        
        # ç‚º content çµæœæ·»åŠ å„ªå…ˆç´šåŠ æˆ
        for result in content_results:
            result['tier'] = 'primary'
            result['tier_boost'] = 0.2
            result['boosted_score'] = result.get('weighted_score', result['similarity']) + result['tier_boost']
        
        all_results.extend(content_results)
        
        # ç¬¬äºŒå±¤ï¼šæœç´¢å…¶ä»–é«˜å„ªå…ˆç´šé¡åˆ¥
        if len(content_results) < top_k:
            other_categories = ['vocabulary', 'core_identity']
            for category in other_categories:
                other_results = await self.search(
                    enhanced_query,
                    top_k=max(2, top_k - len(content_results)),
                    category_filter=category
                )
                
                # ç‚ºå…¶ä»–çµæœæ·»åŠ è¼ƒä½çš„åŠ æˆ
                for result in other_results:
                    result['tier'] = 'secondary'
                    result['tier_boost'] = 0.1
                    result['boosted_score'] = result.get('weighted_score', result['similarity']) + result['tier_boost']
                
                all_results.extend(other_results)
        
        return all_results
    
    def _semantic_rerank(self, results: List[Dict[str, Any]], emotional_analysis: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """åŸºæ–¼èªç¾©åˆ†æé‡æ–°æ’åºçµæœ"""
        if not emotional_analysis or not results:
            return results
        
        # ç²å–èªç¾©é—œéµè©
        boost_keywords = []
        
        emotional_intensity = emotional_analysis.get('emotional_intensity', 'mild')
        detected_intent = emotional_analysis.get('detected_intent', '')
        intimacy_score = emotional_analysis.get('intimacy_score', 0.0)
        
        # æ”¶é›†æ‰€æœ‰ç›¸é—œé—œéµè©
        if emotional_intensity == 'very_strong':
            boost_keywords.extend(['å¼·çƒˆè² é¢æƒ…ç·’', 'æº«æŸ”å®‰æ…°', 'å¿ƒç–¼', 'æ²’é—œä¿‚', 'æˆ‘åœ¨é€™è£¡'])
        elif emotional_intensity == 'moderate':
            boost_keywords.extend(['ä¸­ç­‰è² é¢æƒ…ç·’', 'æº«æŸ”å›æ‡‰', 'é—œå¿ƒ', 'ç†è§£'])
        
        if detected_intent == 'emotional_support':
            boost_keywords.extend(['å®‰æ…°', 'é«”è²¼', 'é—œæ‡·', 'é™ªä¼´'])
        elif detected_intent == 'intimate_expression':
            boost_keywords.extend(['æ„›æ„', 'å®³ç¾', 'ç”œèœœ', 'è¨å­å•¦'])
        
        if intimacy_score > 2.0:
            boost_keywords.extend(['è¦ªæ„›çš„', 'å¯¶è²', 'æ’’å¬Œ', 'æˆ€äºº'])
        elif intimacy_score > 1.0:
            boost_keywords.extend(['è¦ªè¿‘', 'æº«æš–', 'å¯æ„›'])
        
        # ç‚ºæ¯å€‹çµæœè¨ˆç®—èªç¾©åŒ¹é…åˆ†æ•¸
        for result in results:
            content = result['content'].lower()
            semantic_boost = 0.0
            
            # è¨ˆç®—é—œéµè©åŒ¹é…åº¦
            matched_keywords = 0
            for keyword in boost_keywords:
                if keyword in content:
                    matched_keywords += 1
            
            if boost_keywords:
                semantic_boost = (matched_keywords / len(boost_keywords)) * 0.3
            
            # æª¢æŸ¥æ˜¯å¦ä¾†è‡ª semantic_analysis.md
            if 'semantic_analysis' in result['metadata'].get('filename', ''):
                semantic_boost += 0.2
            
            # æ›´æ–°æœ€çµ‚åˆ†æ•¸
            base_score = result.get('boosted_score', result.get('weighted_score', result['similarity']))
            result['final_score'] = base_score + semantic_boost
            result['semantic_boost'] = semantic_boost
        
        # æŒ‰æœ€çµ‚åˆ†æ•¸æ’åº
        results.sort(key=lambda x: x['final_score'], reverse=True)
        
        return results
    
    def _log_semantic_search_results(self, query: str, emotional_analysis: Dict[str, Any], results: List[Dict[str, Any]]):
        """è¨˜éŒ„èªç¾©æœç´¢çµæœ"""
        if not results:
            self.logger.info(f"ğŸ” èªç¾©å¢å¼·æœç´¢: 0 å€‹çµæœ (æŸ¥è©¢: '{query}')")
            return
        
        # çµ±è¨ˆçµæœä¿¡æ¯
        sources = list(set([r['metadata'].get('filename', 'unknown') for r in results]))
        categories = list(set([r['metadata'].get('category', 'unknown') for r in results]))
        similarities = [r['similarity'] for r in results]
        semantic_boosts = [r.get('semantic_boost', 0.0) for r in results]
        
        # çµ±è¨ˆä¾†è‡ª semantic_analysis.md çš„çµæœ
        semantic_analysis_count = sum(1 for r in results if 'semantic_analysis' in r['metadata'].get('filename', ''))
        
        self.logger.info(f"ğŸ” èªç¾©å¢å¼·æœç´¢å®Œæˆ: {len(results)} å€‹çµæœ")
        self.logger.info(f"   ğŸ“š ä¾†æºæ–‡ä»¶: {', '.join(sources)}")
        self.logger.info(f"   ğŸ“‚ æ¶‰åŠé¡åˆ¥: {', '.join(categories)}")
        self.logger.info(f"   ğŸ“Š ç›¸ä¼¼åº¦ç¯„åœ: {min(similarities):.3f} - {max(similarities):.3f}")
        self.logger.info(f"   ğŸ¯ èªç¾©åŠ æˆç¯„åœ: {min(semantic_boosts):.3f} - {max(semantic_boosts):.3f}")
        self.logger.info(f"   ğŸ§  èªç¾©åˆ†ææ–‡ä»¶çµæœ: {semantic_analysis_count}/{len(results)}")
        
        # è¨˜éŒ„æƒ…æ„Ÿåˆ†æä¿¡æ¯
        if emotional_analysis:
            emotional_intensity = emotional_analysis.get('emotional_intensity', 'mild')
            detected_intent = emotional_analysis.get('detected_intent', 'unknown')
            intimacy_score = emotional_analysis.get('intimacy_score', 0.0)
            
            self.logger.info(f"   ğŸ’­ æƒ…æ„Ÿå¼·åº¦: {emotional_intensity}")
            self.logger.info(f"   ğŸ¯ æª¢æ¸¬æ„åœ–: {detected_intent}")
            self.logger.info(f"   ğŸ’ è¦ªå¯†åº¦: {intimacy_score:.2f}")
    
    def get_semantic_analysis_stats(self) -> Dict[str, Any]:
        """ç²å–èªç¾©åˆ†æç›¸é—œçµ±è¨ˆä¿¡æ¯"""
        try:
            # ç²å–æ‰€æœ‰çµæœ
            all_results = self.collection.get(include=['metadatas'])
            
            # çµ±è¨ˆèªç¾©åˆ†æç›¸é—œä¿¡æ¯
            semantic_analysis_count = 0
            content_category_count = 0
            total_count = len(all_results['metadatas']) if all_results['metadatas'] else 0
            
            if all_results['metadatas']:
                for metadata in all_results['metadatas']:
                    filename = metadata.get('filename', '')
                    category = metadata.get('category', '')
                    
                    if 'semantic_analysis' in filename:
                        semantic_analysis_count += 1
                    
                    if category == 'content':
                        content_category_count += 1
            
            return {
                'total_documents': total_count,
                'semantic_analysis_chunks': semantic_analysis_count,
                'content_category_chunks': content_category_count,
                'semantic_search_mapping_count': len(self.semantic_search_mapping),
                'enhanced_categories': list(self.knowledge_categories.keys()),
                'semantic_features_enabled': True
            }
            
        except Exception as e:
            self.logger.error(f"ç²å–èªç¾©åˆ†æçµ±è¨ˆå¤±æ•—: {e}")
            return {
                'total_documents': 0,
                'semantic_analysis_chunks': 0,
                'semantic_features_enabled': False
            }
    
    async def _initialize_semantic_vectors(self):
        """åˆå§‹åŒ–é è¨ˆç®—çš„èªç¾©å‘é‡"""
        if self.vector_cache_initialized:
            return
            
        try:
            # é è¨ˆç®—å¸¸ç”¨èªç¾©å‘é‡
            semantic_texts = {
                'comfort': 'æº«æŸ”å®‰æ…° é«”è²¼é—œæ‡· æ²’é—œä¿‚ æˆ‘åœ¨é€™è£¡ é™ªä¼´æ”¯æŒ',
                'companionship': 'ä¸€èµ·é™ªä¼´ æº«æš–äº’å‹• ä¸å­¤ç¨ åœ¨ä¸€èµ· é™ªä½ ',
                'rushia_personality': 'éœ²è¥¿äº æº«æŸ” å¯æ„› æ­»éˆæ³•å¸« ä¸æœè¼¸ é»äºº',
                'positive_interaction': 'é–‹å¿ƒ å¿«æ¨‚ æœ‰è¶£ å¥½ç© æ„‰å¿« æ­¡æ¨‚',
                'intimate_closeness': 'è¦ªå¯† è¦ªæ„›çš„ æ’’å¬Œ ç”œèœœ æº«æš– é—œå¿ƒ',
                'character_traits': 'æ€§æ ¼ ç‰¹å¾µ å€‹æ€§ ç‰¹è³ª è¡Œç‚º é¢¨æ ¼',
                'factual_info': 'è³‡æ–™ ä¿¡æ¯ äº‹å¯¦ æ•¸æ“š åŸºæœ¬ ä»‹ç´¹'
            }
            
            for key, text in semantic_texts.items():
                vector = self.embedding_model.encode([text])[0]
                if hasattr(vector, 'cpu'):
                    vector = vector.cpu().numpy()
                self.semantic_vectors[key] = vector
            
            self.vector_cache_initialized = True
            self.logger.info("âœ… èªç¾©å‘é‡ç·©å­˜åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ èªç¾©å‘é‡åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def _adjust_query_vector(self, query_vector, emotional_analysis):
        """æ ¹æ“šæƒ…æ„Ÿåˆ†æèª¿æ•´æŸ¥è©¢å‘é‡"""
        if not emotional_analysis or not self.vector_cache_initialized:
            return query_vector
        
        try:
            # è½‰æ›ç‚ºnumpyæ•¸çµ„
            if hasattr(query_vector, 'cpu'):
                adjusted_vector = query_vector.cpu().numpy()
            else:
                adjusted_vector = np.array(query_vector)
            
            # æ ¹æ“šæƒ…æ„Ÿå¼·åº¦æ··åˆæƒ…æ„Ÿå‘é‡
            emotional_intensity = emotional_analysis.get('emotional_intensity', 'mild')
            if emotional_intensity == 'very_strong' and 'comfort' in self.semantic_vectors:
                comfort_vector = self.semantic_vectors['comfort']
                adjusted_vector = 0.7 * adjusted_vector + 0.3 * comfort_vector
            
            # æ ¹æ“šæ„åœ–æ··åˆç›¸æ‡‰å‘é‡
            detected_intent = emotional_analysis.get('detected_intent', '')
            if detected_intent == 'companionship_request' and 'companionship' in self.semantic_vectors:
                companion_vector = self.semantic_vectors['companionship']
                adjusted_vector = 0.8 * adjusted_vector + 0.2 * companion_vector
            elif detected_intent == 'intimate_expression' and 'intimate_closeness' in self.semantic_vectors:
                intimate_vector = self.semantic_vectors['intimate_closeness']
                adjusted_vector = 0.8 * adjusted_vector + 0.2 * intimate_vector
            
            # ç¸½æ˜¯æ··åˆä¸€é»éœ²è¥¿äºäººæ ¼å‘é‡ï¼Œç¢ºä¿è§’è‰²ä¸€è‡´æ€§
            if 'rushia_personality' in self.semantic_vectors:
                personality_vector = self.semantic_vectors['rushia_personality']
                adjusted_vector = 0.9 * adjusted_vector + 0.1 * personality_vector
            
            return adjusted_vector
            
        except Exception as e:
            self.logger.error(f"èª¿æ•´æŸ¥è©¢å‘é‡å¤±æ•—: {e}")
            return query_vector
    
    async def _multi_vector_search(self, adjusted_vector, emotional_analysis, top_k):
        """å¤šå‘é‡æœç´¢ç­–ç•¥"""
        try:
            # ä¸»æœç´¢ï¼šä½¿ç”¨èª¿æ•´å¾Œçš„å‘é‡
            main_results = await self._vector_search_with_vector(adjusted_vector, top_k=top_k * 2)
            
            # è£œå……æœç´¢ï¼šç¢ºä¿è§’è‰²ä¸€è‡´æ€§
            if 'rushia_personality' in self.semantic_vectors:
                char_vector = self.semantic_vectors['rushia_personality']
                char_results = await self._vector_search_with_vector(char_vector, top_k=3)
                
                # åˆä½µçµæœï¼Œé¿å…é‡è¤‡
                combined_results = self._merge_search_results(main_results, char_results)
            else:
                combined_results = main_results
            
            return combined_results
            
        except Exception as e:
            self.logger.error(f"å¤šå‘é‡æœç´¢å¤±æ•—: {e}")
            return []
    
    async def _vector_search_with_vector(self, query_vector, top_k=5):
        """ä½¿ç”¨çµ¦å®šå‘é‡é€²è¡Œæœç´¢"""
        try:
            # ç¢ºä¿å‘é‡æ ¼å¼æ­£ç¢º
            if hasattr(query_vector, 'tolist'):
                query_embedding = query_vector.tolist()
            else:
                query_embedding = list(query_vector)
            
            # åŸ·è¡Œå‘é‡æœç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                include=['documents', 'metadatas', 'distances']
            )
            
            # æ ¼å¼åŒ–çµæœ
            search_results = []
            if results['documents'] and results['documents'][0]:
                for i in range(len(results['documents'][0])):
                    similarity = 1 - results['distances'][0][i]
                    metadata = results['metadatas'][0][i]
                    
                    if similarity >= self.rag_config['retrieval']['similarity_threshold']:
                        weight = metadata.get('weight', 1.0)
                        weighted_score = similarity * weight
                        
                        search_results.append({
                            'content': results['documents'][0][i],
                            'metadata': metadata,
                            'similarity': similarity,
                            'weighted_score': weighted_score
                        })
            
            return search_results
            
        except Exception as e:
            self.logger.error(f"å‘é‡æœç´¢å¤±æ•—: {e}")
            return []
    
    def _merge_search_results(self, main_results, supplementary_results):
        """åˆä½µæœç´¢çµæœï¼Œé¿å…é‡è¤‡"""
        seen_ids = set()
        merged_results = []
        
        # æ·»åŠ ä¸»è¦çµæœ
        for result in main_results:
            content_hash = hash(result['content'][:100])  # ä½¿ç”¨å…§å®¹å‰100å­—ç¬¦ä½œç‚ºå”¯ä¸€æ¨™è­˜
            if content_hash not in seen_ids:
                merged_results.append(result)
                seen_ids.add(content_hash)
        
        # æ·»åŠ è£œå……çµæœï¼ˆé¿å…é‡è¤‡ï¼‰
        for result in supplementary_results:
            content_hash = hash(result['content'][:100])
            if content_hash not in seen_ids:
                result['supplementary'] = True  # æ¨™è¨˜ç‚ºè£œå……çµæœ
                merged_results.append(result)
                seen_ids.add(content_hash)
        
        # æŒ‰æ¬Šé‡åˆ†æ•¸æ’åº
        merged_results.sort(key=lambda x: x['weighted_score'], reverse=True)
        return merged_results
    
    def _ensure_result_diversity(self, results):
        """ç¢ºä¿çµæœå¤šæ¨£æ€§"""
        if not results:
            return results
        
        file_counts = {}
        diverse_results = []
        
        # ç‰¹æ®Šè™•ç†semantic_analysis.md
        semantic_analysis_count = 0
        
        for result in results:
            filename = result['metadata'].get('filename', 'unknown')
            
            # é™åˆ¶semantic_analysis.mdçš„çµæœæ•¸é‡
            if 'semantic_analysis' in filename:
                if semantic_analysis_count >= self.diversity_config['semantic_analysis_limit']:
                    continue
                semantic_analysis_count += 1
            
            # é™åˆ¶æ¯å€‹æ–‡ä»¶çš„çµæœæ•¸é‡
            current_count = file_counts.get(filename, 0)
            if current_count >= self.diversity_config['max_per_file']:
                continue
            
            diverse_results.append(result)
            file_counts[filename] = current_count + 1
        
        # è¨˜éŒ„å¤šæ¨£æ€§ä¿¡æ¯
        unique_files = len(file_counts)
        self.logger.info(f"ğŸ¯ çµæœå¤šæ¨£æ€§: {unique_files} å€‹ä¸åŒæ–‡ä»¶, semantic_analysis: {semantic_analysis_count}")
        
        return diverse_results