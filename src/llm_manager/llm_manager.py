"""
LLM æ¨¡å‹ç®¡ç†å™¨ - é‡æ§‹ç‰ˆ
è² è²¬çµ±ä¸€ç®¡ç†æ‰€æœ‰LLMç›¸é—œåŠŸèƒ½ï¼Œæä¾›ä¸€è‡´çš„å¤–éƒ¨æ¥å£
"""

import asyncio
import logging
import torch
import datetime
from typing import Optional, List, Dict, Any
from pathlib import Path

# å°å…¥é‡æ§‹å¾Œçš„æ¨¡çµ„
from .gpu_manager import GPUResourceManager
from .model_loader import create_model_loader, validate_loading_mode_config, get_available_loading_modes
from .response_generator import ResponseGenerator

# å°å…¥ä¾è³´æ¨¡çµ„
import sys
from pathlib import Path
# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘ä»¥æ”¯æŒçµ•å°å°å…¥
current_dir = Path(__file__).parent
src_dir = current_dir.parent
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from filter.filter import ResponseFilter
from core import RushiaPersonalityCore


class LLMManager:
    """LLMç®¡ç†å™¨ - é‡æ§‹ç‰ˆï¼Œçµ±ä¸€ç®¡ç†æ‰€æœ‰LLMç›¸é—œåŠŸèƒ½"""
    
    def __init__(self, config: dict):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # ğŸ”¥ é©—è­‰è¼‰å…¥æ¨¡å¼é…ç½®
        validation_result = validate_loading_mode_config(config)
        if not validation_result['valid']:
            for error in validation_result['errors']:
                self.logger.error(f"âŒ é…ç½®éŒ¯èª¤: {error}")
            for rec in validation_result['recommendations']:
                self.logger.info(f"ğŸ’¡ å»ºè­°: {rec}")
            raise ValueError("è¼‰å…¥æ¨¡å¼é…ç½®ç„¡æ•ˆ")
        
        # è¨˜éŒ„è­¦å‘Šå’Œå»ºè­°
        for warning in validation_result['warnings']:
            self.logger.warning(f"âš ï¸ {warning}")
        for rec in validation_result['recommendations']:
            self.logger.info(f"ğŸ’¡ å»ºè­°: {rec}")
        
        # ğŸ”¥ æ ¸å¿ƒçµ„ä»¶åˆå§‹åŒ–
        # GPUè³‡æºç®¡ç†å™¨
        self.gpu_resource_manager = GPUResourceManager(config)
        
        # æ¨¡å‹è¼‰å…¥å™¨ - ä½¿ç”¨å·¥å» å‡½æ•¸å‰µå»º
        self.model_loader = create_model_loader(config, self.gpu_resource_manager)
        
        # è¨˜éŒ„é¸æ“‡çš„è¼‰å…¥æ¨¡å¼
        loading_mode = config.get('models', {}).get('llm', {}).get('loading_mode', 'transformers')
        self.logger.info(f"ğŸš€ è¼‰å…¥æ¨¡å¼: {loading_mode}")
        
        # æ ¸å¿ƒäººæ ¼æ¨¡çµ„
        self.personality_core = RushiaPersonalityCore()
        
        # å›æ‡‰éæ¿¾å™¨
        self.response_filter = ResponseFilter(config)
        
        # å›æ‡‰ç”Ÿæˆå™¨
        self.response_generator = ResponseGenerator(
            config, 
            self.model_loader, 
            self.gpu_resource_manager,
            self.personality_core, 
            self.response_filter
        )
        
        self.logger.info("âœ… LLMç®¡ç†å™¨é‡æ§‹ç‰ˆåˆå§‹åŒ–å®Œæˆ")
    
    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹å’Œçµ„ä»¶"""
        self.logger.info("ğŸš€ é–‹å§‹åˆå§‹åŒ–LLMç®¡ç†å™¨...")
        
        try:
            # 1. è¼‰å…¥LLMä¸»æ¨¡å‹
            if not await self.model_loader.load_llm_model():
                raise RuntimeError("LLMæ¨¡å‹è¼‰å…¥å¤±æ•—")
            
            # 2. è¼‰å…¥åµŒå…¥æ¨¡å‹
            if not await self.model_loader.load_embedding_model():
                raise RuntimeError("åµŒå…¥æ¨¡å‹è¼‰å…¥å¤±æ•—")
            
            # 3. è¨­ç½®VTuberäººæ ¼
            if not await self._setup_vtuber_personality():
                raise RuntimeError("VTuberäººæ ¼è¨­ç½®å¤±æ•—")
            
            self.logger.info("âœ… LLMç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ LLMç®¡ç†å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    async def _setup_vtuber_personality(self) -> bool:
        """è¨­ç½® VTuber è§’è‰²äººæ ¼ - å®Œå…¨ä¾è³´ core.json"""
        try:
            # è¼‰å…¥æ ¸å¿ƒäººæ ¼æ•¸æ“š
            if not self.personality_core.load_core_personality():
                self.logger.error("âŒ ç„¡æ³•è¼‰å…¥æ ¸å¿ƒäººæ ¼æ•¸æ“šï¼Œç³»çµ±ç„¡æ³•é‹è¡Œ")
                return False
            
            # è¨˜éŒ„è¼‰å…¥çš„è§’è‰²ä¿¡æ¯
            identity = self.personality_core.get_character_identity()
            personality = self.personality_core.get_personality_traits()
            
            # è¨­ç½®éæ¿¾å™¨çš„è§’è‰²åç¨±
            character_name = identity['name'].get('zh', 'éœ²è¥¿å©­')
            self.response_filter.set_character_name(character_name)
            
            # ğŸ”¥ ä¿®å¾©ï¼šåˆå§‹åŒ–èªç¾©åˆ†æç³»çµ±
            if hasattr(self.personality_core, 'initialize_semantic_analysis'):
                semantic_success = self.personality_core.initialize_semantic_analysis()
                if semantic_success:
                    self.logger.info("âœ… èªç¾©åˆ†æç³»çµ±å·²å•Ÿå‹•")
                else:
                    self.logger.warning("âš ï¸ èªç¾©åˆ†æç³»çµ±åˆå§‹åŒ–å¤±æ•—ï¼Œä½¿ç”¨åŸºç¤æ¨¡å¼")
            
            self.logger.info("âœ… VTuber è§’è‰²äººæ ¼è¨­ç½®å®Œæˆ")
            self.logger.info(f"   è§’è‰²åç¨±: {character_name}")
            self.logger.info(f"   æ€§æ ¼ç‰¹å¾µ: {', '.join(personality['primary_traits'])}")
            self.logger.info(f"   ç•¶å‰æƒ…ç·’: {self.personality_core.current_mood}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ VTuberäººæ ¼è¨­ç½®å¤±æ•—: {e}")
            return False
    
    # ==============================================
    # ğŸ¯ ä¸»è¦å…¬é–‹æ¥å£
    # ==============================================
    
    async def generate_response(
        self, 
        prompt: str, 
        context: Optional[str] = None,
        conversation_history: Optional[List[tuple]] = None,
        stream: bool = False,
        rag_enabled: bool = True
    ) -> str:
        """ç”Ÿæˆå›æ‡‰ - ä¸»è¦å…¬é–‹æ¥å£"""
        return await self.response_generator.generate_response(
            prompt, context, conversation_history, stream, rag_enabled
        )
    
    def get_embeddings(self, texts: List[str]) -> torch.Tensor:
        """ç²å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        try:
            embeddings = self.model_loader.embedding_model.encode(
                texts,
                batch_size=self.config['models']['embedding']['batch_size'],
                convert_to_tensor=True,
                device=self.gpu_resource_manager.device
            )
            return embeddings
        except Exception as e:
            self.logger.error(f"ç”ŸæˆåµŒå…¥å‘é‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise
    
    def set_rag_system_reference(self, rag_system):
        """è¨­ç½®RAGç³»çµ±å¼•ç”¨ï¼ˆç”¨æ–¼å¢å¼·æª¢ç´¢ï¼‰"""
        self.response_generator.set_rag_system_reference(rag_system)
    
    # ==============================================
    # ğŸ”§ ç®¡ç†å’Œç›£æ§æ¥å£
    # ==============================================
    
    def get_model_info(self) -> Dict[str, Any]:
        """ç²å–å®Œæ•´çš„æ¨¡å‹ä¿¡æ¯"""
        try:
            # åŸºç¤æ¨¡å‹ä¿¡æ¯
            model_info = self.model_loader.get_model_info()
            
            # æ·»åŠ è¼‰å…¥æ¨¡å¼ä¿¡æ¯
            loading_mode = self.config.get('models', {}).get('llm', {}).get('loading_mode', 'transformers')
            model_info["loading_mode"] = {
                "current_mode": loading_mode,
                "available_modes": get_available_loading_modes()
            }
            
            # æ·»åŠ æ ¸å¿ƒäººæ ¼ä¿¡æ¯
            personality_info = {}
            if hasattr(self.personality_core, 'core_data') and self.personality_core.core_data:
                personality_info = {
                    "core_loaded": True,
                    "current_mood": self.personality_core.current_mood,
                    "character_name": self.personality_core.get_character_identity()['name'].get('zh', 'éœ²è¥¿å©­'),
                    "personality_traits": len(self.personality_core.get_personality_traits()['primary_traits'])
                }
            else:
                personality_info = {"core_loaded": False}
            
            # æ€§èƒ½çµ±è¨ˆ
            performance_info = {
                "conversation_count": self.response_generator.conversation_count,
                "multi_gpu_enabled": self.gpu_resource_manager.use_multi_gpu,
                "gpu_optimization": "enabled" if self.gpu_resource_manager.use_multi_gpu else "disabled",
                "optimizations_applied": getattr(self.model_loader, 'optimization_applied', [])
            }
            
            # åˆä½µæ‰€æœ‰ä¿¡æ¯
            model_info["personality_core"] = personality_info
            model_info["performance"] = performance_info
            
            return model_info
            
        except Exception as e:
            self.logger.error(f"ç²å–æ¨¡å‹ä¿¡æ¯å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def get_gpu_status(self) -> Dict[str, Any]:
        """ç²å–å³æ™‚GPUç‹€æ…‹"""
        return self.gpu_resource_manager.get_gpu_status()
    
    def optimize_gpu_memory(self):
        """å„ªåŒ–GPUè¨˜æ†¶é«”ä½¿ç”¨"""
        self.gpu_resource_manager.optimize_gpu_memory()
    
    def diagnose_gpu_allocation(self) -> Dict[str, Any]:
        """è¨ºæ–·GPUåˆ†é…ç‹€æ³"""
        return self.gpu_resource_manager.gpu_manager.diagnose_gpu_allocation(
            self.model_loader.llm_model
        )
    
    def get_conversation_stats(self) -> Dict[str, Any]:
        """ç²å–å°è©±çµ±è¨ˆä¿¡æ¯"""
        return self.response_generator.get_conversation_stats()
    
    def reset_conversation_count(self):
        """é‡ç½®å°è©±è¨ˆæ•¸å™¨"""
        self.response_generator.reset_conversation_count()
    
    # ==============================================
    # ğŸ§¹ è³‡æºç®¡ç†æ¥å£
    # ==============================================
    
    def cleanup(self):
        """æ¸…ç†æ‰€æœ‰è³‡æº"""
        self.logger.info("ğŸ§¹ é–‹å§‹æ¸…ç†LLMç®¡ç†å™¨è³‡æº...")
        
        # 1. æ¸…ç†æ¨¡å‹è³‡æº
        self.model_loader.cleanup_models()
        
        # 2. æ¸…ç†GPUè³‡æº
        self.gpu_resource_manager.cleanup_gpu_resources()
        
        self.logger.info("âœ… LLMç®¡ç†å™¨è³‡æºæ¸…ç†å®Œæˆ")
    
    # ==============================================
    # ğŸ”— å‘å¾Œå…¼å®¹æ¥å£
    # ==============================================
    
    @property
    def llm_model(self):
        """å‘å¾Œå…¼å®¹ï¼šLLMæ¨¡å‹è¨ªå•"""
        return self.model_loader.llm_model
    
    @property
    def llm_tokenizer(self):
        """å‘å¾Œå…¼å®¹ï¼šLLM tokenizerè¨ªå•"""
        return self.model_loader.llm_tokenizer
    
    @property
    def embedding_model(self):
        """å‘å¾Œå…¼å®¹ï¼šåµŒå…¥æ¨¡å‹è¨ªå•"""
        return self.model_loader.embedding_model
    
    @property
    def device(self):
        """å‘å¾Œå…¼å®¹ï¼šè¨­å‚™è¨ªå•"""
        return self.gpu_resource_manager.device
    
    @property
    def use_multi_gpu(self):
        """å‘å¾Œå…¼å®¹ï¼šå¤šGPUæ¨™è­˜è¨ªå•"""
        return self.gpu_resource_manager.use_multi_gpu
    
    @property
    def conversation_count(self):
        """å‘å¾Œå…¼å®¹ï¼šå°è©±è¨ˆæ•¸è¨ªå•"""
        return self.response_generator.conversation_count


# ==============================================
# ğŸ­ å·¥å» å‡½æ•¸
# ==============================================

async def create_llm_manager(config: dict) -> Optional[LLMManager]:
    """LLMç®¡ç†å™¨å·¥å» å‡½æ•¸ - ä¾¿æ·å‰µå»ºå’Œåˆå§‹åŒ–"""
    try:
        # å‰µå»ºç®¡ç†å™¨å¯¦ä¾‹
        manager = LLMManager(config)
        
        # åˆå§‹åŒ–æ‰€æœ‰çµ„ä»¶
        if await manager.initialize():
            return manager
        else:
            # åˆå§‹åŒ–å¤±æ•—ï¼Œæ¸…ç†è³‡æº
            manager.cleanup()
            return None
            
    except Exception as e:
        logging.error(f"âŒ LLMç®¡ç†å™¨å‰µå»ºå¤±æ•—: {e}")
        return None


# ==============================================
# ğŸ“Š ç³»çµ±ä¿¡æ¯å’Œè¨ºæ–·å·¥å…·
# ==============================================

def get_system_info() -> Dict[str, Any]:
    """ç²å–ç³»çµ±ä¿¡æ¯"""
    import platform
    import psutil
    
    return {
        "system": {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "pytorch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "cuda_version": torch.version.cuda if torch.cuda.is_available() else "N/A"
        },
        "hardware": {
            "cpu_count": psutil.cpu_count(),
            "memory_gb": round(psutil.virtual_memory().total / (1024**3), 2),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
        },
        "timestamp": str(datetime.datetime.now())
    }


def diagnose_system() -> Dict[str, Any]:
    """ç³»çµ±è¨ºæ–·å·¥å…·"""
    diagnosis = {
        "system_info": get_system_info(),
        "issues": [],
        "recommendations": []
    }
    
    # æª¢æŸ¥CUDAå¯ç”¨æ€§
    if not torch.cuda.is_available():
        diagnosis["issues"].append("CUDAä¸å¯ç”¨ï¼Œåªèƒ½ä½¿ç”¨CPUæ¨¡å¼")
        diagnosis["recommendations"].append("å®‰è£æ”¯æŒCUDAçš„PyTorchç‰ˆæœ¬ä»¥ç²å¾—æ›´å¥½æ€§èƒ½")
    
    # æª¢æŸ¥GPUè¨˜æ†¶é«”
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        if gpu_count < 2:
            diagnosis["recommendations"].append("ä½¿ç”¨å¤šGPUå¯ä»¥æé«˜å¤§æ¨¡å‹è¼‰å…¥é€Ÿåº¦å’Œæ¨ç†æ€§èƒ½")
        
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / (1024**3)
            if memory_gb < 8:
                diagnosis["issues"].append(f"GPU {i} è¨˜æ†¶é«”ä¸è¶³8GBï¼Œå¯èƒ½å½±éŸ¿å¤§æ¨¡å‹è¼‰å…¥")
    
    # æª¢æŸ¥ç³»çµ±è¨˜æ†¶é«”
    import psutil
    memory_gb = psutil.virtual_memory().total / (1024**3)
    if memory_gb < 16:
        diagnosis["issues"].append("ç³»çµ±è¨˜æ†¶é«”ä¸è¶³16GBï¼Œå¯èƒ½å½±éŸ¿æ¨¡å‹è¼‰å…¥é€Ÿåº¦")
    
    return diagnosis
