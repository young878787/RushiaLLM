"""
vLLM æ¨¡å‹è¼‰å…¥å’Œåˆå§‹åŒ–æ¨¡çµ„
è² è²¬è¼‰å…¥å’Œç®¡ç†ä½¿ç”¨ vLLM å¼•æ“çš„ Qwen-8B æ¨¡å‹å’ŒåµŒå…¥æ¨¡å‹
æä¾›èˆ‡ ModelLoader ç›¸åŒçš„æ¥å£ï¼Œæ”¯æ´ç„¡ç¸«åˆ‡æ›
"""

import logging
import torch
import gc
import os
import asyncio
import uuid
import signal
import time
import random
from typing import Optional, Dict, Any, List, TYPE_CHECKING
from pathlib import Path

# vLLM imports - ä½¿ç”¨ TYPE_CHECKING è™•ç†é¡å‹è¨»è§£
if TYPE_CHECKING:
    from vllm import AsyncLLMEngine, LLMEngine, EngineArgs, SamplingParams

try:
    from vllm import AsyncLLMEngine, LLMEngine, EngineArgs, SamplingParams
    from vllm.distributed.parallel_state import destroy_model_parallel
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False

from transformers import AutoTokenizer
from sentence_transformers import SentenceTransformer

from .gpu_manager import OptimizedEmbeddingModel


class VLLMModelLoader:
    """vLLM æ¨¡å‹è¼‰å…¥å™¨ - æä¾›èˆ‡ ModelLoader ç›¸åŒçš„æ¥å£"""
    
    def __init__(self, config: dict, gpu_resource_manager):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.gpu_resource_manager = gpu_resource_manager
        
        # æª¢æŸ¥ vLLM å¯ç”¨æ€§
        if not VLLM_AVAILABLE:
            raise ImportError("vLLM æœªå®‰è£æˆ–ä¸å¯ç”¨ã€‚è«‹å®‰è£ vLLM: pip install vllm")
        
        # æ¨¡å‹çµ„ä»¶
        self.llm_engine: Optional['AsyncLLMEngine'] = None
        self.llm_tokenizer: Optional[AutoTokenizer] = None
        self.embedding_model: Optional[SentenceTransformer] = None
        
        # vLLM ç‰¹å®šé…ç½®
        self.vllm_config = config.get('models', {}).get('llm', {}).get('vllm', {})
        self.tensor_parallel_size = self._calculate_tensor_parallel_size()
        self.pipeline_parallel_size = self.vllm_config.get('pipeline_parallel_size', 1)
        
        # æ€§èƒ½å„ªåŒ–çµ„ä»¶ï¼ˆèˆ‡ ModelLoader ä¿æŒä¸€è‡´ï¼‰
        self.static_cache_enabled = False
        self.torch_compile_enabled = False
        self.optimization_applied = []
        
        self.logger.info("ğŸš€ vLLM æ¨¡å‹è¼‰å…¥å™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"   Tensor Parallel: {self.tensor_parallel_size}")
        self.logger.info(f"   Pipeline Parallel: {self.pipeline_parallel_size}")
    
    def __del__(self):
        """ææ§‹å‡½æ•¸ - ç¢ºä¿è³‡æºè¢«æ¸…ç†"""
        try:
            if hasattr(self, 'llm_engine') and self.llm_engine is not None:
                self.logger.warning("âš ï¸ VLLMModelLoader è¢«ææ§‹æ™‚ç™¼ç¾æœªæ¸…ç†çš„è³‡æºï¼Œæ­£åœ¨åŸ·è¡Œæ¸…ç†...")
                self.cleanup_models()
        except Exception:
            # ææ§‹å‡½æ•¸ä¸­ä¸æ‡‰è©²æ‹‹å‡ºç•°å¸¸
            pass
    
    def _calculate_tensor_parallel_size(self) -> int:
        """è¨ˆç®— Tensor Parallel å¤§å°"""
        available_gpus = len(self.gpu_resource_manager.gpu_manager.available_gpus)
        
        # å¾é…ç½®è®€å–ï¼Œæˆ–è‡ªå‹•è¨ˆç®—
        config_tp_size = self.vllm_config.get('tensor_parallel_size', 'auto')
        
        if config_tp_size == 'auto':
            # è‡ªå‹•è¨ˆç®—ï¼šå„ªå…ˆä½¿ç”¨æ‰€æœ‰å¯ç”¨ GPU
            if available_gpus >= 4:
                return 4  # ä½¿ç”¨4å¼µå¡åš Tensor Parallel
            elif available_gpus >= 2:
                return 2  # ä½¿ç”¨2å¼µå¡
            else:
                return 1  # å–®å¡
        else:
            # ä½¿ç”¨é…ç½®æŒ‡å®šçš„å€¼
            return min(int(config_tp_size), available_gpus)
    
    async def load_llm_model(self) -> bool:
        """è¼‰å…¥ä¸»è¦çš„ LLM æ¨¡å‹ - vLLM ç‰ˆæœ¬"""
        self.logger.info("ğŸš€ è¼‰å…¥ Qwen-8B ä¸»æ¨¡å‹ (vLLMå¼•æ“)...")
        
        model_path = self.config['models']['llm']['model_path']
        
        # ğŸ”¥ è™•ç†ç›¸å°è·¯å¾‘ - å¾å·¥ä½œç›®éŒ„çš„ä¸Šç´šç›®éŒ„é–‹å§‹
        if not os.path.isabs(model_path):
            # å¾ scrpitsV2/LLM ç›®éŒ„å‘ä¸Šå…©ç´šåˆ°é” RushiaLLM æ ¹ç›®éŒ„
            current_dir = Path(__file__).parent.parent.parent.parent.parent  # åˆ°é” RushiaLLM æ ¹ç›®éŒ„
            model_path = str(current_dir / model_path)
            self.logger.info(f"ğŸ”§ ç›¸å°è·¯å¾‘è½‰æ›: {self.config['models']['llm']['model_path']} -> {model_path}")
        
        # é©—è­‰è·¯å¾‘æ˜¯å¦å­˜åœ¨
        if not Path(model_path).exists():
            self.logger.error(f"âŒ æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {model_path}")
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {model_path}")
        
        try:
            # å…ˆè¼‰å…¥ tokenizer (èˆ‡åŸç‰ˆä¿æŒä¸€è‡´)
            self.llm_tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                padding_side="left",
                cache_dir=None
            )
            
            if self.llm_tokenizer.pad_token is None:
                self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token
            
            # é…ç½® vLLM EngineArgs
            engine_args = self._create_vllm_engine_args(model_path)
            
            # ğŸ”§ ä¿®è£œ vLLM 0.10.1.1 å…¼å®¹æ€§å•é¡Œ
            # AsyncLLMEngine.from_engine_args æœŸæœ›çš„å±¬æ€§åœ¨æ–°ç‰ˆ EngineArgs ä¸­è¢«ç§»é™¤
            if not hasattr(engine_args, 'enable_log_requests'):
                engine_args.enable_log_requests = False
            
            # è¨˜éŒ„ vLLM é…ç½®
            self.logger.info("ğŸ”§ vLLM å¼•æ“é…ç½®:")
            self.logger.info(f"   æ¨¡å‹è·¯å¾‘: {model_path}")
            self.logger.info(f"   Tensor Parallel: {engine_args.tensor_parallel_size}")
            self.logger.info(f"   GPU è¨˜æ†¶é«”åˆ©ç”¨ç‡: {engine_args.gpu_memory_utilization}")
            self.logger.info(f"   æœ€å¤§æ¨¡å‹é•·åº¦: {engine_args.max_model_len}")
            self.logger.info(f"   é‡åŒ–: {engine_args.quantization}")
            self.logger.info(f"   å…¼å®¹æ€§ä¿®è£œ: enable_log_requests = {getattr(engine_args, 'enable_log_requests', 'N/A')}")
            
            # å‰µå»º AsyncLLMEngine
            self.logger.info("â³ æ­£åœ¨å‰µå»º vLLM ç•°æ­¥å¼•æ“...")
            self.llm_engine = AsyncLLMEngine.from_engine_args(engine_args)
            
            self.logger.info("âœ… vLLM å¼•æ“å‰µå»ºæˆåŠŸ")
            
            # è¨˜éŒ„è¼‰å…¥å¾Œçš„è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
            if torch.cuda.is_available():
                self.gpu_resource_manager.gpu_manager.log_gpu_status(reason="vLLMæ¨¡å‹è¼‰å…¥å®Œæˆ")
            
            success_msg = f"âœ… Qwen-8B vLLMæ¨¡å‹è¼‰å…¥æˆåŠŸ (TP={self.tensor_parallel_size})"
            if self.tensor_parallel_size > 1:
                success_msg += f" (å¤šGPUä¸¦è¡Œ: {self.tensor_parallel_size}å¼µå¡)"
            self.logger.info(success_msg)
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Qwen-8B vLLMæ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            # æ¸…ç†å¯èƒ½çš„éƒ¨åˆ†åˆå§‹åŒ–è³‡æº
            await self._cleanup_partial_vllm_resources()
            raise e  # ç›´æ¥æ‹‹å‡ºéŒ¯èª¤ï¼Œä¸é€²è¡Œå›é€€
    
    def _create_vllm_engine_args(self, model_path: str) -> 'EngineArgs':
        """å‰µå»º vLLM EngineArgs - å…¼å®¹ vLLM 0.10.1.1ï¼Œæ”¯æŒ8bité‡åŒ–"""
        llm_config = self.config['models']['llm']
        vllm_config = self.vllm_config
        
        # ğŸ”¥ 8bit é‡åŒ–é…ç½® - æ˜ å°„åˆ° vLLM æ”¯æŒçš„ bitsandbytes
        quantization_mode = llm_config.get('quantization', '8bit')
        vllm_quantization = None
        
        if quantization_mode == '8bit':
            # ä½¿ç”¨ bitsandbytes é€²è¡Œ 8bit é‡åŒ–
            vllm_quantization = 'bitsandbytes'
            self.logger.info(f"ğŸ”§ å•Ÿç”¨é‡åŒ–: {quantization_mode} -> vLLM {vllm_quantization}")
        elif quantization_mode == '4bit':
            # ä½¿ç”¨ AWQ ä½œç‚º4bitæ›¿ä»£
            vllm_quantization = 'awq'
            self.logger.info(f"ğŸ”§ å•Ÿç”¨é‡åŒ–: {quantization_mode} -> vLLM {vllm_quantization}")
        else:
            vllm_quantization = None
            self.logger.info("ğŸ”§ æœªå•Ÿç”¨é‡åŒ–")
        
        # ğŸš€ ç”Ÿæˆå‹•æ…‹éš¨æ©Ÿç¨®å­ - ç¢ºä¿æ¯æ¬¡å•Ÿå‹•éƒ½æœ‰ä¸åŒçš„éš¨æ©Ÿæ€§
        import random
        dynamic_seed = int(time.time() * 1000000) % 2147483647 + random.randint(0, 10000)
        self.logger.info(f"ğŸ² ä½¿ç”¨å‹•æ…‹éš¨æ©Ÿç¨®å­: {dynamic_seed}")
        
        # åŸºç¤é…ç½®
        args = EngineArgs(
            model=model_path,
            tokenizer=model_path,
            tensor_parallel_size=self.tensor_parallel_size,
            pipeline_parallel_size=self.pipeline_parallel_size,
            trust_remote_code=True,
            
            # ğŸš€ é—œéµä¿®å¾©ï¼šæ·»åŠ å‹•æ…‹éš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿æ¯æ¬¡ç”Ÿæˆéƒ½æœ‰éš¨æ©Ÿæ€§
            seed=dynamic_seed,
            
            # è¨˜æ†¶é«”é…ç½® - èˆ‡ model_loader 8bit æ¨¡å¼ä¿æŒä¸€è‡´
            gpu_memory_utilization=vllm_config.get('gpu_memory_utilization', 0.8),  # é™ä½è¨˜æ†¶é«”ä½¿ç”¨
            swap_space=vllm_config.get('swap_space', 0.5),  # 2GB swap space
            
            # æ¨¡å‹é…ç½®
            max_model_len=vllm_config.get('max_model_len', llm_config.get('max_length', 2048)),  # èˆ‡é…ç½®ä¸€è‡´
            max_num_seqs=vllm_config.get('max_num_seqs', 128),  # é™ä½ä¸¦ç™¼æ•¸
            max_num_batched_tokens=vllm_config.get('max_num_batched_tokens', None),
            
            # ğŸ”¥ é‡åŒ–é…ç½® - æ”¯æŒ8bitç­‰æ•ˆ
            quantization=vllm_quantization,
            
            # æ•¸æ“šé¡å‹ - èˆ‡ model_loader çš„ FP16 ä¿æŒä¸€è‡´
            dtype=vllm_config.get('dtype', 'half'),  # ä½¿ç”¨ half (FP16)
            
            # KV ç·©å­˜é…ç½®
            kv_cache_dtype=vllm_config.get('kv_cache_dtype', 'auto'),
            
            # æ€§èƒ½é…ç½® - åªä¿ç•™æ”¯æ´çš„åƒæ•¸
            disable_log_stats=vllm_config.get('disable_log_stats', False),
        )
        
        return args
    
    async def load_embedding_model(self) -> bool:
        """è¼‰å…¥åµŒå…¥æ¨¡å‹ - èˆ‡åŸç‰ˆ model_loader çš„ FP16 åˆ†é…ç­–ç•¥å®Œå…¨ä¸€è‡´"""
        self.logger.info("ğŸš€ è¼‰å…¥ Qwen3-Embedding-0.6B åµŒå…¥æ¨¡å‹ (FP16åŸç‰ˆ)...")
        
        model_path = self.config['models']['embedding']['model_path']
        
        # ğŸ”¥ è™•ç†ç›¸å°è·¯å¾‘ - å¾å·¥ä½œç›®éŒ„çš„ä¸Šç´šç›®éŒ„é–‹å§‹
        if not os.path.isabs(model_path):
            # å¾ scrpitsV2/LLM ç›®éŒ„å‘ä¸Šå…©ç´šåˆ°é” RushiaLLM æ ¹ç›®éŒ„
            current_dir = Path(__file__).parent.parent.parent.parent.parent  # åˆ°é” RushiaLLM æ ¹ç›®éŒ„
            model_path = str(current_dir / model_path)
            self.logger.info(f"ğŸ”§ ç›¸å°è·¯å¾‘è½‰æ›: {self.config['models']['embedding']['model_path']} -> {model_path}")
        
        # é©—è­‰è·¯å¾‘æ˜¯å¦å­˜åœ¨
        if not Path(model_path).exists():
            self.logger.error(f"âŒ åµŒå…¥æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {model_path}")
            raise FileNotFoundError(f"åµŒå…¥æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {model_path}")
        
        # ğŸ”¥ ä½¿ç”¨èˆ‡åŸç‰ˆ model_loader å®Œå…¨ç›¸åŒçš„ GPU åˆ†é…é‚è¼¯
        embedding_config = self.gpu_resource_manager.gpu_manager.get_memory_allocation_config("embedding")
        embedding_device = embedding_config.get("device", self.gpu_resource_manager.device)
        embedding_gpu_id = embedding_config.get("target_gpu")
        
        if embedding_gpu_id is not None:
            embedding_device = f"cuda:{embedding_gpu_id}"
            self.logger.info(f"ğŸ¯ åµŒå…¥æ¨¡å‹å°‡ä½¿ç”¨GPU {embedding_gpu_id} (çµ±ä¸€åˆ†é…)")
        else:
            self.logger.info(f"ğŸ¯ åµŒå…¥æ¨¡å‹ä½¿ç”¨è¨­å‚™: {embedding_device}")
        
        # è¨˜éŒ„åµŒå…¥æ¨¡å‹åˆ†é…ä¿¡æ¯
        self.gpu_resource_manager.gpu_manager.log_memory_allocation_info("embedding")
        
        try:
            # ğŸ”¥ å„ªå…ˆå˜—è©¦ä½¿ç”¨ OptimizedEmbeddingModel (èˆ‡åŸç‰ˆä¸€è‡´)
            try:
                # è¼‰å…¥ tokenizer
                embedding_tokenizer = AutoTokenizer.from_pretrained(
                    model_path,
                    trust_remote_code=True,
                    cache_dir=None
                )
                
                # ğŸ”¥ FP16åŸç‰ˆé…ç½® - ç§»é™¤é‡åŒ–ï¼Œèˆ‡åŸç‰ˆå®Œå…¨ä¸€è‡´
                from transformers import AutoModel
                
                model_kwargs = {
                    "trust_remote_code": True,
                    "torch_dtype": torch.float16 if embedding_device != "cpu" else torch.float32,
                    "low_cpu_mem_usage": True,
                    "cache_dir": None,
                    "use_cache": True,
                }
                
                # CPUæ¨¡å¼ç‰¹æ®Šé…ç½®
                if embedding_device == "cpu":
                    model_kwargs.update({
                        "torch_dtype": torch.float32,
                        "device_map": "cpu"
                    })
                
                # å¦‚æœæ˜¯å¤šGPUä¸”æŒ‡å®šäº†ç‰¹å®šGPUï¼Œè¨­ç½®device_map
                if embedding_gpu_id is not None:
                    model_kwargs["device_map"] = {"": embedding_gpu_id}
                elif embedding_device != "cpu":
                    model_kwargs["device_map"] = {"": embedding_device}
                
                # è¼‰å…¥FP16åŸç‰ˆåµŒå…¥æ¨¡å‹
                embedding_model = AutoModel.from_pretrained(
                    model_path,
                    **model_kwargs
                )
                
                if embedding_device == "cpu":
                    embedding_model = embedding_model.to("cpu")
                
                # å‰µå»ºèˆ‡åŸç‰ˆç›¸åŒçš„ OptimizedEmbeddingModel åŒ…è£å™¨
                self.embedding_model = OptimizedEmbeddingModel(
                    model=embedding_model,
                    tokenizer=embedding_tokenizer,
                    device=embedding_device,
                    gpu_id=embedding_gpu_id,
                    max_length=self.config['models']['embedding']['max_length'],
                    batch_size=self.config['models']['embedding']['batch_size']
                )
                
                success_msg = "âœ… Qwen3-Embedding FP16æ¨¡å‹è¼‰å…¥æˆåŠŸ (OptimizedEmbeddingModelæ¨¡å¼)"
                if embedding_gpu_id is not None:
                    success_msg += f" (GPU {embedding_gpu_id})"
                else:
                    success_msg += f" ({embedding_device})"
                self.logger.info(success_msg)
                
                return True
                
            except Exception as optimized_error:
                self.logger.warning(f"OptimizedEmbeddingModel è¼‰å…¥å¤±æ•—: {optimized_error}")
                self.logger.info("ğŸ”„ å›é€€åˆ°SentenceTransformeræ¨™æº–è¼‰å…¥æ–¹æ³•...")
                
                # å›é€€åˆ° SentenceTransformerï¼ˆèˆ‡åŸç‰ˆå›é€€é‚è¼¯ä¸€è‡´ï¼‰
                fallback_device = embedding_device if embedding_device != "cpu" else "cuda" if torch.cuda.is_available() else "cpu"
                
                self.embedding_model = SentenceTransformer(
                    model_path,
                    device=fallback_device,
                    cache_folder=None
                )
                
                success_msg = f"âœ… Qwen3-Embedding æ¨¡å‹è¼‰å…¥æˆåŠŸ (SentenceTransformerå›é€€æ¨¡å¼, {fallback_device})"
                if embedding_gpu_id is not None:
                    success_msg += f" (GPU {embedding_gpu_id})"
                self.logger.info(success_msg)
                
                return True
            
        except Exception as e:
            self.logger.error(f"âŒ Qwen3-Embedding æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise e  # ç›´æ¥æ‹‹å‡ºéŒ¯èª¤ï¼Œä¸é€²è¡Œå›é€€
        
        finally:
            # è¨˜éŒ„è¼‰å…¥å¾Œçš„è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³ï¼ˆèˆ‡åŸç‰ˆä¸€è‡´ï¼‰
            if torch.cuda.is_available():
                self.gpu_resource_manager.gpu_manager.log_gpu_status(reason="åµŒå…¥æ¨¡å‹è¼‰å…¥å®Œæˆ")
    
    async def generate_response_vllm(
        self, 
        prompt: str, 
        sampling_params: Optional['SamplingParams'] = None,
        request_id: Optional[str] = None
    ) -> str:
        """ä½¿ç”¨ vLLM å¼•æ“ç”Ÿæˆå›æ‡‰"""
        if self.llm_engine is None:
            raise RuntimeError("vLLM å¼•æ“æœªåˆå§‹åŒ–")
        
        try:
            # ä½¿ç”¨é»˜èªæ¡æ¨£åƒæ•¸ï¼ˆå¦‚æœæœªæä¾›ï¼‰
            if sampling_params is None:
                sampling_params = self._get_default_sampling_params()
            
            # ç”Ÿæˆå”¯ä¸€çš„è«‹æ±‚ ID
            if request_id is None:
                import uuid
                request_id = str(uuid.uuid4())
            
            # æäº¤ç”Ÿæˆè«‹æ±‚
            results_generator = self.llm_engine.generate(
                prompt=prompt,
                sampling_params=sampling_params,
                request_id=request_id
            )
            
            # ç­‰å¾…ç”Ÿæˆå®Œæˆ
            final_output = None
            async for request_output in results_generator:
                final_output = request_output
            
            if final_output is None:
                raise RuntimeError("vLLM ç”Ÿæˆçµæœç‚ºç©º")
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            generated_text = final_output.outputs[0].text
            return generated_text
            
        except Exception as e:
            self.logger.error(f"vLLM ç”Ÿæˆå›æ‡‰å¤±æ•—: {e}")
            raise e
    
    def _get_default_sampling_params(self) -> 'SamplingParams':
        """ç²å–é»˜èªçš„æ¡æ¨£åƒæ•¸ - vLLM å…¼å®¹ç‰ˆæœ¬ï¼Œæ”¯æ´å‹•æ…‹éš¨æ©Ÿç¨®å­"""
        llm_config = self.config['models']['llm']
        vtuber_config = self.config.get('vtuber', {}).get('response', {})
        
        # ğŸš€ æ¯æ¬¡ç”Ÿæˆä½¿ç”¨ä¸åŒçš„å‹•æ…‹éš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿å›æ‡‰çš„å¤šæ¨£æ€§
        import random
        generation_seed = int(time.time() * 1000000) % 2147483647 + random.randint(0, 1000)
        
        # ğŸ”¥ vLLM SamplingParams åªæ”¯æ´ç‰¹å®šåƒæ•¸ï¼Œç§»é™¤ä¸æ”¯æ´çš„ length_penalty
        # ä»¥ config.yaml é…ç½®ç‚ºä¸»ï¼Œè‹¥æœªè¨­å®šå‰‡ä½¿ç”¨é è¨­å€¼
        return SamplingParams(
            temperature=llm_config.get('temperature', self.config['models']['llm'].get('temperature', 0.75)),
            top_p=llm_config.get('top_p', self.config['models']['llm'].get('top_p', 0.8)),
            top_k=llm_config.get('top_k', self.config['models']['llm'].get('top_k', 40)),
            max_tokens=vtuber_config.get('max_tokens', self.config.get('vtuber', {}).get('response', {}).get('max_tokens', 150)),
            min_tokens=vtuber_config.get('min_tokens', self.config.get('vtuber', {}).get('response', {}).get('min_tokens', 25)),
            repetition_penalty=llm_config.get('repetition_penalty', self.config['models']['llm'].get('repetition_penalty', 1.15)),
            # ğŸ² æ·»åŠ å‹•æ…‹ç¨®å­ï¼Œæ¯æ¬¡ç”Ÿæˆéƒ½ä¸åŒ
            seed=generation_seed,
            # æ³¨æ„ï¼švLLM ä¸æ”¯æ´ length_penaltyï¼Œå·²ç§»é™¤
            stop=None,  # å¯ä»¥æ ¹æ“šéœ€è¦æ·»åŠ åœæ­¢æ¨™è¨˜
            include_stop_str_in_output=False
        )
    
    def create_sampling_params(self, **kwargs) -> 'SamplingParams':
        """å‰µå»ºè‡ªå®šç¾©æ¡æ¨£åƒæ•¸ï¼Œé»˜èªä½¿ç”¨å‹•æ…‹éš¨æ©Ÿç¨®å­"""
        default_params = self._get_default_sampling_params()
        
        # ğŸš€ å¦‚æœæ²’æœ‰æ˜ç¢ºæŒ‡å®šç¨®å­ï¼Œä½¿ç”¨æ–°çš„å‹•æ…‹ç¨®å­ï¼ˆå³ä½¿å·²æœ‰é»˜èªç¨®å­ï¼‰
        if 'seed' not in kwargs:
            import random
            kwargs['seed'] = int(time.time() * 1000000) % 2147483647 + random.randint(0, 1000)
            self.logger.debug(f"ğŸ² å‰µå»ºæ–°çš„å‹•æ…‹ç¨®å­: {kwargs['seed']}")
        
        # æ›´æ–°åƒæ•¸
        for key, value in kwargs.items():
            if hasattr(default_params, key):
                setattr(default_params, key, value)
        
        return default_params
    
    def get_embeddings(self, texts: List[str]) -> torch.Tensor:
        """ç²å–æ–‡æœ¬åµŒå…¥å‘é‡ - èˆ‡åŸç‰ˆ OptimizedEmbeddingModel æ¥å£å…¼å®¹"""
        if self.embedding_model is None:
            raise RuntimeError("åµŒå…¥æ¨¡å‹æœªè¼‰å…¥")
        
        try:
            # ğŸ”¥ æª¢æŸ¥æ˜¯å¦æ˜¯ OptimizedEmbeddingModel åŒ…è£å™¨
            if hasattr(self.embedding_model, 'encode_batch'):
                # ä½¿ç”¨ OptimizedEmbeddingModel çš„æ‰¹æ¬¡ç·¨ç¢¼ï¼ˆèˆ‡åŸç‰ˆä¸€è‡´ï¼‰
                embeddings = self.embedding_model.encode_batch(
                    texts,
                    convert_to_tensor=True
                )
            else:
                # ä½¿ç”¨ SentenceTransformer çš„æ¨™æº–ç·¨ç¢¼ï¼ˆå›é€€æ¨¡å¼ï¼‰
                embeddings = self.embedding_model.encode(
                    texts,
                    batch_size=self.config['models']['embedding']['batch_size'],
                    convert_to_tensor=True,
                    device=self.gpu_resource_manager.device
                )
            
            return embeddings
            
        except Exception as e:
            self.logger.error(f"ç”ŸæˆåµŒå…¥å‘é‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise e
    
    def get_model_input_device(self) -> Optional[str]:
        """ç²å–æ¨¡å‹è¼¸å…¥è¨­å‚™ - vLLM ç‰ˆæœ¬"""
        if self.tensor_parallel_size > 1:
            # å¤šGPUæƒ…æ³ä¸‹ï¼Œè¿”å›ç¬¬ä¸€å€‹GPU
            return f"cuda:{self.gpu_resource_manager.gpu_manager.available_gpus[0]}"
        else:
            return self.gpu_resource_manager.device
    
    def get_model_info(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹ä¿¡æ¯ - vLLM ç‰ˆæœ¬ï¼Œæ”¯æŒ8bité‡åŒ–å’ŒFP16åµŒå…¥"""
        try:
            # ç²å–é‡åŒ–ä¿¡æ¯ - ä½¿ç”¨èˆ‡è¼‰å…¥æ™‚ç›¸åŒçš„æ˜ å°„é‚è¼¯
            quantization_info = self.config['models']['llm'].get('quantization', '8bit')
            
            # æ˜ å°„åˆ° vLLM æ”¯æŒçš„é‡åŒ–æ–¹æ³•
            if quantization_info == '8bit':
                vllm_quantization = 'bitsandbytes'
            elif quantization_info == '4bit':
                vllm_quantization = 'awq'
            else:
                vllm_quantization = None
            
            # åŸºç¤LLMä¿¡æ¯ - èˆ‡åŸç‰ˆ model_loader é¢¨æ ¼ä¸€è‡´
            llm_info = {
                "model_type": "Qwen-8B",
                "engine": "vLLM",
                "precision": "8bit" if quantization_info == '8bit' else quantization_info,
                "quantization": f"{quantization_info} (vLLM {vllm_quantization})" if vllm_quantization else quantization_info,
                "tensor_parallel_size": self.tensor_parallel_size,
                "pipeline_parallel_size": self.pipeline_parallel_size,
                "device": self.gpu_resource_manager.device,
                "multi_gpu": self.tensor_parallel_size > 1,
                "memory_usage": "Managed by vLLM"
            }
            
            # åŸºç¤åµŒå…¥æ¨¡å‹ä¿¡æ¯ - èˆ‡åŸç‰ˆé¢¨æ ¼ä¸€è‡´
            embedding_device = "Unknown"
            embedding_type = "Unknown"
            
            if self.embedding_model:
                if hasattr(self.embedding_model, 'device'):
                    embedding_device = str(self.embedding_model.device)
                    embedding_type = "OptimizedEmbeddingModel (FP16)"
                else:
                    embedding_device = getattr(self.embedding_model, '_target_device', self.gpu_resource_manager.device)
                    embedding_type = "SentenceTransformer (FP16)"
            
            embedding_info = {
                "model_type": "Qwen3-Embedding-0.6B",
                "precision": "FP16",
                "quantization": "FP16åŸç‰ˆï¼ˆç„¡é‡åŒ–ï¼‰",
                "engine": embedding_type,
                "device": embedding_device,
                "memory_usage": "Unknown"
            }
            
            # GPUä¿¡æ¯ - èˆ‡åŸç‰ˆè©³ç´°ç¨‹åº¦ä¸€è‡´
            gpu_info = {}
            if torch.cuda.is_available():
                try:
                    memory_info = self.gpu_resource_manager.gpu_manager.get_gpu_memory_info()
                    
                    gpu_info = {
                        "total_gpus": len(self.gpu_resource_manager.gpu_manager.available_gpus),
                        "available_gpus": self.gpu_resource_manager.gpu_manager.available_gpus,
                        "total_memory_gb": self.gpu_resource_manager.gpu_manager.get_total_gpu_memory(),
                        "available_memory_gb": self.gpu_resource_manager.gpu_manager.get_available_memory(),
                        "per_gpu_info": {}
                    }
                    
                    for gpu_id, info in memory_info.items():
                        gpu_name = self.gpu_resource_manager.gpu_manager.gpu_info[gpu_id]['name']
                        gpu_info["per_gpu_info"][f"gpu_{gpu_id}"] = {
                            "name": gpu_name,
                            "total_gb": info['total_gb'],
                            "allocated_gb": info['allocated_gb'],
                            "reserved_gb": info['reserved_gb'],
                            "free_gb": info['free_gb'],
                            "utilization_percent": info['utilization']
                        }
                except Exception as gpu_error:
                    self.logger.warning(f"ç²å–GPUä¿¡æ¯æ™‚å‡ºéŒ¯: {gpu_error}")
                    gpu_info = {"error": "Failed to get GPU info"}
            
            return {
                "llm_model": llm_info,
                "embedding_model": embedding_info,
                "gpu_cluster": gpu_info,
                "performance_optimization": {
                    "engine": "vLLM",
                    "quantization": f"8bit -> vLLM {vllm_quantization}" if vllm_quantization else "8bit",
                    "tensor_parallel": f"{self.tensor_parallel_size}x GPU",
                    "memory_management": "vLLM Optimized",
                    "batch_processing": "Continuous Batching",
                    "kv_cache": "PagedAttention",
                    "embedding_precision": "FP16åŸç‰ˆ"
                }
            }
            
        except Exception as e:
            self.logger.error(f"ç²å–æ¨¡å‹ä¿¡æ¯å¤±æ•—: {e}")
            return {"error": str(e)}
    
    async def _cleanup_partial_vllm_resources(self):
        """æ¸…ç†éƒ¨åˆ†åˆå§‹åŒ–çš„ vLLM è³‡æº"""
        try:
            if hasattr(self, 'llm_engine') and self.llm_engine is not None:
                # vLLM å¼•æ“æ¸…ç†
                try:
                    # åœæ­¢å¾Œå°ä»»å‹™
                    if hasattr(self.llm_engine, '_background_tasks'):
                        self.logger.info("ğŸ”§ åœæ­¢ vLLM å¾Œå°ä»»å‹™...")
                        for task in self.llm_engine._background_tasks:
                            if not task.done():
                                task.cancel()
                    
                    # æ¸…ç†å¼•æ“æ ¸å¿ƒ
                    if hasattr(self.llm_engine, 'engine'):
                        engine = self.llm_engine.engine
                        
                        # æ¸…ç†æ¨¡å‹åŸ·è¡Œå™¨
                        if hasattr(engine, 'model_executor'):
                            self.logger.info("ğŸ”§ æ¸…ç†æ¨¡å‹åŸ·è¡Œå™¨...")
                            model_executor = engine.model_executor
                            
                            # ğŸ”¥ å¼·åˆ¶çµ‚æ­¢å·¥ä½œé€²ç¨‹
                            if hasattr(model_executor, 'workers'):
                                for worker in model_executor.workers:
                                    try:
                                        if hasattr(worker, 'cleanup'):
                                            worker.cleanup()
                                        # å¼·åˆ¶çµ‚æ­¢é€²ç¨‹
                                        if hasattr(worker, 'pid'):
                                            try:
                                                os.kill(worker.pid, signal.SIGTERM)
                                                time.sleep(0.1)  # çµ¦æ™‚é–“æ­£å¸¸é€€å‡º
                                                os.kill(worker.pid, signal.SIGKILL)  # å¼·åˆ¶çµ‚æ­¢
                                            except (OSError, ProcessLookupError):
                                                pass  # é€²ç¨‹å¯èƒ½å·²ç¶“çµ‚æ­¢
                                    except Exception:
                                        pass
                            
                            # æ¸…ç†é©…å‹•å™¨å·¥ä½œé€²ç¨‹
                            if hasattr(model_executor, 'driver_worker'):
                                self.logger.info("ğŸ”§ æ¸…ç†é©…å‹•å™¨å·¥ä½œé€²ç¨‹...")
                                driver_worker = model_executor.driver_worker
                                try:
                                    if hasattr(driver_worker, 'model_runner'):
                                        if hasattr(driver_worker.model_runner, 'model'):
                                            del driver_worker.model_runner.model
                                    if hasattr(driver_worker, 'cleanup'):
                                        driver_worker.cleanup()
                                    # å¼·åˆ¶çµ‚æ­¢é©…å‹•å™¨é€²ç¨‹
                                    if hasattr(driver_worker, 'pid'):
                                        try:
                                            os.kill(driver_worker.pid, signal.SIGTERM)
                                            time.sleep(0.1)
                                            os.kill(driver_worker.pid, signal.SIGKILL)
                                        except (OSError, ProcessLookupError):
                                            pass
                                except Exception:
                                    pass
                            
                            del engine.model_executor
                        
                        # æ¸…ç†èª¿åº¦å™¨
                        if hasattr(engine, 'scheduler'):
                            self.logger.info("ğŸ”§ æ¸…ç†èª¿åº¦å™¨...")
                            del engine.scheduler
                        
                        # æ¸…ç†ç·©å­˜å¼•æ“
                        if hasattr(engine, 'cache_engine'):
                            self.logger.info("ğŸ”§ æ¸…ç†ç·©å­˜å¼•æ“...")
                            del engine.cache_engine
                    
                    # æœ€çµ‚æ¸…ç†å¼•æ“
                    del self.llm_engine
                    self.llm_engine = None
                    self.logger.info("âœ… vLLM å¼•æ“æ¸…ç†å®Œæˆ")
                    
                except Exception as cleanup_error:
                    self.logger.warning(f"æ¸…ç† vLLM å¼•æ“æ™‚å‡ºéŒ¯: {cleanup_error}")
            
            # æ¸…ç†åˆ†æ•£å¼è³‡æº
            try:
                self.logger.info("ğŸ”§ æ¸…ç†åˆ†æ•£å¼è³‡æº...")
                destroy_model_parallel()
                self.logger.info("âœ… åˆ†æ•£å¼è³‡æºæ¸…ç†å®Œæˆ")
            except Exception as cleanup_error:
                self.logger.warning(f"æ¸…ç† model parallel æ™‚å‡ºéŒ¯: {cleanup_error}")
            
            # ğŸ”¥ å¼·åˆ¶çµ‚æ­¢æ‰€æœ‰æ®˜ç•™çš„ vLLM å·¥ä½œé€²ç¨‹
            try:
                self.logger.info("ğŸ”§ æ¸…ç†æ®˜ç•™çš„ vLLM å·¥ä½œé€²ç¨‹...")
                import psutil
                current_pid = os.getpid()
                
                for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                    try:
                        if proc.info['pid'] != current_pid:
                            cmdline = ' '.join(proc.info['cmdline'] or [])
                            if 'vllm' in cmdline.lower() or 'VllmWorkerProcess' in cmdline:
                                self.logger.info(f"   çµ‚æ­¢æ®˜ç•™é€²ç¨‹: {proc.info['pid']}")
                                proc.terminate()
                                proc.wait(timeout=1)
                    except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                        pass
            except ImportError:
                self.logger.warning("psutil æœªå®‰è£ï¼Œç„¡æ³•æ¸…ç†æ®˜ç•™é€²ç¨‹")
            except Exception as process_error:
                self.logger.warning(f"æ¸…ç†æ®˜ç•™é€²ç¨‹æ™‚å‡ºéŒ¯: {process_error}")
            
            # å¼·åˆ¶GPUè¨˜æ†¶é«”æ¸…ç†
            if torch.cuda.is_available():
                self.logger.info("ğŸ”§ åŸ·è¡Œå¼·åˆ¶GPUè¨˜æ†¶é«”æ¸…ç†...")
                torch.cuda.empty_cache()
                torch.cuda.synchronize()  # ç­‰å¾…æ‰€æœ‰CUDAæ“ä½œå®Œæˆ
                gc.collect()
                torch.cuda.empty_cache()  # å†æ¬¡æ¸…ç†
                self.logger.info("âœ… GPUè¨˜æ†¶é«”æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.warning(f"æ¸…ç†éƒ¨åˆ†åˆå§‹åŒ–è³‡æºæ™‚å‡ºéŒ¯: {e}")
    
    def cleanup_models(self):
        """çµ±ä¸€çš„æ¨¡å‹è³‡æºæ¸…ç†æ–¹æ³• - ç¢ºä¿ä¸€æ¬¡ Ctrl+C å°±èƒ½å®Œå…¨é€€å‡º"""
        self.logger.info("ğŸ§¹ é–‹å§‹æ¸…ç† vLLM æ¨¡å‹è³‡æº...")
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæ¸…ç† vLLM å¼•æ“
            if self.llm_engine is not None:
                try:
                    self.logger.info("ğŸ”§ æ¸…ç† vLLM å¼•æ“...")
                    
                    # åœæ­¢æ‰€æœ‰å¾Œå°ä»»å‹™
                    if hasattr(self.llm_engine, '_background_tasks'):
                        try:
                            for task in self.llm_engine._background_tasks:
                                if not task.done():
                                    task.cancel()
                        except Exception:
                            pass
                    
                    # æ·±åº¦æ¸…ç†å¼•æ“çµ„ä»¶
                    if hasattr(self.llm_engine, 'engine'):
                        engine = self.llm_engine.engine
                        
                        # æ¸…ç†æ¨¡å‹åŸ·è¡Œå™¨å’Œå·¥ä½œé€²ç¨‹
                        if hasattr(engine, 'model_executor'):
                            model_executor = engine.model_executor
                            
                            # æ¸…ç†æ‰€æœ‰å·¥ä½œé€²ç¨‹
                            if hasattr(model_executor, 'workers'):
                                for worker in model_executor.workers:
                                    try:
                                        if hasattr(worker, 'cleanup'):
                                            worker.cleanup()
                                    except Exception:
                                        pass
                            
                            # æ¸…ç†é©…å‹•å™¨å·¥ä½œé€²ç¨‹
                            if hasattr(model_executor, 'driver_worker'):
                                driver_worker = model_executor.driver_worker
                                try:
                                    if hasattr(driver_worker, 'model_runner'):
                                        if hasattr(driver_worker.model_runner, 'model'):
                                            del driver_worker.model_runner.model
                                    if hasattr(driver_worker, 'cleanup'):
                                        driver_worker.cleanup()
                                except Exception:
                                    pass
                            
                            del engine.model_executor
                        
                        # æ¸…ç†èª¿åº¦å™¨å’Œç·©å­˜
                        for attr in ['scheduler', 'cache_engine']:
                            if hasattr(engine, attr):
                                try:
                                    delattr(engine, attr)
                                except Exception:
                                    pass
                    
                    # æœ€çµ‚æ¸…ç†å¼•æ“
                    del self.llm_engine
                    self.llm_engine = None
                    self.logger.info("âœ… vLLM å¼•æ“å·²æ¸…ç†")
                    
                except Exception as e:
                    self.logger.warning(f"æ¸…ç† vLLM å¼•æ“æ™‚å‡ºéŒ¯: {e}")
            
            # ç¬¬äºŒæ­¥ï¼šå¼·åˆ¶çµ‚æ­¢æ‰€æœ‰ vLLM ç›¸é—œé€²ç¨‹
            try:
                self.logger.info("ğŸ”§ å¼·åˆ¶çµ‚æ­¢ vLLM å·¥ä½œé€²ç¨‹...")
                import psutil
                import signal
                import time
                
                current_pid = os.getpid()
                killed_count = 0
                
                # æŸ¥æ‰¾ä¸¦çµ‚æ­¢æ‰€æœ‰ vLLM ç›¸é—œé€²ç¨‹
                for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                    try:
                        if proc.info['pid'] != current_pid:
                            cmdline = ' '.join(proc.info['cmdline'] or [])
                            process_name = proc.info['name'] or ''
                            
                            # æª¢æŸ¥æ˜¯å¦æ˜¯ vLLM å·¥ä½œé€²ç¨‹
                            is_vllm_process = (
                                'VllmWorkerProcess' in cmdline or
                                'vllm_worker' in process_name.lower() or
                                ('vllm' in cmdline.lower() and 'worker' in cmdline.lower()) or
                                ('multiproc_worker' in cmdline.lower() and 'vllm' in cmdline.lower())
                            )
                            
                            if is_vllm_process:
                                self.logger.info(f"   çµ‚æ­¢ vLLM å·¥ä½œé€²ç¨‹ PID {proc.info['pid']}")
                                
                                # å˜—è©¦å„ªé›…é€€å‡º
                                proc.terminate()
                                try:
                                    proc.wait(timeout=1)  # ç­‰å¾…1ç§’
                                except psutil.TimeoutExpired:
                                    # å¼·åˆ¶çµ‚æ­¢
                                    proc.kill()
                                    try:
                                        proc.wait(timeout=1)
                                    except psutil.TimeoutExpired:
                                        pass
                                
                                killed_count += 1
                                
                    except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired):
                        pass
                
                if killed_count > 0:
                    self.logger.info(f"âœ… å·²çµ‚æ­¢ {killed_count} å€‹ vLLM å·¥ä½œé€²ç¨‹")
                    time.sleep(0.5)  # çµ¦é€²ç¨‹çµ‚æ­¢ä¸€é»æ™‚é–“
                else:
                    self.logger.info("âœ… æ²’æœ‰ç™¼ç¾ vLLM å·¥ä½œé€²ç¨‹")
                    
            except ImportError:
                self.logger.warning("âš ï¸ psutil æœªå®‰è£ï¼Œç„¡æ³•å¼·åˆ¶çµ‚æ­¢å·¥ä½œé€²ç¨‹")
                self.logger.warning("   å»ºè­°å®‰è£: pip install psutil")
            except Exception as process_error:
                self.logger.warning(f"çµ‚æ­¢å·¥ä½œé€²ç¨‹æ™‚å‡ºéŒ¯: {process_error}")
            
            # ç¬¬ä¸‰æ­¥ï¼šæ¸…ç†åˆ†æ•£å¼è³‡æº
            try:
                self.logger.info("ğŸ”§ æ¸…ç†åˆ†æ•£å¼è³‡æº...")
                destroy_model_parallel()
                self.logger.info("âœ… Model Parallel è³‡æºå·²æ¸…ç†")
            except Exception as e:
                self.logger.warning(f"æ¸…ç† Model Parallel æ™‚å‡ºéŒ¯: {e}")
            
            # ç¬¬å››æ­¥ï¼šæ¸…ç†å…¶ä»–æ¨¡å‹çµ„ä»¶
            components = [
                ('llm_tokenizer', 'Tokenizer'),
                ('embedding_model', 'åµŒå…¥æ¨¡å‹')
            ]
            
            for attr_name, display_name in components:
                if hasattr(self, attr_name) and getattr(self, attr_name) is not None:
                    try:
                        component = getattr(self, attr_name)
                        # å¦‚æœçµ„ä»¶æœ‰æ¸…ç†æ–¹æ³•ï¼Œèª¿ç”¨å®ƒ
                        if hasattr(component, 'cleanup'):
                            component.cleanup()
                        del component
                        setattr(self, attr_name, None)
                        self.logger.info(f"âœ… {display_name}å·²æ¸…ç†")
                    except Exception as e:
                        self.logger.warning(f"æ¸…ç†{display_name}æ™‚å‡ºéŒ¯: {e}")
            
            # ç¬¬äº”æ­¥ï¼šå¼·åˆ¶GPUè¨˜æ†¶é«”æ¸…ç†
            if torch.cuda.is_available():
                try:
                    self.logger.info("ğŸ”§ åŸ·è¡ŒGPUè¨˜æ†¶é«”æ¸…ç†...")
                    
                    # å¤šæ¬¡æ¸…ç†ç¢ºä¿å¾¹åº•
                    for i in range(3):
                        torch.cuda.empty_cache()
                        gc.collect()
                        if i < 2:
                            time.sleep(0.1)
                    
                    # åŒæ­¥æ‰€æœ‰è¨­å‚™
                    torch.cuda.synchronize()
                    
                    # é€å€‹è¨­å‚™æ¸…ç†
                    for device_id in range(torch.cuda.device_count()):
                        try:
                            with torch.cuda.device(device_id):
                                torch.cuda.empty_cache()
                                torch.cuda.synchronize()
                        except Exception:
                            pass
                    
                    self.logger.info("âœ… GPUè¨˜æ†¶é«”å·²æ¸…ç†")
                    
                    # è¨˜éŒ„æ¸…ç†å¾Œç‹€æ…‹
                    if hasattr(self, 'gpu_resource_manager'):
                        try:
                            self.gpu_resource_manager.gpu_manager.log_gpu_status(reason="vLLMå®Œå…¨æ¸…ç†")
                        except Exception:
                            pass
                            
                except Exception as e:
                    self.logger.warning(f"GPUè¨˜æ†¶é«”æ¸…ç†æ™‚å‡ºéŒ¯: {e}")
            
            self.logger.info("âœ… vLLM æ¨¡å‹è³‡æºå·²å®Œå…¨æ¸…ç†")
            
        except Exception as e:
            self.logger.error(f"æ¸…ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            # å³ä½¿å‡ºéŒ¯ä¹Ÿè¦è¨­ç½®è³‡æºç‚º None
            self.llm_engine = None
            self.llm_tokenizer = None
            self.embedding_model = None
    
    async def _cleanup_partial_vllm_resources(self):
        """æ¸…ç†éƒ¨åˆ†åˆå§‹åŒ–çš„ vLLM è³‡æº - å…§éƒ¨ä½¿ç”¨"""
        # èª¿ç”¨çµ±ä¸€çš„æ¸…ç†æ–¹æ³•
        self.cleanup_models()
    
    # ====================================================
    # å…¼å®¹æ€§æ¥å£ - èˆ‡åŸç‰ˆ ModelLoader ä¿æŒä¸€è‡´
    # ====================================================
    
    @property
    def llm_model(self):
        """å…¼å®¹æ€§å±¬æ€§ï¼šè¿”å› vLLM å¼•æ“"""
        return self.llm_engine
    
    def apply_static_kv_cache_optimization(self) -> bool:
        """vLLM è‡ªå‹•è™•ç† KV ç·©å­˜ï¼Œè¿”å› True è¡¨ç¤ºå·²å„ªåŒ–"""
        self.static_cache_enabled = True
        self.optimization_applied.append("PagedAttention (vLLM)")
        self.logger.info("âœ… vLLM PagedAttention å·²å•Ÿç”¨ï¼ˆè‡ªå‹• KV ç·©å­˜å„ªåŒ–ï¼‰")
        return True
    
    def apply_torch_compile_optimization(self) -> bool:
        """vLLM å¼•æ“å·²å„ªåŒ–ï¼Œç„¡éœ€é¡å¤–ç·¨è­¯"""
        self.torch_compile_enabled = True
        self.optimization_applied.append("vLLM Engine Optimization")
        self.logger.info("âœ… vLLM å¼•æ“å„ªåŒ–å·²å•Ÿç”¨ï¼ˆå…§å»ºå„ªåŒ–ï¼‰")
        return True
    
    async def load_llm_model_optimized(self) -> bool:
        """è¼‰å…¥å„ªåŒ–çš„ vLLM æ¨¡å‹"""
        self.logger.info("ğŸš€ è¼‰å…¥é«˜åº¦å„ªåŒ–çš„ vLLM æ¨¡å‹...")
        
        # vLLM å·²ç¶“æ˜¯é«˜åº¦å„ªåŒ–çš„ï¼Œç›´æ¥è¼‰å…¥
        success = await self.load_llm_model()
        if not success:
            raise RuntimeError("vLLM æ¨¡å‹è¼‰å…¥å¤±æ•—")
        
        # æ¨™è¨˜å„ªåŒ–å·²æ‡‰ç”¨
        self.apply_static_kv_cache_optimization()
        self.apply_torch_compile_optimization()
        
        self.logger.info("âœ… vLLM æ¨¡å‹å„ªåŒ–å®Œæˆ")
        self.logger.info("ğŸ“Š vLLM å…§å»ºå„ªåŒ–:")
        self.logger.info("   PagedAttention: KVç·©å­˜å„ªåŒ–")
        self.logger.info("   Continuous Batching: æ‰¹æ¬¡è™•ç†å„ªåŒ–")
        self.logger.info("   Tensor Parallel: å¤šGPUä¸¦è¡Œ")
        self.logger.info("   Memory Efficiency: è¨˜æ†¶é«”å„ªåŒ–")
        
        return True
