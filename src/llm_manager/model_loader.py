"""
æ¨¡å‹è¼‰å…¥å’Œåˆå§‹åŒ–æ¨¡çµ„
è² è²¬è¼‰å…¥å’Œç®¡ç† Qwen-8B ä¸»æ¨¡å‹å’Œ Qwen3-Embedding åµŒå…¥æ¨¡å‹
ä¸»æ¨¡å‹ï¼š8bité‡åŒ–å„ªåŒ–ï¼ŒåµŒå…¥æ¨¡å‹ï¼šFP16åŸç‰ˆå„ªåŒ–
æ”¯æ´ Transformers å’Œ vLLM å…©ç¨®è¼‰å…¥æ¨¡å¼
"""

import logging
import torch
import gc
import os
import warnings
from typing import Optional, Dict, Any, Union, TYPE_CHECKING
from pathlib import Path

if TYPE_CHECKING:
    from .vllm_model_loader import VLLMModelLoader
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    AutoModel, AutoConfig, BitsAndBytesConfig
)
from sentence_transformers import SentenceTransformer

from .gpu_manager import OptimizedEmbeddingModel

# ğŸ”§ è¨­ç½®transformersæ—¥èªŒéæ¿¾ï¼Œæ¸›å°‘ç„¡é—œè­¦å‘Š
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.ERROR)

# éæ¿¾ç‰¹å®šçš„ç„¡é—œè­¦å‘Š
warnings.filterwarnings("ignore", message=".*generation flags.*", category=UserWarning)
warnings.filterwarnings("ignore", message=".*max_new_tokens.*max_length.*", category=UserWarning)


class ModelLoader:
    """æ¨¡å‹è¼‰å…¥å™¨ - ä¸»æ¨¡å‹ä½¿ç”¨8bité‡åŒ–ï¼ŒåµŒå…¥æ¨¡å‹ä½¿ç”¨FP16å„ªåŒ–"""
    
    def __init__(self, config: dict, gpu_resource_manager):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.gpu_resource_manager = gpu_resource_manager
        
        # æ¨¡å‹çµ„ä»¶
        self.llm_model: Optional[AutoModelForCausalLM] = None
        self.llm_tokenizer: Optional[AutoTokenizer] = None
        self.embedding_model: Optional[SentenceTransformer] = None
        
        # ğŸš€ æ€§èƒ½å„ªåŒ–çµ„ä»¶
        self.static_cache_enabled = False
        self.torch_compile_enabled = False
        self.optimization_applied = []
        
        # ğŸ”§ æ€§èƒ½å„ªåŒ–é…ç½® - å¾config.yamlè®€å–
        perf_config = config.get('performance', {})
        self.enable_static_cache = perf_config.get('enable_static_cache', True)
        self.enable_torch_compile = perf_config.get('enable_torch_compile', True)
        self.max_cache_length = perf_config.get('max_cache_length', 2048)
        
        # è¨˜éŒ„é…ç½®ç‹€æ…‹
        self.logger.info("ğŸ”§ æ€§èƒ½å„ªåŒ–é…ç½®:")
        self.logger.info(f"   éœæ…‹KVç·©å­˜: {'å•Ÿç”¨' if self.enable_static_cache else 'ç¦ç”¨'}")
        self.logger.info(f"   Torch Compile: {'å•Ÿç”¨' if self.enable_torch_compile else 'ç¦ç”¨'}")
        self.logger.info(f"   æœ€å¤§ç·©å­˜é•·åº¦: {self.max_cache_length}")
        
        # ğŸ”§ åˆå§‹åŒ–æ™‚æª¢æŸ¥ç”Ÿæˆé…ç½®
        self._validate_initial_config()
        
        # ğŸ”§ æ€§èƒ½å„ªåŒ–é…ç½®
        self.enable_static_cache = config.get('performance', {}).get('enable_static_cache', True)
        self.enable_torch_compile = config.get('performance', {}).get('enable_torch_compile', True)
        self.max_cache_length = config.get('performance', {}).get('max_cache_length', 2048)
        
        # ï¿½ğŸ”§ åˆå§‹åŒ–æ™‚æª¢æŸ¥ç”Ÿæˆé…ç½®
        self._validate_initial_config()
    
    def _validate_initial_config(self):
        """é©—è­‰åˆå§‹é…ç½®ï¼Œæå‰ç™¼ç¾å•é¡Œ"""
        try:
            # æª¢æŸ¥LLMé…ç½®ä¸­çš„æ½›åœ¨å•é¡Œ
            llm_config = self.config.get('models', {}).get('llm', {})
            
            # æª¢æŸ¥æ˜¯å¦è¨­ç½®äº†è¡çªçš„åƒæ•¸
            deprecated_params = ['early_stopping']
            found_deprecated = [param for param in deprecated_params if param in llm_config]
            
            if found_deprecated:
                self.logger.warning(f"âš ï¸ ç™¼ç¾å·²æ£„ç”¨çš„LLMé…ç½®åƒæ•¸: {found_deprecated}")
                self.logger.warning("ğŸ’¡ é€™äº›åƒæ•¸å°‡è¢«è‡ªå‹•éæ¿¾ï¼Œä¸æœƒå½±éŸ¿é‹è¡Œ")
            
            # æª¢æŸ¥VTuberå›æ‡‰é…ç½®
            vtuber_config = self.config.get('vtuber', {}).get('response', {})
            max_tokens = vtuber_config.get('max_tokens', 150)
            
            if max_tokens > 512:
                self.logger.warning(f"âš ï¸ max_tokens={max_tokens} å¯èƒ½éå¤§ï¼Œå»ºè­°æ§åˆ¶åœ¨512ä»¥å…§")
            
            self.logger.info("âœ… æ¨¡å‹é…ç½®é©—è­‰å®Œæˆ")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ é…ç½®é©—è­‰æ™‚å‡ºç¾å•é¡Œ: {e}")
    
    async def load_llm_model(self) -> bool:
        """è¼‰å…¥ä¸»è¦çš„ LLM æ¨¡å‹ (Qwen-8B) - 8bité‡åŒ–å„ªåŒ–"""
        self.logger.info("ğŸš€ è¼‰å…¥ Qwen-8B ä¸»æ¨¡å‹ (8bité‡åŒ–)...")
        
        model_path = self.config['models']['llm']['model_path']
        
        # ğŸ”¥ è™•ç†ç›¸å°è·¯å¾‘ - å¾å·¥ä½œç›®éŒ„çš„ä¸Šç´šç›®éŒ„é–‹å§‹
        if not os.path.isabs(model_path):
            # å¾ scrpitsV2/LLM ç›®éŒ„å‘ä¸Šå…©ç´šåˆ°é” RushiaLLM æ ¹ç›®éŒ„
            current_dir = Path(__file__).parent.parent.parent.parent.parent  # åˆ°é” RushiaLLM æ ¹ç›®éŒ„
            model_path = str(current_dir / model_path)
            self.logger.info(f"ğŸ”§ ç›¸å°è·¯å¾‘è½‰æ›: {self.config['models']['llm']['model_path']} -> {model_path}")
        
        # é©—è­‰è·¯å¾‘æ˜¯å¦å­˜åœ¨
        if not Path(model_path).exists():
            self.logger.error(f"âŒ æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {model_path}")
            return False
        
        # ç²å–æœ€ä½³è¨­å‚™æ˜ å°„ - ä½¿ç”¨çµ±ä¸€çš„è¨˜æ†¶é«”åˆ†é…é…ç½®
        memory_config = self.gpu_resource_manager.gpu_manager.get_memory_allocation_config("llm")
        device_map = None
        max_memory = None
        
        if self.gpu_resource_manager.use_multi_gpu:
            if memory_config.get("use_device_map"):
                device_map = memory_config.get("device_map", "auto")
                max_memory = memory_config.get("max_memory")
                
                if max_memory:
                    # å—æ§çš„è¨˜æ†¶é«”åˆ†é…ç­–ç•¥
                    allowed_devices = memory_config.get("allowed_devices", [])
                    self.logger.info(f"ğŸš€ ä½¿ç”¨çµ±ä¸€è¨˜æ†¶é«”åˆ†é…ç­–ç•¥")
                    self.logger.info(f"   ç›®æ¨™GPU: {allowed_devices}")
                    self.logger.info(f"   è¨˜æ†¶é«”åˆ†é…: {max_memory}")
                    self.logger.info("   çµ±ä¸€è¨˜æ†¶é«”ç®¡ç†æ¨¡å¼")
                    
                    # è¨˜éŒ„è©³ç´°çš„åˆ†é…ä¿¡æ¯
                    self.gpu_resource_manager.gpu_manager.log_memory_allocation_info("llm")
                else:
                    self.logger.info("ğŸ”§ ä½¿ç”¨æ¨™æº–Autoåˆ†é…ç­–ç•¥")
            else:
                device_map = "auto"
                self.logger.info("ğŸ”„ å›é€€åˆ°Autoåˆ†é…ç­–ç•¥")
        else:
            self.logger.info("ğŸ“± å–®GPUæ¨¡å¼ï¼Œä¸ä½¿ç”¨device_map")
        
        # ğŸ”¥ 8bité‡åŒ–é…ç½®
        try:
            # è¼‰å…¥ tokenizer
            self.llm_tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                padding_side="left",
                cache_dir=None  # é¿å…å¤šé€²ç¨‹è¡çª
            )
            
            # è¨­ç½® pad_token
            if self.llm_tokenizer.pad_token is None:
                self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token
            
            # ğŸ”¥ 8bité‡åŒ–é…ç½® - ä½¿ç”¨BitsAndBytesConfig
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_threshold=6.0,
                llm_int8_enable_fp32_cpu_offload=False,
                llm_int8_has_fp16_weight=False
            )
            
            model_kwargs = {
                "trust_remote_code": True,
                "quantization_config": quantization_config,
                "torch_dtype": torch.float16,  # åŸºç¤ç²¾åº¦ç‚ºFP16ï¼Œé‡åŒ–å¾Œç‚º8bit
                "low_cpu_mem_usage": True,
                "cache_dir": None,
                # æ¨™æº–attentionå¯¦ç¾
                "attn_implementation": "eager",  # ä½¿ç”¨æ¨™æº–attention
                "use_cache": True,  # å•Ÿç”¨KVç·©å­˜æå‡ç”Ÿæˆé€Ÿåº¦
            }
            
            # ğŸ”§ CPUæ¨¡å¼å›é€€é…ç½®
            if self.gpu_resource_manager.device == "cpu":
                # CPUæ¨¡å¼ä¸æ”¯æŒé‡åŒ–ï¼Œå›é€€åˆ°FP32
                model_kwargs = {
                    "trust_remote_code": True,
                    "torch_dtype": torch.float32,
                    "low_cpu_mem_usage": True,
                    "cache_dir": None,
                    "attn_implementation": "eager",
                    "use_cache": True,
                }
                self.logger.info("ğŸ”§ CPUæ¨¡å¼ï¼šå›é€€åˆ°FP32ï¼Œç„¡é‡åŒ–")
            
            # æ·»åŠ è¨­å‚™æ˜ å°„å’Œè¨˜æ†¶é«”é™åˆ¶
            if self.gpu_resource_manager.use_multi_gpu:
                model_kwargs["device_map"] = device_map
                if max_memory:
                    model_kwargs["max_memory"] = max_memory
                    self.logger.info(f"ğŸ”§ çµ±ä¸€è¨˜æ†¶é«”é™åˆ¶: {max_memory}")
                self.logger.info("ğŸ”§ å•Ÿç”¨å¤šGPUçµ±ä¸€è¨­å‚™æ˜ å°„")
            elif self.gpu_resource_manager.device != "cpu":
                model_kwargs["device_map"] = "auto"
                self.logger.info("ğŸ”§ å•Ÿç”¨å–®GPU Autoè¨­å‚™æ˜ å°„")
            
            # ğŸ”§ é¦–æ¬¡å˜—è©¦è¼‰å…¥8bité‡åŒ–æ¨¡å‹
            try:
                self.llm_model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    **model_kwargs
                )
                
                self.logger.info("âœ… 8bité‡åŒ–æ¨¡å‹è¼‰å…¥æˆåŠŸï¼ˆé¦–æ¬¡å˜—è©¦ï¼‰")
                
            except Exception as first_error:
                # ç¬¬ä¸€æ¬¡å¤±æ•—ï¼Œå˜—è©¦å›é€€ç­–ç•¥
                self.logger.warning(f"8bité‡åŒ–è¼‰å…¥å¤±æ•—: {first_error}")
                self.logger.info("ğŸ”„ å˜—è©¦å›é€€ç­–ç•¥...")
                
                # ç§»é™¤å¯èƒ½æœ‰å•é¡Œçš„è¨˜æ†¶é«”é™åˆ¶
                fallback_kwargs = model_kwargs.copy()
                if 'max_memory' in fallback_kwargs:
                    del fallback_kwargs['max_memory']
                    self.logger.info("ğŸ”„ ç§»é™¤è¨˜æ†¶é«”é™åˆ¶ï¼Œä½¿ç”¨æ¨™æº–åˆ†é…")
                
                # å¦‚æœæ˜¯å¤šGPUï¼Œåªä½¿ç”¨åŸºæœ¬çš„autoåˆ†é…
                if self.gpu_resource_manager.use_multi_gpu:
                    fallback_kwargs["device_map"] = "auto"
                    self.logger.info("ä½¿ç”¨æ¨™æº–Autoåˆ†é…ä½œç‚ºå›é€€")
                
                self.llm_model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    **fallback_kwargs
                )
                self.logger.info("âœ… 8bité‡åŒ–æ¨¡å‹è¼‰å…¥æˆåŠŸï¼ˆå›é€€ç­–ç•¥ï¼‰")
            
            # å¦‚æœæ˜¯CPUæ¨¡å¼ï¼Œæ‰‹å‹•ç§»å‹•åˆ°è¨­å‚™
            if self.gpu_resource_manager.device == "cpu":
                self.llm_model = self.llm_model.to(self.gpu_resource_manager.device)
            
            # è¨˜éŒ„è¼‰å…¥å¾Œçš„è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³ï¼ˆç°¡åŒ–é¡¯ç¤ºï¼‰
            if torch.cuda.is_available():
                self.gpu_resource_manager.gpu_manager.log_gpu_status(reason="LLMæ¨¡å‹è¼‰å…¥å®Œæˆ")
            
            success_msg = "âœ… Qwen-8B 8bité‡åŒ–æ¨¡å‹è¼‰å…¥æˆåŠŸ"
            if self.gpu_resource_manager.use_multi_gpu:
                total_gpus = len(memory_config.get("allowed_devices", []))
                if total_gpus > 0:
                    success_msg += f" (çµ±ä¸€åˆ†é…: {total_gpus}å¼µå¡ï¼Œæ¯å¡{memory_config.get('memory_per_gpu', '4GB')})"
                else:
                    success_msg += f" (å¤šGPUåˆ†é…: {len(self.gpu_resource_manager.gpu_manager.available_gpus)}å¼µå¡)"
            else:
                success_msg += " (å–®GPU)"
            self.logger.info(success_msg)
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Qwen-8B 8bité‡åŒ–æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            # æ¸…ç†GPUè¨˜æ†¶é«”
            if torch.cuda.is_available():
                self.gpu_resource_manager.gpu_manager.clear_gpu_memory()
            return False
    
    def apply_static_kv_cache_optimization(self) -> bool:
        """æ‡‰ç”¨éœæ…‹KVç·©å­˜å„ªåŒ– - 8bité‡åŒ–å…¼å®¹ç‰ˆ"""
        if self.llm_model is None:
            self.logger.warning("âš ï¸ æ¨¡å‹æœªè¼‰å…¥ï¼Œç„¡æ³•æ‡‰ç”¨éœæ…‹KVç·©å­˜")
            return False
        
        if not self.enable_static_cache:
            self.logger.info("ğŸ”§ éœæ…‹KVç·©å­˜å·²åœ¨é…ç½®ä¸­ç¦ç”¨")
            return False
        
        try:
            self.logger.info("ğŸš€ é–‹å§‹æ‡‰ç”¨éœæ…‹KVç·©å­˜å„ªåŒ–...")
            
            # æª¢æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒéœæ…‹ç·©å­˜
            model_config = getattr(self.llm_model, 'config', None)
            if model_config and hasattr(model_config, 'cache_implementation'):
                # ğŸ”¥ å•Ÿç”¨éœæ…‹KVç·©å­˜
                model_config.cache_implementation = "static"
                self.logger.info("âœ… æ¨¡å‹åŸç”Ÿæ”¯æŒéœæ…‹ç·©å­˜ï¼Œå·²å•Ÿç”¨")
                
                # è¨­ç½®ç·©å­˜é…ç½®
                max_cache_length = self.max_cache_length
                if self.gpu_resource_manager.use_multi_gpu:
                    max_cache_length = min(max_cache_length * 1.5, 3072)  # å¤šGPUå¯ä»¥ä½¿ç”¨æ›´å¤§ç·©å­˜
                
                # è¨­ç½®æœ€å¤§ç·©å­˜é•·åº¦
                if hasattr(model_config, 'max_cache_len'):
                    model_config.max_cache_len = int(max_cache_length)
                
                self.logger.info(f"ğŸ“Š éœæ…‹KVç·©å­˜é…ç½®:")
                self.logger.info(f"   æœ€å¤§ç·©å­˜é•·åº¦: {max_cache_length}")
                self.logger.info(f"   å¤šGPUæ¨¡å¼: {self.gpu_resource_manager.use_multi_gpu}")
                self.logger.info(f"   é æœŸè¨˜æ†¶é«”ç¯€çœ: ~20-30%")
                self.logger.info(f"   é æœŸé€Ÿåº¦æå‡: ~15-25%")
                
            else:
                # æ‰‹å‹•å¯¦ç¾éœæ…‹ç·©å­˜é‚è¼¯
                self._apply_manual_static_cache()
            
            # è¨­ç½®æ¨¡å‹ç‚ºæ¨ç†å„ªåŒ–æ¨¡å¼
            self._configure_model_for_inference()
            
            self.static_cache_enabled = True
            self.optimization_applied.append("éœæ…‹KVç·©å­˜")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ éœæ…‹KVç·©å­˜å„ªåŒ–å¤±æ•—: {e}")
            self.logger.info("ğŸ’¡ å¯èƒ½åŸå› :")
            self.logger.info("   1. æ¨¡å‹ä¸æ”¯æŒéœæ…‹ç·©å­˜")
            self.logger.info("   2. è¨˜æ†¶é«”ä¸è¶³")
            self.logger.info("   3. 8bité‡åŒ–è¡çª")
            return False
    
    def _apply_manual_static_cache(self):
        """æ‰‹å‹•å¯¦ç¾éœæ…‹ç·©å­˜é‚è¼¯ - 8bité‡åŒ–å…¼å®¹"""
        try:
            self.logger.info("ğŸ”§ æ‡‰ç”¨æ‰‹å‹•éœæ…‹ç·©å­˜é…ç½®...")
            
            # è¨­ç½®æ¨¡å‹å…¨åŸŸç·©å­˜é…ç½®
            if hasattr(self.llm_model.config, 'use_cache'):
                self.llm_model.config.use_cache = True
            
            # ç‚ºattentionå±¤è¨­ç½®éœæ…‹ç·©å­˜æ¨™è¨˜
            attention_layers_found = 0
            for name, module in self.llm_model.named_modules():
                if any(keyword in name.lower() for keyword in ['attention', 'attn', 'self_attn']):
                    # è¨­ç½®éœæ…‹ç·©å­˜ç›¸é—œå±¬æ€§
                    if hasattr(module, 'past_key_value') or hasattr(module, 'layer_past'):
                        module._use_static_cache = True
                        module._max_cache_length = self.max_cache_length
                        attention_layers_found += 1
            
            if attention_layers_found > 0:
                self.logger.info(f"âœ… ç‚º{attention_layers_found}å€‹attentionå±¤é…ç½®äº†éœæ…‹ç·©å­˜")
            else:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°attentionå±¤ï¼Œä½¿ç”¨åŸºç¤ç·©å­˜å„ªåŒ–")
            
        except Exception as e:
            self.logger.warning(f"æ‰‹å‹•éœæ…‹ç·©å­˜é…ç½®å¤±æ•—: {e}")
    
    def _configure_model_for_inference(self):
        """é…ç½®æ¨¡å‹ç‚ºæ¨ç†å„ªåŒ–æ¨¡å¼"""
        try:
            # ğŸ”¥ å•Ÿç”¨æ¨ç†æ¨¡å¼
            self.llm_model.eval()
            
            # ğŸ”¥ ç¦ç”¨æ¢¯åº¦è¨ˆç®—ï¼ˆæ¨ç†å°ˆç”¨ï¼‰
            for param in self.llm_model.parameters():
                param.requires_grad = False
            
            # ğŸ”¥ è¨­ç½®ç·©å­˜ç›¸é—œé…ç½®
            if hasattr(self.llm_model.config, 'use_cache'):
                self.llm_model.config.use_cache = True
            
            self.logger.info("âœ… æ¨¡å‹æ¨ç†å„ªåŒ–é…ç½®å®Œæˆ")
            
        except Exception as e:
            self.logger.warning(f"æ¨¡å‹æ¨ç†å„ªåŒ–é…ç½®å¤±æ•—: {e}")
    
    def apply_torch_compile_optimization(self) -> bool:
        """æ‡‰ç”¨Torch Compileå„ªåŒ– - 8bité‡åŒ–å…¼å®¹ç‰ˆ"""
        if self.llm_model is None:
            self.logger.warning("âš ï¸ æ¨¡å‹æœªè¼‰å…¥ï¼Œç„¡æ³•æ‡‰ç”¨Torch Compile")
            return False
        
        if not self.enable_torch_compile:
            self.logger.info("ğŸ”§ Torch Compileå·²åœ¨é…ç½®ä¸­ç¦ç”¨")
            return False
        
        # æª¢æŸ¥Torch Compileå¯ç”¨æ€§
        if not hasattr(torch, 'compile'):
            self.logger.warning("âš ï¸ ç•¶å‰PyTorchç‰ˆæœ¬ä¸æ”¯æŒtorch.compile (éœ€è¦ >= 2.0)")
            return False
        
        if not torch.cuda.is_available():
            self.logger.warning("âš ï¸ Torch Compileéœ€è¦CUDAæ”¯æŒ")
            return False
        
        try:
            self.logger.info("ğŸš€ é–‹å§‹æ‡‰ç”¨Torch Compileå„ªåŒ–...")
            
            # ğŸ”¥ æª¢æŸ¥8bité‡åŒ–å…¼å®¹æ€§ä¸¦é¸æ“‡æœ€ä½³ç·¨è­¯æ¨¡å¼
            compile_mode = self._get_optimal_compile_mode()
            
            self.logger.info(f"ğŸ”§ é¸æ“‡ç·¨è­¯æ¨¡å¼: {compile_mode}")
            self.logger.info("â³ æ­£åœ¨ç·¨è­¯æ¨¡å‹ï¼ˆé¦–æ¬¡ç·¨è­¯éœ€è¦æ™‚é–“ï¼‰...")
            
            # ç·¨è­¯æ¨¡å‹
            compiled_model = torch.compile(
                self.llm_model,
                mode=compile_mode,
                fullgraph=False,  # 8bité‡åŒ–æ¨¡å‹é€šå¸¸éœ€è¦éƒ¨åˆ†åœ–ç·¨è­¯
                dynamic=True,     # æ”¯æŒå‹•æ…‹è¼¸å…¥å½¢ç‹€
                backend="inductor"  # ä½¿ç”¨inductorå¾Œç«¯
            )
            
            # æ›¿æ›åŸæ¨¡å‹
            self.llm_model = compiled_model
            
            # é ç†±ç·¨è­¯æ¨¡å‹
            self._warmup_compiled_model()
            
            self.torch_compile_enabled = True
            self.optimization_applied.append("Torch Compile")
            
            self.logger.info(f"âœ… Torch Compileå„ªåŒ–å®Œæˆ (æ¨¡å¼: {compile_mode})")
            self.logger.info("ğŸ“Š é æœŸæ•ˆèƒ½æå‡:")
            self.logger.info("   æ¨ç†é€Ÿåº¦: +20-40%")
            self.logger.info("   è¨˜æ†¶é«”æ•ˆç‡: è¼•å¾®æå‡")
            self.logger.info("   å¾ŒçºŒæ¨ç†: ç„¡ç·¨è­¯å»¶é²")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Torch Compileå„ªåŒ–å¤±æ•—: {e}")
            self.logger.info("ğŸ’¡ å¯èƒ½åŸå› :")
            self.logger.info("   1. PyTorchç‰ˆæœ¬ < 2.0")
            self.logger.info("   2. CUDAç‰ˆæœ¬ä¸å…¼å®¹")
            self.logger.info("   3. 8bité‡åŒ–èˆ‡ç·¨è­¯å™¨è¡çª")
            self.logger.info("   4. æ¨¡å‹æ¶æ§‹ä¸æ”¯æŒç·¨è­¯")
            return False
    
    def _get_optimal_compile_mode(self) -> str:
        """ç²å–æœ€é©åˆçš„ç·¨è­¯æ¨¡å¼"""
        
        # æ ¹æ“šGPUé…ç½®é¸æ“‡ç·¨è­¯æ¨¡å¼
        if self.gpu_resource_manager.use_multi_gpu:
            # å¤šGPUç’°å¢ƒï¼šä½¿ç”¨ä¿å®ˆæ¨¡å¼ï¼Œé¿å…åŒæ­¥å•é¡Œ
            return "reduce-overhead"
        else:
            # å–®GPUç’°å¢ƒï¼šå¯ä»¥ä½¿ç”¨æ›´æ¿€é€²çš„å„ªåŒ–
            gpu_memory = self.gpu_resource_manager.gpu_manager.get_total_gpu_memory()
            if gpu_memory >= 16:
                return "max-autotune"  # é«˜è¨˜æ†¶é«”ï¼šæœ€å¤§å„ªåŒ–
            else:
                return "reduce-overhead"  # ä½è¨˜æ†¶é«”ï¼šæ¸›å°‘é–‹éŠ·
    
    def _warmup_compiled_model(self):
        """é ç†±ç·¨è­¯æ¨¡å‹ - è§¸ç™¼å¯¦éš›ç·¨è­¯"""
        try:
            if self.llm_tokenizer is None:
                self.logger.warning("âš ï¸ Tokenizeræœªè¼‰å…¥ï¼Œè·³éæ¨¡å‹é ç†±")
                return
            
            self.logger.info("ğŸ”¥ æ­£åœ¨é ç†±ç·¨è­¯æ¨¡å‹ï¼ˆè§¸ç™¼ç·¨è­¯éç¨‹ï¼‰...")
            
            # å‰µå»ºæ¸¬è©¦è¼¸å…¥
            test_input = "Hello"
            test_tokens = self.llm_tokenizer.encode(
                test_input, 
                return_tensors="pt"
            )
            
            # ç§»å‹•åˆ°æ­£ç¢ºè¨­å‚™
            input_device = self.get_model_input_device()
            if input_device:
                test_tokens = test_tokens.to(input_device)
            
            # é ç†±æ¨ç†ï¼ˆè§¸ç™¼ç·¨è­¯ï¼‰
            with torch.no_grad():
                start_time = torch.cuda.Event(enable_timing=True)
                end_time = torch.cuda.Event(enable_timing=True)
                
                start_time.record()
                _ = self.llm_model.generate(
                    test_tokens,
                    max_new_tokens=5,
                    do_sample=False,
                    use_cache=True,
                    pad_token_id=self.llm_tokenizer.pad_token_id
                )
                end_time.record()
                
                torch.cuda.synchronize()
                compile_time = start_time.elapsed_time(end_time) / 1000.0  # è½‰æ›ç‚ºç§’
                
                self.logger.info(f"âœ… æ¨¡å‹é ç†±å®Œæˆ (ç·¨è­¯æ™‚é–“: {compile_time:.2f}ç§’)")
                self.logger.info("ğŸš€ å¾ŒçºŒæ¨ç†å°‡äº«å—ç·¨è­¯åŠ é€Ÿ")
        
        except Exception as e:
            self.logger.warning(f"æ¨¡å‹é ç†±å¤±æ•—: {e}")
            self.logger.info("ğŸ’¡ ç·¨è­¯å¯èƒ½åœ¨é¦–æ¬¡å¯¦éš›ä½¿ç”¨æ™‚è§¸ç™¼")
    
    async def load_llm_model_optimized(self) -> bool:
        """è¼‰å…¥å„ªåŒ–çš„LLMæ¨¡å‹ - 8bité‡åŒ– + éœæ…‹KVç·©å­˜ + Torch Compile"""
        self.logger.info("ğŸš€ è¼‰å…¥é«˜åº¦å„ªåŒ–çš„LLMæ¨¡å‹...")
        
        # å…ˆè¼‰å…¥åŸºç¤8bité‡åŒ–æ¨¡å‹
        success = await self.load_llm_model()
        if not success:
            self.logger.error("âŒ åŸºç¤æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œç„¡æ³•æ‡‰ç”¨å„ªåŒ–")
            return False
        
        # æ‡‰ç”¨å„ªåŒ–
        optimizations_applied = []
        
        # 1. æ‡‰ç”¨éœæ…‹KVç·©å­˜
        if self.apply_static_kv_cache_optimization():
            optimizations_applied.append("éœæ…‹KVç·©å­˜")
        
        # 2. æ‡‰ç”¨Torch Compile
        if self.apply_torch_compile_optimization():
            optimizations_applied.append("Torch Compile")
        
        # çµæœå ±å‘Š
        if optimizations_applied:
            self.logger.info(f"âœ… æˆåŠŸæ‡‰ç”¨å„ªåŒ–: {', '.join(optimizations_applied)}")
            self.logger.info("ğŸ“Š é æœŸç¸½é«”æå‡:")
            
            if len(optimizations_applied) == 2:
                self.logger.info("   æ¨ç†é€Ÿåº¦: +35-65%")
                self.logger.info("   è¨˜æ†¶é«”æ•ˆç‡: +20-30%")
                self.logger.info("   ç·©å­˜å‘½ä¸­ç‡: é¡¯è‘—æå‡")
            elif "éœæ…‹KVç·©å­˜" in optimizations_applied:
                self.logger.info("   æ¨ç†é€Ÿåº¦: +15-25%")
                self.logger.info("   è¨˜æ†¶é«”æ•ˆç‡: +20-30%")
            elif "Torch Compile" in optimizations_applied:
                self.logger.info("   æ¨ç†é€Ÿåº¦: +20-40%")
                self.logger.info("   ç·¨è­¯å„ªåŒ–: å·²å•Ÿç”¨")
        else:
            self.logger.warning("âš ï¸ æœªèƒ½æ‡‰ç”¨ä»»ä½•å„ªåŒ–ï¼Œä½¿ç”¨æ¨™æº–8bité…ç½®")
            self.logger.info("ğŸ’¡ å»ºè­°æª¢æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„performanceè¨­ç½®")
        
        return True
    
    async def load_embedding_model(self) -> bool:
        """è¼‰å…¥åµŒå…¥æ¨¡å‹ (Qwen3-Embedding-0.6B) - FP16åŸç‰ˆï¼Œç„¡é‡åŒ–"""
        self.logger.info("ğŸš€ è¼‰å…¥ Qwen3-Embedding-0.6B åµŒå…¥æ¨¡å‹ (FP16åŸç‰ˆ)...")
        
        model_path = self.config['models']['embedding']['model_path']
        
        # ğŸ”¥ è™•ç†ç›¸å°è·¯å¾‘ - å¾å·¥ä½œç›®éŒ„çš„ä¸Šç´šç›®éŒ„é–‹å§‹
        if not os.path.isabs(model_path):
            # å¾ scrpitsV2/LLM ç›®éŒ„å‘ä¸Šå…©ç´šåˆ°é” RushiaLLM æ ¹ç›®éŒ„
            current_dir = Path(__file__).parent.parent.parent.parent.parent  # åˆ°é” RushiaLLM æ ¹ç›®éŒ„
            model_path = str(current_dir / model_path)
            self.logger.info(f"ğŸ”§ ç›¸å°è·¯å¾‘è½‰æ›: {self.config['models']['embedding']['model_path']} -> {model_path}")
        
        # é©—è­‰è·¯å¾‘æ˜¯å¦å­˜åœ¨
        if not Path(model_path).exists():
            self.logger.error(f"âŒ åµŒå…¥æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {model_path}")
            return False
        
        # ç²å–åµŒå…¥æ¨¡å‹çš„è¨­å‚™åˆ†é… - ä½¿ç”¨çµ±ä¸€é…ç½®
        embedding_config = self.gpu_resource_manager.gpu_manager.get_memory_allocation_config("embedding")
        embedding_device = embedding_config.get("device", self.gpu_resource_manager.device)
        embedding_gpu_id = embedding_config.get("target_gpu")
        
        if embedding_gpu_id is not None:
            embedding_device = f"cuda:{embedding_gpu_id}"
            self.logger.info(f"ğŸ¯ åµŒå…¥æ¨¡å‹å°‡ä½¿ç”¨GPU {embedding_gpu_id} (çµ±ä¸€åˆ†é…)")
        else:
            self.logger.info(f"ğŸ¯ åµŒå…¥æ¨¡å‹ä½¿ç”¨è¨­å‚™: {embedding_device}")
        
        # è¨˜éŒ„åµŒå…¥æ¨¡å‹åˆ†é…ä¿¡æ¯
        self.gpu_resource_manager.gpu_manager.log_memory_allocation_info("embedding")
        
        try:
            # è¼‰å…¥ tokenizer
            embedding_tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                cache_dir=None
            )
            
            # ğŸ”¥ FP16åŸç‰ˆé…ç½® - ç§»é™¤é‡åŒ–
            model_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": torch.float16 if embedding_device != "cpu" else torch.float32,
                "low_cpu_mem_usage": True,
                "cache_dir": None,
                # ğŸ”¥ ç‚ºembeddingæ¨¡å‹ä¹Ÿå•Ÿç”¨å„ªåŒ–
                "use_cache": True,
            }
            
            # CPUæ¨¡å¼ç‰¹æ®Šé…ç½®
            if embedding_device == "cpu":
                model_kwargs["torch_dtype"] = torch.float32
                self.logger.info("ğŸ”§ Embedding CPUæ¨¡å¼ï¼šä½¿ç”¨FP32")
            
            # å¦‚æœæ˜¯å¤šGPUä¸”æŒ‡å®šäº†ç‰¹å®šGPUï¼Œè¨­ç½®device_map
            if embedding_gpu_id is not None:
                model_kwargs["device_map"] = {"": embedding_gpu_id}
                self.logger.info(f"ğŸ”§ Embeddingæ¨¡å‹æ˜ å°„åˆ°GPU {embedding_gpu_id}")
            elif embedding_device != "cpu":
                model_kwargs["device_map"] = "auto"
                self.logger.info("ğŸ”§ Embeddingæ¨¡å‹ä½¿ç”¨Autoè¨­å‚™æ˜ å°„")
            
            # è¼‰å…¥FP16åŸç‰ˆåµŒå…¥æ¨¡å‹
            embedding_model = AutoModel.from_pretrained(
                model_path,
                **model_kwargs
            )
            
            if embedding_device == "cpu":
                embedding_model = embedding_model.to(embedding_device)
            
            # å‰µå»ºè‡ªå®šç¾©çš„åµŒå…¥æ¨¡å‹åŒ…è£å™¨
            self.embedding_model = OptimizedEmbeddingModel(
                model=embedding_model,
                tokenizer=embedding_tokenizer,
                device=embedding_device,
                gpu_id=embedding_gpu_id,
                max_length=self.config['models']['embedding']['max_length'],
                batch_size=self.config['models']['embedding']['batch_size']
            )
            
            success_msg = "âœ… Qwen3-Embedding FP16æ¨¡å‹è¼‰å…¥æˆåŠŸ"
            if embedding_gpu_id is not None:
                success_msg += f" (GPU {embedding_gpu_id})"
            else:
                success_msg += f" ({embedding_device})"
            self.logger.info(success_msg)
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Qwen3-Embedding FP16æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            # å›é€€åˆ°SentenceTransformeræ¨™æº–æ¨¡å¼
            self.logger.info("ğŸ”„ å›é€€åˆ°SentenceTransformeræ¨™æº–è¼‰å…¥æ–¹æ³•...")
            try:
                fallback_device = embedding_device if embedding_device != "cpu" else "cuda" if torch.cuda.is_available() else "cpu"
                
                self.embedding_model = SentenceTransformer(
                    model_path,
                    device=fallback_device,
                    cache_folder=None
                )
                self.logger.info(f"âœ… Qwen3-Embedding æ¨¡å‹è¼‰å…¥æˆåŠŸ (SentenceTransformeræ¨¡å¼, {fallback_device})")
                return True
            except Exception as fallback_error:
                self.logger.error(f"âŒ SentenceTransformerè¼‰å…¥ä¹Ÿå¤±æ•—: {fallback_error}")
                return False
        
        finally:
            # è¨˜éŒ„è¼‰å…¥å¾Œçš„è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³ï¼ˆç°¡åŒ–é¡¯ç¤ºï¼‰
            if torch.cuda.is_available():
                self.gpu_resource_manager.gpu_manager.log_gpu_status(reason="Embeddingæ¨¡å‹è¼‰å…¥å®Œæˆ")
    
    def get_model_input_device(self) -> Optional[str]:
        """ç²å–æ¨¡å‹è¼¸å…¥å±¤çš„è¨­å‚™ä½ç½® - ç²¾ç¢ºå®šä½ç‰ˆ"""
        try:
            if not hasattr(self.llm_model, 'hf_device_map'):
                return None
            
            device_map = self.llm_model.hf_device_map
            
            # æŒ‰å„ªå…ˆç´šæœç´¢è¼¸å…¥å±¤
            input_layer_patterns = [
                # åµŒå…¥å±¤å„ªå…ˆï¼ˆTransformerçš„çœŸæ­£è¼¸å…¥å±¤ï¼‰
                'model.embed_tokens',
                'transformer.wte', 
                'transformer.word_embeddings',
                'embeddings.word_embeddings',
                'embed_tokens',
                'wte',
                # ç¬¬ä¸€å€‹Transformerå±¤ä½œç‚ºå‚™é¸
                'model.layers.0',
                'transformer.h.0',
                'transformer.layers.0',
                'layers.0',
                'h.0'
            ]
            
            for pattern in input_layer_patterns:
                for module_name, device in device_map.items():
                    if pattern in module_name:
                        device_str = f"cuda:{device}" if isinstance(device, int) else str(device)
                        self.logger.debug(f"æ‰¾åˆ°è¼¸å…¥å±¤è¨­å‚™: {module_name} -> {device_str}")
                        return device_str
            
            # å¦‚æœæ²’æ‰¾åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨GPU
            if self.gpu_resource_manager.gpu_manager.available_gpus:
                fallback_device = f"cuda:{self.gpu_resource_manager.gpu_manager.available_gpus[0]}"
                self.logger.warning(f"æœªæ‰¾åˆ°æ˜ç¢ºè¼¸å…¥å±¤ï¼Œä½¿ç”¨å›é€€è¨­å‚™: {fallback_device}")
                return fallback_device
            
            return None
            
        except Exception as e:
            self.logger.error(f"ç²å–æ¨¡å‹è¼¸å…¥è¨­å‚™å¤±æ•—: {e}")
            return None
    
    def get_model_info(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹ä¿¡æ¯ - FP16ç‰ˆæœ¬"""
        try:
            # åŸºç¤LLMä¿¡æ¯
            llm_info = {
                "model_type": "Qwen-8B",
                "precision": "8bit",  # æ›´æ–°ç‚º8bité‡åŒ–
                "quantization": "8bit (BitsAndBytesConfig)",  # 8bité‡åŒ–
                "attention": "Eager",  # æ¨™æº–attentionå¯¦ç¾
                "device": self.gpu_resource_manager.device,
                "multi_gpu": self.gpu_resource_manager.use_multi_gpu,
                "memory_usage": "Unknown"
            }
            
            # åŸºç¤åµŒå…¥æ¨¡å‹ä¿¡æ¯
            embedding_info = {
                "model_type": "Qwen3-Embedding-0.6B", 
                "precision": "FP16",  # æ›´æ–°ç‚ºFP16
                "quantization": "None",  # ç„¡é‡åŒ–
                "device": getattr(self.embedding_model, 'device', self.gpu_resource_manager.device) if self.embedding_model else self.gpu_resource_manager.device,
                "memory_usage": "Unknown"
            }
            
            # å¤šGPUè©³ç´°ä¿¡æ¯
            gpu_info = {}
            if torch.cuda.is_available():
                memory_info = self.gpu_resource_manager.gpu_manager.get_gpu_memory_info()
                
                # ç¸½é«”GPUä¿¡æ¯
                gpu_info = {
                    "total_gpus": len(self.gpu_resource_manager.gpu_manager.available_gpus),
                    "available_gpus": self.gpu_resource_manager.gpu_manager.available_gpus,
                    "total_memory_gb": self.gpu_resource_manager.gpu_manager.get_total_gpu_memory(),
                    "available_memory_gb": self.gpu_resource_manager.gpu_manager.get_available_memory(),
                    "per_gpu_info": {}
                }
                
                # æ¯å¼µå¡çš„è©³ç´°ä¿¡æ¯
                for gpu_id, info in memory_info.items():
                    gpu_name = self.gpu_resource_manager.gpu_manager.gpu_info[gpu_id]['name']
                    gpu_info["per_gpu_info"][f"gpu_{gpu_id}"] = {
                        "name": gpu_name,
                        "total_gb": info['total_gb'],
                        "allocated_gb": info['allocated_gb'],
                        "reserved_gb": info['reserved_gb'],
                        "free_gb": info['free_gb'],
                        "utilization_percent": info['utilization']
                    }
                
                # æ›´æ–°LLMå’Œembeddingçš„è¨˜æ†¶é«”ä¿¡æ¯
                if self.gpu_resource_manager.use_multi_gpu:
                    # è¨ˆç®—LLMä½¿ç”¨çš„ç¸½è¨˜æ†¶é«”ï¼ˆä½¿ç”¨çµ±ä¸€é…ç½®çš„GPUæ•¸é‡ï¼‰
                    memory_config = self.gpu_resource_manager.gpu_manager.get_memory_allocation_config("llm")
                    llm_gpus = memory_config.get("allowed_devices", self.gpu_resource_manager.gpu_manager.available_gpus[:4])
                    llm_memory_used = sum(memory_info.get(gpu_id, {}).get('reserved_gb', 0) for gpu_id in llm_gpus)
                    llm_info["gpu_ids"] = llm_gpus
                    llm_info["memory_usage_gb"] = round(llm_memory_used, 2)
                    llm_info["memory_per_gpu"] = memory_config.get("memory_per_gpu", "4GB")
                    
                    # åµŒå…¥æ¨¡å‹è¨˜æ†¶é«”ä½¿ç”¨
                    embedding_config = self.gpu_resource_manager.gpu_manager.get_memory_allocation_config("embedding")
                    embedding_gpu = embedding_config.get("target_gpu")
                    if embedding_gpu is not None:
                        embedding_memory = memory_info.get(embedding_gpu, {}).get('reserved_gb', 0)
                        embedding_info["gpu_id"] = embedding_gpu
                        embedding_info["memory_usage_gb"] = round(embedding_memory, 2)
                else:
                    # å–®GPUæ¨¡å¼
                    gpu_0_info = memory_info.get(0, {})
                    llm_info["memory_usage_gb"] = round(gpu_0_info.get('reserved_gb', 0), 2)
                    embedding_info["memory_usage_gb"] = round(gpu_0_info.get('allocated_gb', 0), 2)
            
            return {
                "llm_model": llm_info,
                "embedding_model": embedding_info,
                "gpu_cluster": gpu_info,
                "performance_optimization": {
                    "llm_quantization": "8bit",
                    "embedding_precision": "FP16",
                    "attention_implementation": "eager",
                    "memory_allocation": "Unified 4Ã—4.5GB strategy",
                    "expected_benefits": "Consistent memory usage, optimized for 4-GPU setup"
                }
            }
            
        except Exception as e:
            self.logger.error(f"ç²å–æ¨¡å‹ä¿¡æ¯å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def get_optimized_generation_config(self, base_config: Dict[str, Any]) -> Dict[str, Any]:
        """ç²å–8bité‡åŒ–å„ªåŒ–çš„ç”Ÿæˆé…ç½® - ä¿®å¾©ç‰ˆï¼Œé¿å…åƒæ•¸è¡çª"""
        optimized_config = base_config.copy()
        
        if torch.cuda.is_available():
            # ğŸ”¥ 8bité‡åŒ–å„ªåŒ–åƒæ•¸
            optimized_config.update({
                "use_cache": True,  # å•Ÿç”¨KVç·©å­˜
                "do_sample": True,
                # åŸºç¤å„ªåŒ–åƒæ•¸
                "pad_token_id": self.llm_tokenizer.pad_token_id if self.llm_tokenizer else None,
                "eos_token_id": self.llm_tokenizer.eos_token_id if self.llm_tokenizer else None,
                # æå‡ä¸¦è¡Œè™•ç†èƒ½åŠ›
                "output_attentions": False,  # é—œé–‰attentionè¼¸å‡ºä»¥ç¯€çœè¨˜æ†¶é«”
                "output_hidden_states": False,  # é—œé–‰hidden statesè¼¸å‡º
            })
            
            # ğŸ”¥ ä¿®å¾©ï¼šä¸è¨­ç½®max_lengthï¼Œé¿å…èˆ‡max_new_tokensè¡çª
            # åªæœ‰åœ¨æ²’æœ‰è¨­ç½®max_new_tokensæ™‚æ‰è¨­ç½®max_length
            if "max_new_tokens" not in optimized_config:
                # æ ¹æ“šGPUæ•¸é‡èª¿æ•´æœ€å¤§é•·åº¦
                if self.gpu_resource_manager.use_multi_gpu:
                    # å¤šGPUå¯ä»¥è™•ç†æ›´å¤§çš„åºåˆ—
                    optimized_config["max_length"] = 3072
                else:
                    # å–®GPUä¿æŒä¿å®ˆè¨­ç½®
                    optimized_config["max_length"] = 2048
            
            self.logger.info("ğŸš€ å·²å•Ÿç”¨8bité‡åŒ–å„ªåŒ–ç”Ÿæˆé…ç½®")
        else:
            # æ¨™æº–é…ç½®å›é€€
            optimized_config.update({
                "use_cache": True,
                "output_attentions": False,
                "output_hidden_states": False,
            })
            self.logger.info("ğŸ”§ ä½¿ç”¨æ¨™æº–ç”Ÿæˆé…ç½®")
        
        return optimized_config
    
    def get_static_cache_generation_config(self, base_config: Dict[str, Any]) -> Dict[str, Any]:
        """ç²å–éœæ…‹KVç·©å­˜å„ªåŒ–çš„ç”Ÿæˆé…ç½®"""
        optimized_config = base_config.copy()
        
        # ğŸ”¥ éœæ…‹KVç·©å­˜å°ˆç”¨é…ç½®
        optimized_config.update({
            # å•Ÿç”¨ç·©å­˜
            "use_cache": True,
            
            # éœæ…‹ç·©å­˜é…ç½®
            "cache_implementation": "static" if hasattr(self.llm_model, 'config') and hasattr(self.llm_model.config, 'cache_implementation') else None,
            
            # å„ªåŒ–ç”Ÿæˆéç¨‹
            "do_sample": True,
            "temperature": 0.7,
            "top_p": 0.8,
            
            # é¿å…åƒæ•¸è¡çª
            "output_attentions": False,
            "output_hidden_states": False,
            "return_dict_in_generate": True,
            
            # Tokenç›¸é—œé…ç½®
            "pad_token_id": self.llm_tokenizer.pad_token_id if self.llm_tokenizer else None,
            "eos_token_id": self.llm_tokenizer.eos_token_id if self.llm_tokenizer else None,
        })
        
        # è¨­ç½®ç·©å­˜é•·åº¦
        cache_length = self.max_cache_length
        if self.gpu_resource_manager.use_multi_gpu:
            cache_length = min(cache_length * 1.5, 3072)
        
        # åªåœ¨æ²’æœ‰max_new_tokensæ™‚è¨­ç½®max_length
        if "max_new_tokens" not in optimized_config:
            optimized_config["max_length"] = cache_length
        
        # æ¸…ç†Noneå€¼
        optimized_config = {k: v for k, v in optimized_config.items() if v is not None}
        
        self.logger.info("ğŸš€ å·²å•Ÿç”¨éœæ…‹KVç·©å­˜ç”Ÿæˆé…ç½®")
        self.logger.info(f"   ç·©å­˜é•·åº¦: {cache_length}")
        
        return optimized_config
    
    def get_torch_compile_generation_config(self, base_config: Dict[str, Any]) -> Dict[str, Any]:
        """ç²å–Torch Compileå„ªåŒ–çš„ç”Ÿæˆé…ç½®"""
        optimized_config = base_config.copy()
        
        # ğŸ”¥ Torch Compileå°ˆç”¨å„ªåŒ–
        optimized_config.update({
            # ç·©å­˜é…ç½®
            "use_cache": True,
            
            # ç·¨è­¯å‹å¥½çš„æ¡æ¨£é…ç½®
            "do_sample": True,
            "temperature": 0.7,
            "top_p": 0.8,
            
            # é¿å…å‹•æ…‹å½¢ç‹€è®ŠåŒ–ï¼ˆç·¨è­¯å™¨å‹å¥½ï¼‰
            "pad_token_id": self.llm_tokenizer.pad_token_id if self.llm_tokenizer else None,
            "eos_token_id": self.llm_tokenizer.eos_token_id if self.llm_tokenizer else None,
            
            # é—œé–‰ä¸å¿…è¦çš„è¼¸å‡º
            "output_attentions": False,
            "output_hidden_states": False,
            "return_dict_in_generate": True,
            
            # æ‰¹æ¬¡é…ç½®ï¼ˆç·¨è­¯å™¨å„ªåŒ–ï¼‰
            "num_beams": 1,  # é¿å…è¤‡é›œçš„beam search
        })
        
        # æ ¹æ“šGPUé…ç½®èª¿æ•´
        if self.gpu_resource_manager.use_multi_gpu:
            optimized_config.update({
                "synced_gpus": True,
            })
            if "max_new_tokens" not in optimized_config:
                optimized_config["max_length"] = 3072
        else:
            if "max_new_tokens" not in optimized_config:
                optimized_config["max_length"] = 2048
        
        self.logger.info("ğŸš€ å·²å•Ÿç”¨Torch Compileå„ªåŒ–ç”Ÿæˆé…ç½®")
        
        return optimized_config
    
    def get_combined_optimization_config(self, base_config: Dict[str, Any]) -> Dict[str, Any]:
        """ç²å–çµ„åˆå„ªåŒ–é…ç½® - éœæ…‹ç·©å­˜ + Torch Compile"""
        # å…ˆæ‡‰ç”¨éœæ…‹ç·©å­˜é…ç½®
        config = self.get_static_cache_generation_config(base_config)
        
        # å†æ‡‰ç”¨Torch Compileé…ç½®ï¼ˆåˆä½µå…¼å®¹è¨­ç½®ï¼‰
        torch_config = self.get_torch_compile_generation_config(base_config)
        
        # åˆä½µé…ç½®ï¼Œå„ªå…ˆä½¿ç”¨å…¼å®¹çš„è¨­ç½®
        config.update({
            # ä¿æŒéœæ…‹ç·©å­˜è¨­ç½®
            "cache_implementation": config.get("cache_implementation"),
            
            # ä½¿ç”¨Torch Compileçš„æ‰¹æ¬¡å„ªåŒ–
            "num_beams": torch_config.get("num_beams", 1),
            "synced_gpus": torch_config.get("synced_gpus", False),
            
            # çµ±ä¸€çš„é•·åº¦è¨­ç½®
            "max_length": torch_config.get("max_length", config.get("max_length")),
        })
        
        # æ·»åŠ çµ„åˆå„ªåŒ–ç‰¹æ®Šé…ç½®
        config.update({
            # æœ€å¤§åŒ–ç·©å­˜æ•ˆç‡
            "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
            
            # ç·¨è­¯å™¨å‹å¥½è¨­ç½®
            "early_stopping": False,
        })
        
        # è¨˜éŒ„æ‡‰ç”¨çš„å„ªåŒ–
        applied_optimizations = []
        if self.static_cache_enabled:
            applied_optimizations.append("éœæ…‹KVç·©å­˜")
        if self.torch_compile_enabled:
            applied_optimizations.append("Torch Compile")
        
        self.logger.info(f"ğŸš€ å·²å•Ÿç”¨çµ„åˆå„ªåŒ–é…ç½®: {', '.join(applied_optimizations)}")
        
        return config
    
    def get_optimization_status(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰å„ªåŒ–ç‹€æ…‹"""
        return {
            "static_cache_enabled": self.static_cache_enabled,
            "torch_compile_enabled": self.torch_compile_enabled,
            "optimizations_applied": self.optimization_applied.copy(),
            "max_cache_length": self.max_cache_length,
            "performance_config": {
                "enable_static_cache": self.enable_static_cache,
                "enable_torch_compile": self.enable_torch_compile,
            },
            "model_info": {
                "quantization": "8bit",
                "precision": "FP16",
                "multi_gpu": self.gpu_resource_manager.use_multi_gpu,
                "device": self.gpu_resource_manager.device,
            }
        }
    
    def cleanup_models(self):
        """æ¸…ç†æ¨¡å‹è³‡æº"""
        self.logger.info("é–‹å§‹æ¸…ç†æ¨¡å‹è³‡æº...")
        
        # æ¸…ç†æ¨¡å‹
        if self.llm_model:
            del self.llm_model
            self.llm_model = None
            
        if self.embedding_model:
            del self.embedding_model
            self.embedding_model = None
        
        # Pythonåƒåœ¾å›æ”¶
        gc.collect()
        
        self.logger.info("âœ… æ¨¡å‹è³‡æºå·²æ¸…ç†")


# ====================================================
# ğŸ­ æ¨¡å‹è¼‰å…¥å™¨å·¥å» å‡½æ•¸
# ====================================================

def create_model_loader(config: dict, gpu_resource_manager):
    """
    æ¨¡å‹è¼‰å…¥å™¨å·¥å» å‡½æ•¸
    æ ¹æ“šé…ç½®é¸æ“‡ Transformers æˆ– vLLM è¼‰å…¥æ–¹å¼
    """
    logger = logging.getLogger(__name__)
    
    # æª¢æŸ¥è¼‰å…¥æ¨¡å¼é…ç½®
    loading_mode = config.get('models', {}).get('llm', {}).get('loading_mode', 'transformers')
    
    if loading_mode.lower() == 'vllm':
        logger.info("ğŸš€ é¸æ“‡ vLLM è¼‰å…¥æ¨¡å¼")
        try:
            from .vllm_model_loader import VLLMModelLoader
            return VLLMModelLoader(config, gpu_resource_manager)
        except ImportError as e:
            logger.error(f"âŒ vLLM ä¸å¯ç”¨: {e}")
            logger.error("è«‹å®‰è£ vLLM: pip install vllm")
            raise ImportError("vLLM è¼‰å…¥å™¨ä¸å¯ç”¨ï¼Œè«‹å®‰è£ vLLM æˆ–åˆ‡æ›åˆ° transformers æ¨¡å¼")
    
    elif loading_mode.lower() == 'transformers':
        logger.info("ğŸš€ é¸æ“‡ Transformers è¼‰å…¥æ¨¡å¼")
        return ModelLoader(config, gpu_resource_manager)
    
    else:
        logger.error(f"âŒ ä¸æ”¯æ´çš„è¼‰å…¥æ¨¡å¼: {loading_mode}")
        logger.error("æ”¯æ´çš„æ¨¡å¼: 'transformers', 'vllm'")
        raise ValueError(f"ä¸æ”¯æ´çš„è¼‰å…¥æ¨¡å¼: {loading_mode}")


def get_available_loading_modes() -> Dict[str, Dict[str, Any]]:
    """ç²å–å¯ç”¨çš„è¼‰å…¥æ¨¡å¼ä¿¡æ¯"""
    modes = {
        'transformers': {
            'available': True,
            'description': 'Hugging Face Transformers - 8bité‡åŒ–å„ªåŒ–',
            'features': [
                '8bité‡åŒ–ç¯€çœè¨˜æ†¶é«”',
                'éœæ…‹KVç·©å­˜å„ªåŒ–',
                'Torch CompileåŠ é€Ÿ',
                'å¤šGPUè¨­å‚™æ˜ å°„'
            ],
            'best_for': 'è¨˜æ†¶é«”æœ‰é™æˆ–éœ€è¦ç²¾ç¢ºæ§åˆ¶çš„ç’°å¢ƒ'
        }
    }
    
    # æª¢æŸ¥ vLLM å¯ç”¨æ€§
    try:
        import vllm
        modes['vllm'] = {
            'available': True,
            'description': 'vLLMé«˜æ€§èƒ½æ¨ç†å¼•æ“',
            'features': [
                'PagedAttentionè¨˜æ†¶é«”å„ªåŒ–',
                'Continuous Batchingæ‰¹æ¬¡è™•ç†',
                'Tensor Parallelå¤šGPUä¸¦è¡Œ',
                'é«˜ååé‡æ¨ç†'
            ],
            'best_for': 'é«˜ä¸¦ç™¼æ¨ç†æˆ–å¤§è¦æ¨¡éƒ¨ç½²',
            'version': vllm.__version__
        }
    except ImportError:
        modes['vllm'] = {
            'available': False,
            'description': 'vLLMé«˜æ€§èƒ½æ¨ç†å¼•æ“ (æœªå®‰è£)',
            'install_command': 'pip install vllm',
            'features': [
                'PagedAttentionè¨˜æ†¶é«”å„ªåŒ–',
                'Continuous Batchingæ‰¹æ¬¡è™•ç†', 
                'Tensor Parallelå¤šGPUä¸¦è¡Œ',
                'é«˜ååé‡æ¨ç†'
            ],
            'best_for': 'é«˜ä¸¦ç™¼æ¨ç†æˆ–å¤§è¦æ¨¡éƒ¨ç½²'
        }
    
    return modes


def validate_loading_mode_config(config: dict) -> Dict[str, Any]:
    """é©—è­‰è¼‰å…¥æ¨¡å¼é…ç½®"""
    logger = logging.getLogger(__name__)
    
    validation_result = {
        'valid': True,
        'warnings': [],
        'errors': [],
        'recommendations': []
    }
    
    llm_config = config.get('models', {}).get('llm', {})
    loading_mode = llm_config.get('loading_mode', 'transformers')
    
    # æª¢æŸ¥è¼‰å…¥æ¨¡å¼æ˜¯å¦æ”¯æ´
    available_modes = get_available_loading_modes()
    
    if loading_mode not in available_modes:
        validation_result['valid'] = False
        validation_result['errors'].append(f"ä¸æ”¯æ´çš„è¼‰å…¥æ¨¡å¼: {loading_mode}")
        validation_result['recommendations'].append(f"å¯ç”¨æ¨¡å¼: {list(available_modes.keys())}")
        return validation_result
    
    if not available_modes[loading_mode]['available']:
        validation_result['valid'] = False
        validation_result['errors'].append(f"è¼‰å…¥æ¨¡å¼ {loading_mode} ä¸å¯ç”¨")
        if 'install_command' in available_modes[loading_mode]:
            validation_result['recommendations'].append(
                f"å®‰è£å‘½ä»¤: {available_modes[loading_mode]['install_command']}"
            )
        return validation_result
    
    # vLLM ç‰¹å®šé…ç½®æª¢æŸ¥
    if loading_mode == 'vllm':
        vllm_config = llm_config.get('vllm', {})
        
        # æª¢æŸ¥ Tensor Parallel é…ç½®
        tensor_parallel = vllm_config.get('tensor_parallel_size', 'auto')
        available_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0
        
        if tensor_parallel != 'auto' and isinstance(tensor_parallel, int):
            if tensor_parallel > available_gpus:
                validation_result['warnings'].append(
                    f"Tensor Parallel å¤§å° ({tensor_parallel}) è¶…éå¯ç”¨GPUæ•¸é‡ ({available_gpus})"
                )
                validation_result['recommendations'].append("å°‡è‡ªå‹•èª¿æ•´ç‚ºå¯ç”¨GPUæ•¸é‡")
        
        # æª¢æŸ¥è¨˜æ†¶é«”é…ç½®
        gpu_memory_util = vllm_config.get('gpu_memory_utilization', 0.85)
        if gpu_memory_util > 0.95:
            validation_result['warnings'].append(
                f"GPUè¨˜æ†¶é«”åˆ©ç”¨ç‡ ({gpu_memory_util}) éé«˜ï¼Œå¯èƒ½å°è‡´OOM"
            )
            validation_result['recommendations'].append("å»ºè­°è¨­ç½®ç‚º 0.85-0.9 ä¹‹é–“")
    
    # Transformers ç‰¹å®šé…ç½®æª¢æŸ¥
    elif loading_mode == 'transformers':
        # æª¢æŸ¥é‡åŒ–é…ç½®
        quantization = llm_config.get('quantization', '8bit')
        if quantization not in ['8bit', '4bit', 'none']:
            validation_result['warnings'].append(f"ä¸å»ºè­°çš„é‡åŒ–è¨­ç½®: {quantization}")
            validation_result['recommendations'].append("å»ºè­°ä½¿ç”¨: '8bit', '4bit', 'none'")
    
    logger.info(f"âœ… è¼‰å…¥æ¨¡å¼é…ç½®é©—è­‰å®Œæˆ: {loading_mode}")
    if validation_result['warnings']:
        for warning in validation_result['warnings']:
            logger.warning(f"âš ï¸ {warning}")
    
    return validation_result
