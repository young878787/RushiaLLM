"""
å›æ‡‰ç”Ÿæˆå’Œè™•ç†æ¨¡çµ„
è² è²¬ç”Ÿæˆå›æ‡‰ã€è™•ç†æç¤ºè©ã€æƒ…æ„Ÿåˆ†ææ•´åˆå’Œå°è©±æ­·å²è™•ç†
"""

import asyncio
import logging
import torch
import datetime
from typing import Optional, List, Dict, Any, Tuple


class ResponseGenerator:
    """å›æ‡‰ç”Ÿæˆå™¨ - è² è²¬ç”Ÿæˆå’Œè™•ç†æ‰€æœ‰å›æ‡‰é‚è¼¯"""
    
    def __init__(self, config: dict, model_loader, gpu_resource_manager, personality_core, response_filter):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # çµ„ä»¶å¼•ç”¨
        self.model_loader = model_loader
        self.gpu_resource_manager = gpu_resource_manager
        self.personality_core = personality_core
        self.response_filter = response_filter
        
        # RAGç³»çµ±å¼•ç”¨ï¼ˆå°‡ç”±å¤–éƒ¨è¨­ç½®ï¼‰
        self._rag_system_ref = None
        
        # å°è©±è¨ˆæ•¸å™¨ï¼ˆç”¨æ–¼å‹•æ…‹ç³»çµ±æç¤ºè©ï¼‰
        self.conversation_count = 0
    
    def set_rag_system_reference(self, rag_system):
        """è¨­ç½®RAGç³»çµ±å¼•ç”¨ï¼ˆç”¨æ–¼å¢å¼·æª¢ç´¢ï¼‰"""
        self._rag_system_ref = rag_system
    
    async def generate_response(
        self, 
        prompt: str, 
        context: Optional[str] = None,
        conversation_history: Optional[List[tuple]] = None,
        stream: bool = False,
        rag_enabled: bool = True
    ) -> str:
        """ç”Ÿæˆå›æ‡‰ - æ”¯æŒå‹•æ…‹ç³»çµ±æç¤ºè©å’Œå°è©±æ­·å²çš„èªç¾©åˆ†æå¢å¼·ç‰ˆ"""
        try:
            # å¢åŠ å°è©±è¨ˆæ•¸
            self.conversation_count += 1
            
            # æƒ…æ„Ÿåˆ†æ - æ”¯æŒå°è©±æ­·å²
            if hasattr(self.personality_core, 'analyze_emotional_triggers_enhanced'):
                emotional_analysis = self.personality_core.analyze_emotional_triggers_enhanced(
                    prompt, conversation_history
                )
            else:
                emotional_analysis = self.personality_core.analyze_emotional_triggers(prompt)
            
            
            # æ ¹æ“šæƒ…ç·’åˆ†æèª¿æ•´å¿ƒæƒ…
            if emotional_analysis['suggested_mood'] != self.personality_core.current_mood:
                self.personality_core.update_mood(emotional_analysis['suggested_mood'])
            
            # å‰µå»ºä¸Šä¸‹æ–‡æç¤ºï¼ˆç”¨æ–¼å‹•æ…‹ç³»çµ±æç¤ºè©ï¼‰
            context_hints = self._create_context_hints(emotional_analysis, conversation_history)
            
            # ä½¿ç”¨å‹•æ…‹ç³»çµ±æç¤ºè©ç”Ÿæˆï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(self.personality_core, 'generate_dynamic_system_prompt'):
                system_prompt = self.personality_core.generate_dynamic_system_prompt(
                    conversation_count=self.conversation_count,
                    context_hints=context_hints
                )
                self.logger.info(f"ğŸ“ ä½¿ç”¨å‹•æ…‹ç³»çµ±æç¤ºè© (ç¬¬ {self.conversation_count} æ¬¡å°è©±)")
            elif hasattr(self.personality_core, 'generate_system_prompt_enhanced'):
                system_prompt = self.personality_core.generate_system_prompt_enhanced()
            else:
                system_prompt = self.personality_core.generate_system_prompt()
            
            # ç²å–å›æ‡‰æç¤º
            response_hints = self.personality_core.get_contextual_response_hints(emotional_analysis)
            
            # èªç¾©å‘é‡å¢å¼·çš„ä¸Šä¸‹æ–‡æª¢ç´¢ (åªåœ¨RAGå•Ÿç”¨æ™‚)
            enhanced_context = context
            if rag_enabled and not context:
                enhanced_context = await self._get_semantic_vector_enhanced_context(prompt, emotional_analysis)
            
            # æ§‹å»ºå®Œæ•´çš„æç¤ºï¼ˆåŒ…å«å°è©±æ­·å²ï¼‰
            full_prompt = self._build_enhanced_prompt_with_history(
                system_prompt, prompt, enhanced_context, conversation_history, emotional_analysis, response_hints
            )
            
            # ç²å–å‹•æ…‹ç”Ÿæˆåƒæ•¸
            generation_config = self._get_adaptive_generation_config(emotional_analysis)
            
            # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ vLLM
            loading_mode = self.config.get('models', {}).get('llm', {}).get('loading_mode', 'transformers')
            
            if loading_mode == 'vllm':
                # ä½¿ç”¨ vLLM ç”Ÿæˆ
                response = await self._generate_vllm_response(full_prompt, generation_config)
            else:
                # ä½¿ç”¨ Transformers ç”Ÿæˆ
                # Tokenize
                inputs = self.model_loader.llm_tokenizer(
                    full_prompt,
                    return_tensors="pt",
                    truncation=True,
                    max_length=self.config['models']['llm']['max_length'] - 512
                ).to(self.gpu_resource_manager.device)
                
                # ç”Ÿæˆå›æ‡‰ - åœ¨åŸ·è¡Œå™¨ä¸­é‹è¡Œï¼Œé¿å…é˜»å¡
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    self._generate_model_response,
                    inputs,
                    generation_config
                )
            
            # æ‡‰ç”¨éæ¿¾å™¨è™•ç†å›æ‡‰
            filtered_response = self.response_filter.filter_response(response)
            
            # é©—è­‰å›æ‡‰æœ‰æ•ˆæ€§
            if not self.response_filter.validate_response(filtered_response):
                self.logger.warning("ç”Ÿæˆçš„å›æ‡‰ç„¡æ•ˆï¼Œä½¿ç”¨å‚™ç”¨å›æ‡‰")
                return "å—¯å—¯ï¼Œæˆ‘éœ€è¦æƒ³æƒ³å‘¢ï½"
            
            return filtered_response
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘ç¾åœ¨ç„¡æ³•å›æ‡‰ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
    
    async def _generate_vllm_response(self, prompt: str, generation_config: Dict[str, Any]) -> str:
        """ä½¿ç”¨ vLLM å¼•æ“ç”Ÿæˆå›æ‡‰"""
        try:
            # æª¢æŸ¥æ˜¯å¦æœ‰ vLLM å°ˆç”¨çš„ç”Ÿæˆæ–¹æ³•
            if hasattr(self.model_loader, 'generate_response_vllm'):
                # å‰µå»º vLLM æ¡æ¨£åƒæ•¸
                sampling_params = self.model_loader.create_sampling_params(
                    temperature=generation_config.get('temperature', 0.75),
                    top_p=generation_config.get('top_p', 0.8),
                    top_k=generation_config.get('top_k', 40),
                    max_tokens=generation_config.get('max_new_tokens', 150),
                    min_tokens=generation_config.get('min_new_tokens', 25),
                    repetition_penalty=generation_config.get('repetition_penalty', 1.15),
                    length_penalty=generation_config.get('length_penalty', 1.0)
                )
                
                # ä½¿ç”¨ vLLM ç”Ÿæˆ
                response = await self.model_loader.generate_response_vllm(
                    prompt=prompt,
                    sampling_params=sampling_params
                )
                
                self.logger.debug(f"vLLM ç”Ÿæˆå®Œæˆï¼Œé•·åº¦: {len(response)}")
                return response
            else:
                self.logger.error("vLLM æ¨¡å‹è¼‰å…¥å™¨ç¼ºå°‘ generate_response_vllm æ–¹æ³•")
                return "å—¯å—¯ï¼Œæˆ‘éœ€è¦æƒ³æƒ³å‘¢ï½"
                
        except Exception as e:
            self.logger.error(f"vLLM ç”Ÿæˆå›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘ç¾åœ¨ç„¡æ³•å›æ‡‰ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
    
    def _generate_model_response(self, inputs, generation_config) -> str:
        """åŒæ­¥ç”Ÿæˆæ¨¡å‹å›æ‡‰ï¼ˆåœ¨åŸ·è¡Œå™¨ä¸­é‹è¡Œï¼‰- å¤šGPUå„ªåŒ–ç‰ˆ"""
        try:
            # å¦‚æœæ˜¯å¤šGPUç’°å¢ƒï¼Œç¢ºä¿è¼¸å…¥åœ¨æ­£ç¢ºçš„è¨­å‚™ä¸Š
            if self.gpu_resource_manager.use_multi_gpu and hasattr(self.model_loader.llm_model, 'hf_device_map'):
                # æ›´ç²¾æº–åœ°å®šä½è¼¸å…¥å±¤è¨­å‚™
                input_device = self.model_loader.get_model_input_device()
                current_device = str(inputs['input_ids'].device)
                
                if input_device and current_device != input_device:
                    self.logger.debug(f"ç§»å‹•è¼¸å…¥å¼µé‡ï¼š{current_device} â†’ {input_device}")
                    try:
                        inputs = {k: v.to(input_device) for k, v in inputs.items() if hasattr(v, 'to')}
                    except Exception as e:
                        self.logger.error(f"è¼¸å…¥å¼µé‡è¨­å‚™ç§»å‹•å¤±æ•—: {e}")
                        # å›é€€åˆ°CPUå†ç§»å‹•åˆ°ç›®æ¨™è¨­å‚™
                        inputs = {k: v.cpu().to(input_device) for k, v in inputs.items() if hasattr(v, 'to')}
            
            with torch.no_grad():
                # å¤šGPUç’°å¢ƒä¸‹çš„æ™ºèƒ½åŒæ­¥ç­–ç•¥
                if self.gpu_resource_manager.use_multi_gpu:
                    # åªåœ¨å¿…è¦æ™‚åŒæ­¥ï¼Œé¿å…ä¸å¿…è¦çš„å»¶é²
                    if len(self.gpu_resource_manager.gpu_manager.available_gpus) > 1:
                        # æª¢æŸ¥æ˜¯å¦éœ€è¦åŒæ­¥ï¼šæœ‰å¤šå€‹è¨­å‚™ä¸”æ¨¡å‹åˆ†ä½ˆåœ¨å¤šGPUä¸Š
                        if hasattr(self.model_loader.llm_model, 'hf_device_map') and len(set(self.model_loader.llm_model.hf_device_map.values())) > 1:
                            torch.cuda.synchronize()  # åƒ…åœ¨çœŸæ­£éœ€è¦æ™‚åŒæ­¥
                
                # å„ªåŒ–ï¼šä½¿ç”¨è¨­å‚™ç‰¹å®šçš„ç”Ÿæˆé…ç½®
                effective_config = generation_config.copy()
                
                # åœ¨å¤šGPUç’°å¢ƒä¸‹èª¿æ•´æ‰¹è™•ç†ç­–ç•¥
                if self.gpu_resource_manager.use_multi_gpu:
                    # æ¸›å°‘ä¸å¿…è¦çš„è¨ˆç®—é–‹éŠ·
                    effective_config['use_cache'] = True
                    effective_config['output_attentions'] = False
                    effective_config['output_hidden_states'] = False
                
                outputs = self.model_loader.llm_model.generate(
                    **inputs,
                    **effective_config
                )
                
                # æ™ºèƒ½åŒæ­¥ - åªåœ¨æœ‰è·¨è¨­å‚™æ“ä½œæ™‚åŒæ­¥
                if self.gpu_resource_manager.use_multi_gpu and hasattr(self.model_loader.llm_model, 'hf_device_map'):
                    if len(set(self.model_loader.llm_model.hf_device_map.values())) > 1:
                        torch.cuda.synchronize()
            
            # è§£ç¢¼å›æ‡‰
            response = self.model_loader.llm_tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            return response
            
        except torch.cuda.OutOfMemoryError as e:
            self.logger.error(f"GPUè¨˜æ†¶é«”ä¸è¶³: {e}")
            # å˜—è©¦æ¸…ç†è¨˜æ†¶é«”ä¸¦é‡è©¦
            self.gpu_resource_manager.gpu_manager.clear_gpu_memory()
            try:
                # é™ä½æ‰¹æ¬¡å¤§å°é‡è©¦
                with torch.no_grad():
                    outputs = self.model_loader.llm_model.generate(
                        **inputs,
                        max_new_tokens=min(generation_config.get('max_new_tokens', 150), 100),
                        **{k: v for k, v in generation_config.items() if k != 'max_new_tokens'}
                    )
                
                response = self.model_loader.llm_tokenizer.decode(
                    outputs[0][inputs['input_ids'].shape[1]:],
                    skip_special_tokens=True
                ).strip()
                
                self.logger.warning("âš ï¸  è¨˜æ†¶é«”ä¸è¶³ï¼Œå·²é™ä½ç”Ÿæˆé•·åº¦")
                return response
            except Exception as retry_error:
                self.logger.error(f"é‡è©¦ç”Ÿæˆå¤±æ•—: {retry_error}")
                return "æŠ±æ­‰ï¼Œè¨˜æ†¶é«”ä¸è¶³ï¼Œç„¡æ³•ç”Ÿæˆå›æ‡‰ã€‚"
        
        except Exception as e:
            self.logger.error(f"æ¨¡å‹ç”Ÿæˆå¤±æ•—: {e}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆéç¨‹ä¸­å‡ºç¾å•é¡Œã€‚"
    
    def _build_enhanced_prompt_with_history(
        self, 
        system_prompt: str,
        user_input: str, 
        context: Optional[str] = None,
        conversation_history: Optional[List[tuple]] = None,
        emotional_analysis: Optional[Dict[str, Any]] = None,
        response_hints: Optional[Dict[str, Any]] = None
    ) -> str:
        """æ§‹å»ºåŒ…å«å°è©±æ­·å²çš„å¢å¼·ç‰ˆæç¤ºè©"""
        prompt_parts = [system_prompt]
        
        # æ·»åŠ å°è©±æ­·å²ä¸Šä¸‹æ–‡
        if conversation_history and len(conversation_history) > 0:
            history_context = self._build_conversation_context(conversation_history)
            prompt_parts.append(f"\n<|conversation_history|>\n{history_context}\n<|end_history|>")
        
        # æ·»åŠ èªç¾©åˆ†æä¸Šä¸‹æ–‡
        if context:
            prompt_parts.append(f"\n<|context|>\n{context}\n<|end_context|>")
        
        # æ·»åŠ æƒ…æ„Ÿç†è§£ä¿¡æ¯ï¼ˆä½¿ç”¨äººæ€§åŒ–èªè¨€ï¼‰
        if emotional_analysis and emotional_analysis.get('emotional_intensity') != 'mild':
            emotion_guidance = emotional_analysis.get('response_guidance', '')
            intimacy_guidance = emotional_analysis.get('intimacy_guidance', '')
            intent_guidance = emotional_analysis.get('intent_guidance', '')
            
            human_guidance = []
            if emotion_guidance:
                human_guidance.append(f"æƒ…æ„Ÿç†è§£ï¼š{emotion_guidance}")
            if intimacy_guidance:
                human_guidance.append(f"äº’å‹•æ–¹å¼ï¼š{intimacy_guidance}")
            if intent_guidance:
                human_guidance.append(f"å›æ‡‰é‡é»ï¼š{intent_guidance}")
            
            if human_guidance:
                guidance_text = "ã€".join(human_guidance)
                prompt_parts.append(f"\n<|guidance|>{guidance_text}<|end_guidance|>")
        
        # æ·»åŠ å›æ‡‰é¢¨æ ¼æç¤ºï¼ˆäººæ€§åŒ–è¡¨é”ï¼‰
        if response_hints and response_hints.get('response_style') != 'normal':
            style_hint = ""
            if response_hints['response_style'] == 'intense':
                style_hint = "éœ²è¥¿äºç¾åœ¨éœ€è¦è¡¨ç¾å¾—ç‰¹åˆ¥æº«æŸ”é«”è²¼ï¼Œç”¨æœ€é—œå¿ƒçš„èªæ°£å›æ‡‰"
            elif response_hints['response_style'] == 'moderate':
                style_hint = "éœ²è¥¿äºè¦æ¯”å¹³å¸¸æ›´æº«æš–ä¸€äº›ï¼Œè¡¨ç¾å‡ºé—œå¿ƒå’Œç†è§£"
            
            if response_hints.get('special_phrases'):
                style_hint += f"ï¼Œå¯ä»¥ä½¿ç”¨é€™äº›è¡¨é”æ–¹å¼ï¼š{', '.join(response_hints['special_phrases'])}"
            
            if style_hint:
                prompt_parts.append(f"\n<|style|>{style_hint}<|end_style|>")
        
        prompt_parts.append(f"\n<|user|>{user_input}<|end|>")
        
        # ç²å–è§’è‰²åç¨± - å®Œå…¨å¾ core.json ç²å–
        character_name = "AIåŠ©æ‰‹"  # é»˜èªå›é€€åç¨±
        if hasattr(self.personality_core, 'core_data') and self.personality_core.core_data:
            character_name = self.personality_core.get_character_identity()['name'].get('zh', 'éœ²è¥¿å©­')
        
        prompt_parts.append(f"\n<|assistant|>{character_name}ï¼š")
        
        return "\n".join(prompt_parts)
    
    def _build_conversation_context(self, conversation_history: List[tuple]) -> str:
        """æ§‹å»ºå°è©±æ­·å²ä¸Šä¸‹æ–‡ï¼ˆä¿ç•™æœ€è¿‘7è¼ªï¼‰"""
        if not conversation_history:
            return ""
        
        # åªä½¿ç”¨æœ€è¿‘çš„7è¼ªå°è©±
        recent_history = conversation_history[-7:]
        
        context_parts = []
        for i, (user_msg, bot_response) in enumerate(recent_history, 1):
            # é™åˆ¶æ¯æ¢æ¶ˆæ¯çš„é•·åº¦ï¼Œé¿å…æç¤ºè©éé•·
            user_msg_short = user_msg[:100] + "..." if len(user_msg) > 100 else user_msg
            bot_response_short = bot_response[:150] + "..." if len(bot_response) > 150 else bot_response
            
            context_parts.append(f"ç¬¬{i}è¼ªå°è©±:")
            context_parts.append(f"ç”¨æˆ¶: {user_msg_short}")
            context_parts.append(f"éœ²è¥¿äº: {bot_response_short}")
            context_parts.append("")  # ç©ºè¡Œåˆ†éš”
        
        return "\n".join(context_parts).strip()
    
    async def _get_semantic_vector_enhanced_context(self, user_input: str, emotional_analysis: Dict[str, Any]) -> Optional[str]:
        """åŸºæ–¼èªç¾©å‘é‡å¢å¼·çš„æ™ºèƒ½æª¢ç´¢"""
        try:
            if not self._rag_system_ref:
                return None
            
            # ä½¿ç”¨æ–°çš„èªç¾©å‘é‡å¢å¼·æœç´¢
            search_results = await self._rag_system_ref.semantic_enhanced_search(
                user_input, 
                emotional_analysis, 
                top_k=8  # å¢åŠ æª¢ç´¢æ•¸é‡ä»¥ç²å¾—æ›´å¤šæ¨£æ€§
            )
            
            if not search_results:
                self.logger.info("ğŸ” èªç¾©å‘é‡æœç´¢æœªæ‰¾åˆ°çµæœï¼Œå˜—è©¦æ¨™æº–æœç´¢")
                # å›é€€åˆ°æ¨™æº–æœç´¢
                search_results = await self._rag_system_ref.search(user_input, top_k=5)
            
            if search_results:
                return self._build_diverse_context(search_results, emotional_analysis)
            
            return None
            
        except Exception as e:
            self.logger.error(f"èªç¾©å‘é‡å¢å¼·æª¢ç´¢å¤±æ•—: {e}")
            # å›é€€åˆ°æ¨™æº–æª¢ç´¢
            return await self._get_enhanced_context(user_input, emotional_analysis)

    async def _get_enhanced_context(self, user_input: str, emotional_analysis: Dict[str, Any]) -> Optional[str]:
        """æ ¹æ“šæƒ…ç·’åˆ†æç²å–å¢å¼·ç‰ˆä¸Šä¸‹æ–‡ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        try:
            # æ ¹æ“šæƒ…ç·’ç‹€æ…‹èª¿æ•´æª¢ç´¢ç­–ç•¥
            current_mood = self.personality_core.current_mood
            
            # ç¢ºå®šæª¢ç´¢é¡åˆ¥å„ªå…ˆç´š
            category_priority = self._determine_search_category(emotional_analysis, current_mood)
            
            # ä½¿ç”¨å¢å¼·ç‰ˆRAGç³»çµ±æª¢ç´¢
            if self._rag_system_ref:
                # ç›´æ¥èª¿ç”¨searchæ–¹æ³•ï¼Œä¿ç•™æœç´¢æ—¥èªŒ
                search_results = await self._rag_system_ref.search(
                    user_input, 
                    top_k=5, 
                    category_filter=category_priority
                )
                
                if search_results:
                    # æ‰‹å‹•æ§‹å»ºä¸Šä¸‹æ–‡
                    context_parts = []
                    for result in search_results:
                        content = result['content'].strip()
                        metadata = result['metadata']
                        
                        # æ§‹å»ºä¾†æºä¿¡æ¯
                        source_info = []
                        if metadata.get('category'):
                            source_info.append(f"é¡åˆ¥: {metadata['category']}")
                        if metadata.get('section_title'):
                            source_info.append(f"ç« ç¯€: {metadata['section_title']}")
                        if metadata.get('filename'):
                            source_info.append(f"æ–‡ä»¶: {metadata['filename']}")
                        
                        source_str = " | ".join(source_info) if source_info else "æœªçŸ¥ä¾†æº"
                        similarity = result['similarity']
                        
                        context_parts.append(f"[{source_str} | ç›¸é—œåº¦: {similarity:.3f}]\n{content}")
                    
                    return "\n\n---\n\n".join(context_parts)
                
                return None
            
            return None
            
        except Exception as e:
            self.logger.error(f"ç²å–å¢å¼·ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            return None
    
    def _determine_search_category(self, emotional_analysis: Dict[str, Any], current_mood: str) -> Optional[str]:
        """æ ¹æ“šæƒ…ç·’åˆ†æç¢ºå®šæœç´¢é¡åˆ¥å„ªå…ˆç´š"""
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æƒ…ç·’è§¸ç™¼
        if emotional_analysis.get('trigger_strength', 0) >= 2:
            emotional_category = emotional_analysis.get('emotional_category', 'neutral')
            
            if emotional_category == 'angry':
                # æ†¤æ€’æƒ…ç·’ - å„ªå…ˆæœç´¢è©å½™å’Œèº«ä»½ç›¸é—œ
                return 'vocabulary'
            elif emotional_category == 'happy':
                # é–‹å¿ƒæƒ…ç·’ - å„ªå…ˆæœç´¢äº’å‹•å’Œå…§å®¹ç›¸é—œ
                return 'relationships'
        
        # æ ¹æ“šç•¶å‰å¿ƒæƒ…ç¢ºå®šé¡åˆ¥
        if current_mood == 'gaming':
            return 'content'  # éŠæˆ²æ¨¡å¼å„ªå…ˆæœç´¢å…§å®¹ç›¸é—œ
        elif current_mood == 'embarrassed':
            return 'core_identity'  # æ•æ„Ÿæ¨¡å¼å„ªå…ˆæœç´¢èº«ä»½ç›¸é—œ
        elif current_mood == 'protective':
            return 'relationships'  # ä¿è­·æ¨¡å¼å„ªå…ˆæœç´¢é—œä¿‚ç›¸é—œ
        
        # é»˜èªä¸é™åˆ¶é¡åˆ¥
        return None
    
    def _get_emotional_search_priority(self, emotional_state: str) -> Dict[str, Any]:
        """æ ¹æ“šæƒ…æ„Ÿç‹€æ…‹ç¢ºå®šæª¢ç´¢å„ªå…ˆç´š"""
        priority_map = {
            'very_strong': {
                'top_k': 8,
                'category': 'content',
                'keywords': ['å¼·çƒˆè² é¢æƒ…ç·’', 'æº«æŸ”å®‰æ…°', 'å¿ƒç–¼é—œå¿ƒ']
            },
            'moderate': {
                'top_k': 6,
                'category': 'content', 
                'keywords': ['ä¸­ç­‰è² é¢æƒ…ç·’', 'æº«æŸ”å›æ‡‰', 'é—œå¿ƒç†è§£']
            },
            'mild': {
                'top_k': 5,
                'category': 'content',
                'keywords': ['å¹³éœæƒ…ç·’', 'è‡ªç„¶è¦ªåˆ‡', 'å¯æ„›é¢¨æ ¼']
            }
        }
        return priority_map.get(emotional_state, priority_map['mild'])
    
    def _get_intimacy_style_filter(self, intimacy_score: float) -> Dict[str, Any]:
        """æ ¹æ“šè¦ªå¯†åº¦ç²å–é¢¨æ ¼éæ¿¾å™¨"""
        if intimacy_score > 2.0:
            return {
                'style': 'very_intimate',
                'keywords': ['è¦ªæ„›çš„', 'å¯¶è²', 'æ„›ä½ ', 'æˆ€äºº', 'æ’’å¬Œ', 'ç”œèœœ']
            }
        elif intimacy_score > 1.0:
            return {
                'style': 'warm_close',
                'keywords': ['è¦ªè¿‘', 'æº«æš–', 'é»äºº', 'å¯æ„›', 'é—œå¿ƒ']
            }
        else:
            return {
                'style': 'friendly',
                'keywords': ['å‹å–„', 'è¦ªåˆ‡', 'ç¦®è²Œ', 'è‡ªç„¶']
            }
    
    def _build_enhanced_query(self, user_input: str, search_focus: str, emotional_state: str) -> str:
        """æ§‹å»ºå¢å¼·æª¢ç´¢æŸ¥è©¢"""
        
        # åŸºæ–¼æª¢ç´¢é‡é»æ·»åŠ é—œéµè©
        focus_keywords = {
            'companionship_responses': 'é™ªä¼´ ä¸€èµ· æº«æš–',
            'comfort_expressions': 'å®‰æ…° æº«æŸ” é—œå¿ƒ',
            'helpful_responses': 'å¹«åŠ© è§£ç­” æ¨‚æ„',
            'casual_interactions': 'èŠå¤© è¼•é¬† å¯æ„›',
            'intimate_responses': 'è¦ªå¯† æ’’å¬Œ ç”œèœœ',
            'general_responses': 'å›æ‡‰ äº’å‹•'
        }
        
        # åŸºæ–¼æƒ…æ„Ÿç‹€æ…‹æ·»åŠ ä¿®é£¾è©
        emotional_modifiers = {
            'very_strong': 'ç‰¹åˆ¥æº«æŸ” æ·±åº¦é—œæ‡·',
            'moderate': 'æº«æŸ” é—œå¿ƒ',
            'mild': 'è¦ªåˆ‡ è‡ªç„¶'
        }
        
        enhanced_query = f"{user_input} {focus_keywords.get(search_focus, '')} {emotional_modifiers.get(emotional_state, '')}"
        return enhanced_query.strip()
    
    def _filter_by_response_style(self, search_results: List, response_style_filter: Dict[str, Any]) -> List:
        """æ ¹æ“šå›æ‡‰é¢¨æ ¼éæ¿¾æª¢ç´¢çµæœ"""
        if not search_results:
            return []
        
        filtered_results = []
        style_keywords = response_style_filter.get('keywords', [])
        
        for result in search_results:
            content = result['content']
            style_score = 0
            
            # è¨ˆç®—é¢¨æ ¼åŒ¹é…åº¦
            for keyword in style_keywords:
                if keyword in content:
                    style_score += 1
            
            result['style_score'] = style_score / len(style_keywords) if style_keywords else 0
            filtered_results.append(result)
        
        # æŒ‰é¢¨æ ¼åŒ¹é…åº¦å’Œç›¸ä¼¼åº¦æ’åº
        filtered_results.sort(key=lambda x: (x['style_score'], x['similarity']), reverse=True)
        
        return filtered_results[:5]
    
    def _build_diverse_context(self, search_results: List, emotional_analysis: Dict) -> str:
        """æ§‹å»ºå¤šæ¨£åŒ–ä¸Šä¸‹æ–‡"""
        if not search_results:
            return ""
        
        context_parts = []
        
        # çµ±è¨ˆçµæœä¾†æº
        file_sources = {}
        for result in search_results:
            filename = result['metadata'].get('filename', 'unknown')
            if filename not in file_sources:
                file_sources[filename] = []
            file_sources[filename].append(result)
        
        # è¨˜éŒ„å¤šæ¨£æ€§ä¿¡æ¯
        self.logger.info(f"ğŸ¯ ä¸Šä¸‹æ–‡ä¾†æºå¤šæ¨£æ€§: {len(file_sources)} å€‹æ–‡ä»¶")
        for filename, results in file_sources.items():
            self.logger.info(f"   ğŸ“„ {filename}: {len(results)} å€‹ç‰‡æ®µ")
        
        # æ§‹å»ºåˆ†é¡ä¸Šä¸‹æ–‡
        for result in search_results[:6]:  # æœ€å¤š6å€‹çµæœ
            content = result['content'].strip()
            metadata = result['metadata']
            
            # æ§‹å»ºä¾†æºä¿¡æ¯
            source_info = []
            if metadata.get('category'):
                source_info.append(f"é¡åˆ¥: {metadata['category']}")
            if metadata.get('section_title'):
                source_info.append(f"ç« ç¯€: {metadata['section_title']}")
            if metadata.get('filename'):
                source_info.append(f"æ–‡ä»¶: {metadata['filename']}")
            
            source_str = " | ".join(source_info) if source_info else "æœªçŸ¥ä¾†æº"
            similarity = result['similarity']
            
            # æ¨™è¨˜æ˜¯å¦ç‚ºè£œå……çµæœ
            result_type = "è£œå……" if result.get('supplementary', False) else "ä¸»è¦"
            
            context_parts.append(f"[{result_type}] [{source_str} | ç›¸é—œåº¦: {similarity:.3f}]\n{content}")
        
        return "\n\n---\n\n".join(context_parts)
    
    def _get_adaptive_generation_config(self, emotional_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºæ–¼æƒ…æ„Ÿç†è§£å‹•æ…‹èª¿æ•´ç”Ÿæˆåƒæ•¸ - ä¿®å¾©ç‰ˆï¼Œç§»é™¤ç„¡æ•ˆåƒæ•¸"""
        
        base_config = {
            "max_new_tokens": self.config['vtuber']['response']['max_tokens'],
            "min_new_tokens": self.config['vtuber']['response'].get('min_tokens', 10),
            "temperature": self.config['models']['llm']['temperature'],
            "top_p": self.config['models']['llm']['top_p'],
            "top_k": self.config['models']['llm']['top_k'],
            "repetition_penalty": self.config['models']['llm'].get('repetition_penalty', 1.15),
            "no_repeat_ngram_size": self.config['models']['llm'].get('no_repeat_ngram_size', 3),
            "length_penalty": self.config['models']['llm'].get('length_penalty', 1.0),
            # ğŸ”¥ ç§»é™¤ early_stopping åƒæ•¸ - åœ¨æ–°ç‰ˆtransformersä¸­ç„¡æ•ˆ
            "do_sample": True,
        }
        
        # åªæœ‰åœ¨tokenizerå·²åˆå§‹åŒ–æ™‚æ‰æ·»åŠ tokenç›¸é—œé…ç½®
        if self.model_loader.llm_tokenizer is not None:
            base_config["pad_token_id"] = self.model_loader.llm_tokenizer.pad_token_id
            base_config["eos_token_id"] = self.model_loader.llm_tokenizer.eos_token_id
        
        # ğŸ”¥ æ‡‰ç”¨FP16å„ªåŒ– - ä½†ä¸è¨­ç½®max_lengthä»¥é¿å…è¡çª
        if hasattr(self.model_loader, 'get_optimized_generation_config'):
            base_config = self.model_loader.get_optimized_generation_config(base_config)
            self.logger.debug("âœ… å·²å¥—ç”¨FP16ç”Ÿæˆå„ªåŒ–")
        
        # æ ¹æ“šæƒ…æ„Ÿå¼·åº¦èª¿æ•´å‰µé€ æ€§
        emotional_intensity = emotional_analysis.get('emotional_intensity', 'mild')
        
        if emotional_intensity == 'very_strong':
            # å¼·çƒˆæƒ…æ„Ÿéœ€è¦æ›´æœ‰å‰µé€ æ€§å’Œè¡¨é”åŠ›çš„å›æ‡‰
            base_config["temperature"] = min(0.8, base_config["temperature"] + 0.2)
            base_config["top_p"] = min(0.9, base_config["top_p"] + 0.1)
            base_config["max_new_tokens"] = min(200, base_config["max_new_tokens"] + 50)
            
        elif emotional_intensity == 'moderate':
            # ä¸­ç­‰æƒ…æ„Ÿéœ€è¦é©åº¦çš„è¡¨é”åŠ›
            base_config["temperature"] = min(0.7, base_config["temperature"] + 0.1)
            base_config["max_new_tokens"] = min(180, base_config["max_new_tokens"] + 30)
        
        # æ ¹æ“šè¦ªå¯†åº¦èª¿æ•´å›æ‡‰é•·åº¦å’Œç´°è†©åº¦
        intimacy_score = emotional_analysis.get('intimacy_score', 0.0)
        
        if intimacy_score > 2.0:
            # é«˜è¦ªå¯†åº¦éœ€è¦æ›´é•·æ›´ç´°è†©çš„å›æ‡‰ï¼Œä½†æ§åˆ¶åœ¨100å­—å…§
            base_config["max_new_tokens"] = min(200, base_config["max_new_tokens"] + 30)
            base_config["min_new_tokens"] = max(25, base_config["min_new_tokens"] + 5)
            base_config["repetition_penalty"] = max(1.05, base_config["repetition_penalty"] - 0.1)
        
        # æ ¹æ“šæ„åœ–é¡å‹èª¿æ•´æº–ç¢ºæ€§
        detected_intent = emotional_analysis.get('detected_intent', '')
        
        if 'question' in detected_intent or detected_intent == 'asking_info':
            # å•é¡Œé¡éœ€è¦è©³ç´°ä½†æº–ç¢ºçš„å›æ‡‰
            base_config["temperature"] = max(0.5, base_config["temperature"] - 0.1)
            base_config["top_k"] = max(30, base_config["top_k"] - 10)
            base_config["max_new_tokens"] = min(200, base_config["max_new_tokens"] + 30)
            base_config["min_new_tokens"] = max(30, base_config["min_new_tokens"] + 20)
            base_config["repetition_penalty"] = max(1.05, base_config["repetition_penalty"] - 0.1)
            
        elif detected_intent == 'intimate_expression':
            # è¦ªå¯†è¡¨é”éœ€è¦æ›´æœ‰æƒ…æ„Ÿçš„å›æ‡‰
            base_config["temperature"] = min(0.75, base_config["temperature"] + 0.15)
            base_config["repetition_penalty"] = max(1.1, base_config["repetition_penalty"] - 0.05)
        
        # åŸºæ–¼å›æ‡‰æœŸæœ›çš„å‹•æ…‹èª¿æ•´
        response_expectation = emotional_analysis.get('response_expectation', 'normal')
        
        if response_expectation == 'detailed':
            # è©³ç´°å›æ‡‰ï¼šå•é¡Œã€å®‰æ…°ã€è¤‡é›œè©±é¡Œï¼Œä½†æ§åˆ¶åœ¨åˆç†ç¯„åœå…§
            base_config["max_new_tokens"] = min(200, base_config["max_new_tokens"] + 40)
            base_config["min_new_tokens"] = max(30, base_config["min_new_tokens"] + 10)
            base_config["temperature"] = min(0.75, base_config["temperature"] + 0.05)
            
        elif response_expectation == 'short':
            # ç°¡çŸ­å›æ‡‰ï¼šç¢ºèªã€ç°¡å–®å•å€™
            base_config["max_new_tokens"] = max(50, base_config["max_new_tokens"] - 50)
            base_config["min_new_tokens"] = max(10, base_config["min_new_tokens"] - 5)
        
        # ğŸ”¥ æ–°å¢ï¼šé…ç½®é©—è­‰å’Œæ¸…ç†
        base_config = self._validate_generation_config(base_config)
        
        return base_config
    
    def _validate_generation_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """é©—è­‰å’Œæ¸…ç†ç”Ÿæˆé…ç½®ï¼Œç§»é™¤ç„¡æ•ˆåƒæ•¸"""
        validated_config = config.copy()
        
        # ç§»é™¤å·²æ£„ç”¨æˆ–ç„¡æ•ˆçš„åƒæ•¸
        deprecated_params = [
            'early_stopping',  # å·²åœ¨æ–°ç‰ˆtransformersä¸­ç§»é™¤
            'forced_bos_token_id',  # å¾ˆå°‘ä½¿ç”¨ä¸”å¯èƒ½é€ æˆå•é¡Œ
            'forced_eos_token_id',  # å¾ˆå°‘ä½¿ç”¨ä¸”å¯èƒ½é€ æˆå•é¡Œ
        ]
        
        for param in deprecated_params:
            if param in validated_config:
                removed_value = validated_config.pop(param)
                self.logger.debug(f"ğŸ”§ ç§»é™¤ç„¡æ•ˆåƒæ•¸: {param}={removed_value}")
        
        # ç¢ºä¿token IDçš„æœ‰æ•ˆæ€§
        for token_param in ['pad_token_id', 'eos_token_id', 'bos_token_id']:
            if token_param in validated_config and validated_config[token_param] is None:
                validated_config.pop(token_param, None)
                self.logger.debug(f"ğŸ”§ ç§»é™¤ç©ºçš„tokenåƒæ•¸: {token_param}")
        
        # é©—è­‰æ•¸å€¼åƒæ•¸çš„åˆç†ç¯„åœ
        if 'temperature' in validated_config:
            validated_config['temperature'] = max(0.1, min(2.0, validated_config['temperature']))
        
        if 'top_p' in validated_config:
            validated_config['top_p'] = max(0.1, min(1.0, validated_config['top_p']))
        
        if 'repetition_penalty' in validated_config:
            validated_config['repetition_penalty'] = max(1.0, min(2.0, validated_config['repetition_penalty']))
        
        # ç¢ºä¿max_new_tokensåˆç†
        if 'max_new_tokens' in validated_config:
            validated_config['max_new_tokens'] = max(10, min(512, validated_config['max_new_tokens']))
        
        if 'min_new_tokens' in validated_config:
            validated_config['min_new_tokens'] = max(1, min(validated_config.get('max_new_tokens', 180), validated_config['min_new_tokens']))
        
        self.logger.debug(f"ğŸ” ç”Ÿæˆé…ç½®é©—è­‰å®Œæˆ: {len(validated_config)} å€‹åƒæ•¸")
        
        return validated_config
    
    def _create_context_hints(self, emotional_analysis: Dict[str, Any], conversation_history: Optional[List[tuple]] = None) -> Dict[str, Any]:
        """å‰µå»ºä¸Šä¸‹æ–‡æç¤ºï¼ˆç”¨æ–¼å‹•æ…‹ç³»çµ±æç¤ºè©ï¼‰"""
        context_hints = {}
        
        # 1. æƒ…æ„Ÿç‹€æ…‹æç¤º
        emotional_intensity = emotional_analysis.get('emotional_intensity', 'mild')
        if emotional_intensity == 'very_strong':
            context_hints['emotional_state'] = 'sad'  # éœ€è¦å®‰æ…°
        elif emotional_intensity == 'moderate':
            context_hints['emotional_state'] = 'excited'  # æœ‰æƒ…ç·’æ³¢å‹•
        else:
            context_hints['emotional_state'] = 'calm'  # å¹³éœ
        
        # 2. å°è©±ä¸»é¡Œé¡å‹æ¨æ¸¬
        detected_intent = emotional_analysis.get('detected_intent', 'unknown')
        if detected_intent in ['question_asking', 'asking_info']:
            context_hints['topic_type'] = 'casual'
        elif detected_intent == 'intimate_expression':
            context_hints['topic_type'] = 'emotional'
        elif detected_intent == 'companionship_request':
            context_hints['topic_type'] = 'daily_life'
        elif self.personality_core.current_mood == 'gaming':
            context_hints['topic_type'] = 'gaming'
        else:
            context_hints['topic_type'] = 'casual'
        
        # 3. å°è©±æ­·å²åˆ†æ
        if conversation_history and len(conversation_history) > 0:
            # åˆ†æå°è©±é•·åº¦å’Œé »ç‡
            if len(conversation_history) > 5:
                context_hints['conversation_depth'] = 'deep'
            elif len(conversation_history) > 2:
                context_hints['conversation_depth'] = 'moderate'
            else:
                context_hints['conversation_depth'] = 'new'
            
            # åˆ†ææœ€è¿‘çš„äº¤äº’æ¨¡å¼
            recent_user_messages = [msg[0] for msg in conversation_history[-3:]]
            total_length = sum(len(msg) for msg in recent_user_messages)
            avg_length = total_length / len(recent_user_messages) if recent_user_messages else 0
            
            if avg_length > 50:
                context_hints['interaction_style'] = 'detailed'
            elif avg_length > 20:
                context_hints['interaction_style'] = 'normal'
            else:
                context_hints['interaction_style'] = 'brief'
        
        # 4. æ™‚é–“ä¸Šä¸‹æ–‡
        current_hour = datetime.datetime.now().hour
        
        if 6 <= current_hour <= 11:
            context_hints['time_period'] = 'morning'
        elif 12 <= current_hour <= 17:
            context_hints['time_period'] = 'afternoon'
        elif 18 <= current_hour <= 22:
            context_hints['time_period'] = 'evening'
        else:
            context_hints['time_period'] = 'night'
        
        return context_hints
    
    def get_conversation_stats(self) -> Dict[str, Any]:
        """ç²å–å°è©±çµ±è¨ˆä¿¡æ¯"""
        return {
            'conversation_count': self.conversation_count,
            'current_mood': self.personality_core.current_mood,
            'intimacy_level': getattr(self.personality_core, 'intimacy_level', 0.0),
            'semantic_enabled': getattr(self.personality_core, '_semantic_enabled', False)
        }
    
    def reset_conversation_count(self):
        """é‡ç½®å°è©±è¨ˆæ•¸å™¨ï¼ˆç”¨æ–¼æ–°å°è©±é–‹å§‹ï¼‰"""
        self.conversation_count = 0
        self.logger.info("å°è©±è¨ˆæ•¸å™¨å·²é‡ç½®")
