# LLM Manager é‡æ§‹ç‰ˆ - ä½¿ç”¨æŒ‡å—

## ğŸ“ æ¨¡çµ„çµæ§‹

```
llm_manager/
â”œâ”€â”€ __init__.py              # æ¨¡çµ„å…¥å£å’Œå…¬é–‹æ¥å£
â”œâ”€â”€ llm_manager.py          # ä¸»è¦ç®¡ç†å™¨ - çµ±ä¸€æ¥å£
â”œâ”€â”€ gpu_manager.py          # GPUè³‡æºç®¡ç†æ¨¡çµ„
â”œâ”€â”€ model_loader.py         # æ¨¡å‹è¼‰å…¥å’Œåˆå§‹åŒ–æ¨¡çµ„
â”œâ”€â”€ response_generator.py   # å›æ‡‰ç”Ÿæˆå’Œè™•ç†æ¨¡çµ„
â””â”€â”€ README.md              # ä½¿ç”¨æŒ‡å—ï¼ˆæœ¬æ–‡ä»¶ï¼‰
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from llm_manager import create_llm_manager

# å‰µå»ºä¸¦åˆå§‹åŒ–LLMç®¡ç†å™¨
config = {...}  # ä½ çš„é…ç½®
manager = await create_llm_manager(config)

if manager:
    # ç”Ÿæˆå›æ‡‰
    response = await manager.generate_response("ä½ å¥½ï¼")
    print(response)
    
    # æ¸…ç†è³‡æº
    manager.cleanup()
```

### è©³ç´°ä½¿ç”¨

```python
from llm_manager import LLMManager

# æ‰‹å‹•å‰µå»ºç®¡ç†å™¨
manager = LLMManager(config)

# åˆå§‹åŒ–æ‰€æœ‰çµ„ä»¶
if await manager.initialize():
    
    # ç”Ÿæˆå¸¶ä¸Šä¸‹æ–‡çš„å›æ‡‰
    response = await manager.generate_response(
        prompt="ä»Šå¤©å¤©æ°£æ€éº¼æ¨£ï¼Ÿ",
        context="ä»Šå¤©æ˜¯æ™´å¤©ï¼Œæº«åº¦25åº¦",
        conversation_history=[
            ("æ—©ä¸Šå¥½ï¼", "æ—©ä¸Šå¥½ï¼ä»Šå¤©æ˜¯ç¾å¥½çš„ä¸€å¤©å‘¢ï½"),
            ("ä½ ä»Šå¤©å¿ƒæƒ…å¦‚ä½•ï¼Ÿ", "å¿ƒæƒ…å¾ˆå¥½å‘¢ï¼è¬è¬ä½ çš„é—œå¿ƒï½")
        ],
        rag_enabled=True
    )
    
    # ç²å–æ¨¡å‹ä¿¡æ¯
    info = manager.get_model_info()
    print(f"GPUä½¿ç”¨æƒ…æ³: {info['gpu_cluster']}")
    
    # ç²å–å°è©±çµ±è¨ˆ
    stats = manager.get_conversation_stats()
    print(f"å°è©±æ¬¡æ•¸: {stats['conversation_count']}")
    
    # æ¸…ç†è³‡æº
    manager.cleanup()
```

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½

### 1. GPUç®¡ç†

```python
# ç²å–GPUç‹€æ…‹
status = manager.get_gpu_status()

# å„ªåŒ–GPUè¨˜æ†¶é«”
manager.optimize_gpu_memory()

# è¨ºæ–·GPUåˆ†é…
diagnosis = manager.diagnose_gpu_allocation()
```

### 2. æ¨¡å‹ä¿¡æ¯

```python
# ç²å–å®Œæ•´æ¨¡å‹ä¿¡æ¯
info = manager.get_model_info()

# è¨ªå•å…·é«”çµ„ä»¶ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
llm_model = manager.llm_model
tokenizer = manager.llm_tokenizer
embedding = manager.embedding_model
```

### 3. åµŒå…¥å‘é‡

```python
# ç²å–æ–‡æœ¬åµŒå…¥
texts = ["ä½ å¥½", "å†è¦‹"]
embeddings = manager.get_embeddings(texts)
```

### 4. RAGç³»çµ±æ•´åˆ

```python
# è¨­ç½®RAGç³»çµ±å¼•ç”¨
manager.set_rag_system_reference(your_rag_system)

# ä½¿ç”¨RAGå¢å¼·çš„å›æ‡‰ç”Ÿæˆ
response = await manager.generate_response(
    "å‘Šè¨´æˆ‘é—œæ–¼éœ²è¥¿å©­çš„ä¿¡æ¯", 
    rag_enabled=True
)
```

## ğŸ—ï¸ æ¨¡çµ„åŒ–æ¶æ§‹å„ªå‹¢

### 1. **è·è²¬åˆ†é›¢**
- `gpu_manager.py`: å°ˆæ³¨GPUè³‡æºç®¡ç†
- `model_loader.py`: å°ˆæ³¨æ¨¡å‹è¼‰å…¥é‚è¼¯  
- `response_generator.py`: å°ˆæ³¨å›æ‡‰ç”Ÿæˆè™•ç†
- `llm_manager.py`: æä¾›çµ±ä¸€æ¥å£

### 2. **æ˜“æ–¼ç¶­è­·**
- æ¯å€‹æ¨¡çµ„åŠŸèƒ½å–®ä¸€æ˜ç¢º
- å•é¡Œå®šä½æ›´ç²¾æº–
- ä»£ç¢¼çµæ§‹æ¸…æ™°

### 3. **è‰¯å¥½æ“´å±•æ€§**
- å¯ä»¥ç¨ç«‹å„ªåŒ–å„å€‹æ¨¡çµ„
- å®¹æ˜“æ·»åŠ æ–°åŠŸèƒ½
- æ”¯æŒæ’ä»¶åŒ–æ¶æ§‹

### 4. **å‘å¾Œå…¼å®¹**
- ä¿æŒåŸæœ‰APIä¸è®Š
- ç¾æœ‰ä»£ç¢¼ç„¡éœ€ä¿®æ”¹
- å¹³æ»‘é·ç§»

## ğŸ“Š ç³»çµ±è¨ºæ–·å·¥å…·

```python
from llm_manager import get_system_info, diagnose_system

# ç²å–ç³»çµ±ä¿¡æ¯
sys_info = get_system_info()

# ç³»çµ±è¨ºæ–·
diagnosis = diagnose_system()
print("æª¢æ¸¬åˆ°çš„å•é¡Œ:", diagnosis["issues"])
print("å»ºè­°:", diagnosis["recommendations"])
```

## âš¡ æ€§èƒ½å„ªåŒ–

### å¤šGPUé…ç½®
- LLMä¸»æ¨¡å‹ï¼šè‡ªå‹•ä½¿ç”¨å‰4å¼µGPUå¡
- åµŒå…¥æ¨¡å‹ï¼šä½¿ç”¨ç¬¬5å¼µGPUæˆ–æœ€å¾Œä¸€å¼µ
- æ™ºèƒ½è¨˜æ†¶é«”åˆ†é…å’Œè² è¼‰å‡è¡¡

### é‡åŒ–å„ªåŒ–
- LLMæ¨¡å‹ï¼š4bité‡åŒ– (BitsAndBytesConfig)
- åµŒå…¥æ¨¡å‹ï¼š8bité‡åŒ–
- è‡ªå‹•å›é€€åˆ°æ¨™æº–æ¨¡å¼

### è¨˜æ†¶é«”ç®¡ç†
- æ™ºèƒ½è¨˜æ†¶é«”æ¸…ç†
- OOMè‡ªå‹•é‡è©¦æ©Ÿåˆ¶
- å‹•æ…‹æ‰¹æ¬¡å¤§å°èª¿æ•´

## ğŸ” èª¿è©¦å’Œç›£æ§

### GPUç‹€æ…‹ç›£æ§
```python
# å³æ™‚GPUç‹€æ…‹
status = manager.get_gpu_status()
print(f"GPUæ•¸é‡: {status['gpu_count']}")
print(f"ç¸½è¨˜æ†¶é«”: {status['total_memory_gb']:.1f}GB")
print(f"å¯ç”¨è¨˜æ†¶é«”: {status['available_memory_gb']:.1f}GB")
```

### å°è©±çµ±è¨ˆ
```python
# å°è©±çµ±è¨ˆä¿¡æ¯
stats = manager.get_conversation_stats()
print(f"å°è©±æ¬¡æ•¸: {stats['conversation_count']}")
print(f"ç•¶å‰å¿ƒæƒ…: {stats['current_mood']}")
print(f"è¦ªå¯†åº¦: {stats['intimacy_level']}")
```

## ğŸš¨ éŒ¯èª¤è™•ç†

### åˆå§‹åŒ–å¤±æ•—è™•ç†
```python
manager = await create_llm_manager(config)
if manager is None:
    # æª¢æŸ¥ç³»çµ±è¨ºæ–·
    diagnosis = diagnose_system()
    print("åˆå§‹åŒ–å¤±æ•—ï¼Œå¯èƒ½çš„åŸå› :", diagnosis["issues"])
```

### è¨˜æ†¶é«”ä¸è¶³è™•ç†
```python
try:
    response = await manager.generate_response(prompt)
except torch.cuda.OutOfMemoryError:
    # è‡ªå‹•æ¸…ç†è¨˜æ†¶é«”ä¸¦é‡è©¦
    manager.optimize_gpu_memory()
    response = await manager.generate_response(prompt)
```

## ğŸ“ é…ç½®æ–‡ä»¶ç¤ºä¾‹

```yaml
models:
  llm:
    model_path: "/path/to/Qwen-8B"
    device: "cuda"
    max_length: 4096
    temperature: 0.7
    top_p: 0.9
    top_k: 50
  
  embedding:
    model_path: "/path/to/Qwen3-Embedding-0.6B" 
    max_length: 512
    batch_size: 32

vtuber:
  response:
    max_tokens: 150
    min_tokens: 10
    filter_repetition: true
    enable_traditional_chinese: true
```

## ğŸ”„ é·ç§»æŒ‡å—

### å¾èˆŠç‰ˆæœ¬é·ç§»

```python
# èˆŠä»£ç¢¼
from src.llm_manager import LLMManager

# æ–°ä»£ç¢¼ - ç„¡éœ€ä¿®æ”¹ï¼
from llm_manager import LLMManager
# æ‰€æœ‰ç¾æœ‰APIä¿æŒä¸è®Š
```

### æ–°åŠŸèƒ½ä½¿ç”¨

```python
# ä½¿ç”¨æ–°çš„å·¥å» å‡½æ•¸
manager = await create_llm_manager(config)

# ä½¿ç”¨æ–°çš„è¨ºæ–·å·¥å…·
diagnosis = diagnose_system()

# ä½¿ç”¨æ¨¡çµ„ä¿¡æ¯
from llm_manager import get_module_info
info = get_module_info()
```
