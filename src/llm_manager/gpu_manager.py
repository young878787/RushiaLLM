"""
GPU è³‡æºç®¡ç†æ¨¡çµ„
è² è²¬å¤šGPUæ™ºèƒ½åˆ†é…ã€è¨˜æ†¶é«”ç®¡ç†å’Œè¨­å‚™å„ªåŒ–
"""

import logging
import torch
import numpy as np
import datetime
import gc
import os
from typing import Optional, List, Dict, Any


class MultiGPUManager:
    """å¤šGPUç®¡ç†å™¨ - æ™ºèƒ½åˆ†é…å’Œç®¡ç†å¤šå¼µGPUå¡"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.available_gpus = []
        self.gpu_info = {}
        self.device_map = {}
        
        # ğŸ”¥ æ–°å¢ï¼šæ—¥èªŒæ§åˆ¶æ©Ÿåˆ¶
        self._status_logged = False  # è¿½è¹¤æ˜¯å¦å·²ç¶“é¡¯ç¤ºéç‹€æ…‹
        self._last_log_time = None   # æœ€å¾Œä¸€æ¬¡è¨˜éŒ„æ™‚é–“
        self._log_interval = 300     # æœ€å°é–“éš”æ™‚é–“ï¼ˆç§’ï¼‰
        
        self._initialize_gpus()
    
    def _initialize_gpus(self):
        """åˆå§‹åŒ–GPUä¿¡æ¯"""
        if not torch.cuda.is_available():
            self.logger.warning("CUDA ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ CPU")
            return
        
        gpu_count = torch.cuda.device_count()
        self.logger.info(f"æª¢æ¸¬åˆ° {gpu_count} å¼µGPUå¡")
        
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            gpu_info = {
                'id': i,
                'name': props.name,
                'total_memory': props.total_memory,
                'total_memory_gb': props.total_memory / (1024**3),
                'available': True,
                'allocated_memory': 0,
                'reserved_memory': 0
            }
            
            self.available_gpus.append(i)
            self.gpu_info[i] = gpu_info
            
            self.logger.info(f"GPU {i}: {props.name} - {gpu_info['total_memory_gb']:.1f}GB")
    
    def get_optimal_device_map(self, model_type: str = "llm") -> Dict[str, Any]:
        """ç²å–æœ€ä½³çš„è¨­å‚™æ˜ å°„ç­–ç•¥ - æ··åˆç²¾åº¦å„ªåŒ–ç‰ˆ"""
        if not self.available_gpus:
            return None
        
        if model_type == "llm":
            # ğŸ”¥ LLMä¸»æ¨¡å‹ï¼š8bité‡åŒ–ï¼Œçµ±ä¸€ä½¿ç”¨4å¼µå¡é…ç½®
            if len(self.available_gpus) >= 4:
                primary_gpus = self.available_gpus[:4]
                
                self.logger.info(f"ğŸš€ LLM 8bité‡åŒ–æ¨¡å‹å°‡ä½¿ç”¨GPU {primary_gpus} (Autoåˆ†é…æ¨¡å¼)")
                
                # ğŸ”§ çµ±ä¸€è¨˜æ†¶é«”åˆ†é…ç­–ç•¥ï¼š4å¼µå¡ï¼Œæ¯å¼µ4GB
                return {
                    "auto_mode": True,
                    "allowed_devices": primary_gpus,
                    "max_memory": {i: "4GB" for i in primary_gpus},
                    "device_map_strategy": "balanced"
                }
            else:
                # GPUä¸è¶³4å¼µï¼Œä½¿ç”¨æ¨™æº–autoï¼Œ8bité‡åŒ–è¨˜æ†¶é«”æ•ˆç‡é«˜
                self.logger.info("ğŸ”§ GPUæ•¸é‡ä¸è¶³4å¼µï¼Œ8bité‡åŒ–æ¨¡å‹è¨˜æ†¶é«”æ•ˆç‡é«˜")
                self.logger.info("ğŸ’¡ 8bité‡åŒ–å¯ä»¥åœ¨æ›´å°‘GPUä¸Šé‹è¡Œ")
                return "auto"
        
        elif model_type == "embedding":
            # ğŸ”¥ åµŒå…¥æ¨¡å‹ï¼šFP16ç‰ˆæœ¬ï¼Œä½¿ç”¨ç¬¬5å¼µå¡ï¼ˆåŸºæ–¼4å¼µLLMå¡ä¹‹å¾Œï¼‰
            if len(self.available_gpus) >= 5:
                embedding_gpu = self.available_gpus[4]  # ä½¿ç”¨ç¬¬5å¼µå¡
                self.logger.info(f"ğŸ¯ Embedding FP16æ¨¡å‹å°‡ä½¿ç”¨GPU {embedding_gpu}")
                return embedding_gpu
            elif len(self.available_gpus) > 1:
                # å¦‚æœæœ‰å¤šå¼µå¡ä½†ä¸è¶³5å¼µï¼Œä½¿ç”¨æœ€å¾Œä¸€å¼µ
                embedding_gpu = self.available_gpus[-1]
                self.logger.info(f"ğŸ¯ Embedding FP16æ¨¡å‹å°‡ä½¿ç”¨GPU {embedding_gpu} (æœ€å¾Œä¸€å¼µå¡)")
                return embedding_gpu
            else:
                # åªæœ‰1å¼µå¡ï¼Œå…±äº«ä½¿ç”¨ï¼Œ8bit+FP16æ··åˆé…ç½®ä¸‹è¨˜æ†¶é«”å£“åŠ›å°
                self.logger.info("ğŸ”§ åªæœ‰1å¼µGPUï¼Œ8bit LLM + FP16 Embeddingå…±äº«ä½¿ç”¨")
                self.logger.info("ğŸ’¡ 8bité‡åŒ–é™ä½è¨˜æ†¶é«”å£“åŠ›ï¼Œå…±äº«ä½¿ç”¨å¯è¡Œ")
                return 0
        
        return "auto"
    
    def get_memory_allocation_config(self, model_type: str = "llm") -> Dict[str, Any]:
        """çµ±ä¸€çš„è¨˜æ†¶é«”åˆ†é…é…ç½®æ–¹æ³•"""
        device_map_result = self.get_optimal_device_map(model_type)
        
        if model_type == "llm":
            if isinstance(device_map_result, dict) and device_map_result.get("auto_mode"):
                # è¿”å›LLMæ¨¡å‹çš„å®Œæ•´é…ç½®
                return {
                    "use_device_map": True,
                    "device_map": "auto",
                    "max_memory": device_map_result["max_memory"],
                    "allowed_devices": device_map_result["allowed_devices"],
                    "memory_per_gpu": "4GB",
                    "total_gpus": len(device_map_result["allowed_devices"])
                }
            else:
                # æ¨™æº–autoé…ç½®
                return {
                    "use_device_map": True,
                    "device_map": "auto",
                    "max_memory": None,
                    "allowed_devices": self.available_gpus,
                    "memory_per_gpu": "auto",
                    "total_gpus": len(self.available_gpus)
                }
        
        elif model_type == "embedding":
            if isinstance(device_map_result, int):
                # æŒ‡å®šGPU
                return {
                    "use_device_map": True,
                    "device_map": {"": device_map_result},
                    "target_gpu": device_map_result,
                    "device": f"cuda:{device_map_result}"
                }
            else:
                # å›é€€é…ç½®
                return {
                    "use_device_map": True,
                    "device_map": "auto",
                    "target_gpu": None,
                    "device": "auto"
                }
        
        return {}
    
    def log_memory_allocation_info(self, model_type: str = "llm"):
        """è¨˜éŒ„è¨˜æ†¶é«”åˆ†é…ä¿¡æ¯"""
        config = self.get_memory_allocation_config(model_type)
        
        if model_type == "llm":
            if config.get("max_memory"):
                self.logger.info("ğŸ“Š LLMè¨˜æ†¶é«”åˆ†é…ç­–ç•¥:")
                self.logger.info(f"   ä½¿ç”¨GPU: {config['allowed_devices']}")
                self.logger.info(f"   æ¯å¼µå¡è¨˜æ†¶é«”: {config['memory_per_gpu']}")
                self.logger.info(f"   ç¸½è¨˜æ†¶é«”åˆ†é…: {config['total_gpus']} Ã— {config['memory_per_gpu']} = {float(config['memory_per_gpu'].rstrip('GB')) * config['total_gpus']}GB")
            else:
                self.logger.info("ğŸ“Š LLMä½¿ç”¨æ¨™æº–Autoåˆ†é…ç­–ç•¥")
        
        elif model_type == "embedding":
            if config.get("target_gpu") is not None:
                self.logger.info(f"ğŸ“Š Embeddingåˆ†é…åˆ°GPU {config['target_gpu']}")
            else:
                self.logger.info("ğŸ“Š Embeddingä½¿ç”¨Autoåˆ†é…ç­–ç•¥")
    
    def get_gpu_memory_info(self) -> Dict[int, Dict[str, float]]:
        """ç²å–æ‰€æœ‰GPUçš„è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
        memory_info = {}
        
        for gpu_id in self.available_gpus:
            torch.cuda.set_device(gpu_id)
            allocated = torch.cuda.memory_allocated(gpu_id) / (1024**3)
            reserved = torch.cuda.memory_reserved(gpu_id) / (1024**3)
            total = self.gpu_info[gpu_id]['total_memory_gb']
            
            memory_info[gpu_id] = {
                'allocated_gb': allocated,
                'reserved_gb': reserved,
                'total_gb': total,
                'free_gb': total - reserved,
                'utilization': (reserved / total) * 100
            }
            
            self.gpu_info[gpu_id]['allocated_memory'] = allocated
            self.gpu_info[gpu_id]['reserved_memory'] = reserved
        
        return memory_info
    
    def clear_gpu_memory(self, gpu_ids: Optional[List[int]] = None):
        """æ¸…ç†æŒ‡å®šGPUçš„è¨˜æ†¶é«”"""
        if gpu_ids is None:
            gpu_ids = self.available_gpus
        
        for gpu_id in gpu_ids:
            if gpu_id in self.available_gpus:
                torch.cuda.set_device(gpu_id)
                torch.cuda.empty_cache()
        
        gc.collect()
        self.logger.info(f"å·²æ¸…ç†GPU {gpu_ids} çš„è¨˜æ†¶é«”")
    
    def get_total_gpu_memory(self) -> float:
        """ç²å–ç¸½GPUè¨˜æ†¶é«”"""
        return sum(info['total_memory_gb'] for info in self.gpu_info.values())
    
    def get_available_memory(self) -> float:
        """ç²å–å¯ç”¨GPUè¨˜æ†¶é«”"""
        memory_info = self.get_gpu_memory_info()
        return sum(info['free_gb'] for info in memory_info.values())
    
    def diagnose_gpu_allocation(self, llm_model=None) -> Dict[str, Any]:
        """è¨ºæ–·GPUåˆ†é…ç‹€æ³ - å°ˆé–€ç”¨æ–¼æ’æŸ¥è¨­å‚™ä¸åŒæ­¥å•é¡Œ"""
        diagnosis = {
            "timestamp": str(datetime.datetime.now()),
            "device_allocation": {},
            "tensor_devices": {},
            "model_distribution": {},
            "potential_issues": []
        }
        
        try:
            # 1. æª¢æŸ¥æ¨¡å‹è¨­å‚™åˆ†ä½ˆ
            if llm_model and hasattr(llm_model, 'hf_device_map'):
                device_distribution = {}
                for module_name, device in llm_model.hf_device_map.items():
                    device_str = f"cuda:{device}" if isinstance(device, int) else str(device)
                    if device_str not in device_distribution:
                        device_distribution[device_str] = []
                    device_distribution[device_str].append(module_name)
                
                diagnosis["model_distribution"] = device_distribution
                
                # æª¢æŸ¥æ˜¯å¦æœ‰è¨­å‚™åˆ†ä½ˆä¸å‡
                device_counts = {device: len(modules) for device, modules in device_distribution.items()}
                if len(device_counts) > 1:
                    max_modules = max(device_counts.values())
                    min_modules = min(device_counts.values())
                    if max_modules - min_modules > 5:  # æ¨¡çµ„æ•¸é‡å·®ç•°éå¤§
                        diagnosis["potential_issues"].append(
                            f"è¨­å‚™åˆ†ä½ˆä¸å‡ï¼š{device_counts}"
                        )
            
            # 2. æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨
            memory_info = self.get_gpu_memory_info()
            diagnosis["memory_usage"] = memory_info
            
            # æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨ä¸å‡
            if len(memory_info) > 1:
                utilizations = [info['utilization'] for info in memory_info.values()]
                max_util = max(utilizations)
                min_util = min(utilizations)
                if max_util - min_util > 30:  # ä½¿ç”¨ç‡å·®ç•°è¶…é30%
                    diagnosis["potential_issues"].append(
                        f"è¨˜æ†¶é«”ä½¿ç”¨ä¸å‡ï¼šæœ€é«˜{max_util:.1f}% æœ€ä½{min_util:.1f}%"
                    )
            
            # 3. æª¢æŸ¥è¨­å‚™å¯è¦‹æ€§
            diagnosis["cuda_visible_devices"] = os.environ.get("CUDA_VISIBLE_DEVICES", "æœªè¨­å®š")
            diagnosis["available_gpus"] = self.available_gpus
            
            return diagnosis
            
        except Exception as e:
            diagnosis["error"] = str(e)
            return diagnosis
    
    def log_gpu_status(self, force: bool = False, reason: str = ""):
        """è¨˜éŒ„GPUç‹€æ…‹ - å„ªåŒ–ç‰ˆï¼Œé¿å…é‡è¤‡é¡¯ç¤º"""
        import time
        
        current_time = time.time()
        
        # ğŸ”¥ æ§åˆ¶æ—¥èªŒé¡¯ç¤ºé »ç‡
        if not force:
            # å¦‚æœå·²ç¶“é¡¯ç¤ºéä¸”æœªè¶…éé–“éš”æ™‚é–“ï¼Œå‰‡è·³é
            if (self._status_logged and 
                self._last_log_time and 
                current_time - self._last_log_time < self._log_interval):
                return
        
        memory_info = self.get_gpu_memory_info()
        total_memory = self.get_total_gpu_memory()
        available_memory = self.get_available_memory()
        
        # ğŸ”¥ ç°¡åŒ–çš„ç‹€æ…‹å ±å‘Š
        if not self._status_logged:
            # ç¬¬ä¸€æ¬¡é¡¯ç¤ºæ™‚ï¼Œé¡¯ç¤ºå®Œæ•´ä¿¡æ¯
            self.logger.info("=== GPU ç‹€æ…‹å ±å‘Š ===")
            self.logger.info(f"ç¸½GPUè¨˜æ†¶é«”: {total_memory:.1f}GB")
            self.logger.info(f"å¯ç”¨è¨˜æ†¶é«”: {available_memory:.1f}GB")
            
            for gpu_id, info in memory_info.items():
                self.logger.info(f"GPU {gpu_id}: {info['utilization']:.1f}% ä½¿ç”¨ç‡ "
                               f"({info['reserved_gb']:.1f}GB/{info['total_gb']:.1f}GB)")
            self.logger.info("===================")
            self._status_logged = True
        else:
            # å¾ŒçºŒé¡¯ç¤ºæ™‚ï¼Œåªé¡¯ç¤ºç°¡åŒ–ä¿¡æ¯
            if reason:
                self.logger.info(f"ğŸ”„ GPUç‹€æ…‹æ›´æ–° ({reason}): å¯ç”¨ {available_memory:.1f}GB/{total_memory:.1f}GB")
            else:
                self.logger.info(f"ğŸ”„ GPUè¨˜æ†¶é«”: å¯ç”¨ {available_memory:.1f}GB/{total_memory:.1f}GB")
        
        self._last_log_time = current_time


class OptimizedEmbeddingModel:
    """å„ªåŒ–çš„åµŒå…¥æ¨¡å‹åŒ…è£å™¨ï¼Œæ”¯æŒ8bité‡åŒ–å’Œå¤šGPU"""
    
    def __init__(self, model, tokenizer, device, max_length=512, batch_size=32, gpu_id=None):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.gpu_id = gpu_id
        self.max_length = max_length
        self.batch_size = batch_size
        
    def encode(self, texts, batch_size=None, convert_to_tensor=True, device=None, **kwargs):
        """ç·¨ç¢¼æ–‡æœ¬ç‚ºåµŒå…¥å‘é‡ - æ”¯æŒå¤šGPU"""
        if isinstance(texts, str):
            texts = [texts]
        
        if batch_size is None:
            batch_size = self.batch_size
        
        # å¦‚æœæŒ‡å®šäº†GPU IDï¼Œè¨­ç½®ç•¶å‰è¨­å‚™
        if self.gpu_id is not None and torch.cuda.is_available():
            torch.cuda.set_device(self.gpu_id)
        
        all_embeddings = []
        
        # åˆ†æ‰¹è™•ç†
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            # Tokenize
            inputs = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt"
            ).to(self.device)
            
            # ç²å–åµŒå…¥
            with torch.no_grad():
                outputs = self.model(**inputs)
                
                # ä½¿ç”¨ [CLS] token æˆ–å¹³å‡æ± åŒ–
                if hasattr(outputs, 'last_hidden_state'):
                    # å¹³å‡æ± åŒ–
                    attention_mask = inputs['attention_mask']
                    token_embeddings = outputs.last_hidden_state
                    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
                    embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
                else:
                    # ä½¿ç”¨ pooler_output å¦‚æœå¯ç”¨
                    embeddings = outputs.pooler_output if hasattr(outputs, 'pooler_output') else outputs[0][:, 0]
                
                # æ­£è¦åŒ–
                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
                
                all_embeddings.append(embeddings)
        
        # åˆä½µæ‰€æœ‰æ‰¹æ¬¡
        final_embeddings = torch.cat(all_embeddings, dim=0)
        
        if convert_to_tensor:
            return final_embeddings
        else:
            return final_embeddings.cpu().numpy()


class GPUResourceManager:
    """GPUè³‡æºç®¡ç†å™¨ - çµ±ä¸€ç®¡ç†GPUç›¸é—œåŠŸèƒ½"""
    
    def __init__(self, config: dict):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.gpu_manager = MultiGPUManager()
        self.use_multi_gpu = False
        self.device = self._setup_device_strategy()
        
        # è¨˜éŒ„GPUä½¿ç”¨æƒ…æ³ï¼ˆåƒ…åœ¨åˆå§‹åŒ–æ™‚é¡¯ç¤ºä¸€æ¬¡ï¼‰
        if torch.cuda.is_available():
            self.gpu_manager.log_gpu_status(force=True, reason="åˆå§‹åŒ–")
    
    def _setup_device_strategy(self) -> str:
        """è¨­ç½®è¨­å‚™ç­–ç•¥"""
        config_device = self.config['models']['llm']['device']
        
        if config_device == "cuda" and torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            if gpu_count > 1:
                self.logger.info(f"âœ… å¤šGPUæ¨¡å¼å•Ÿç”¨ï¼Œæª¢æ¸¬åˆ° {gpu_count} å¼µGPU")
                self.use_multi_gpu = True
                return "cuda"  # è¿”å›cudaï¼Œå…·é«”åˆ†é…ç”±device_mapè™•ç†
            else:
                self.logger.info("âœ… å–®GPUæ¨¡å¼å•Ÿç”¨")
                self.use_multi_gpu = False
                return "cuda:0"
        elif config_device == "cuda" and not torch.cuda.is_available():
            self.logger.warning("CUDA ä¸å¯ç”¨ï¼Œåˆ‡æ›åˆ° CPU")
            self.use_multi_gpu = False
            return "cpu"
        else:
            self.use_multi_gpu = False
            return config_device
    
    def get_gpu_status(self) -> Dict[str, Any]:
        """ç²å–å³æ™‚GPUç‹€æ…‹"""
        if not torch.cuda.is_available():
            return {"error": "CUDAä¸å¯ç”¨"}
        
        return {
            "gpu_count": len(self.gpu_manager.available_gpus),
            "memory_info": self.gpu_manager.get_gpu_memory_info(),
            "total_memory_gb": self.gpu_manager.get_total_gpu_memory(),
            "available_memory_gb": self.gpu_manager.get_available_memory(),
            "multi_gpu_enabled": self.use_multi_gpu
        }
    
    def optimize_gpu_memory(self):
        """å„ªåŒ–GPUè¨˜æ†¶é«”ä½¿ç”¨"""
        if not torch.cuda.is_available():
            return
        
        self.logger.info("é–‹å§‹GPUè¨˜æ†¶é«”å„ªåŒ–...")
        
        # æ¸…ç†æœªä½¿ç”¨çš„è¨˜æ†¶é«”
        self.gpu_manager.clear_gpu_memory()
        
        # è¨˜éŒ„å„ªåŒ–çµæœ
        memory_info = self.gpu_manager.get_gpu_memory_info()
        total_freed = 0
        
        for gpu_id, info in memory_info.items():
            freed = info['total_gb'] - info['reserved_gb']
            total_freed += freed
            self.logger.info(f"GPU {gpu_id}: é‡‹æ”¾ {freed:.1f}GB è¨˜æ†¶é«”")
        
        self.logger.info(f"âœ… è¨˜æ†¶é«”å„ªåŒ–å®Œæˆï¼Œç¸½å…±é‡‹æ”¾ {total_freed:.1f}GB")
    
    def cleanup_gpu_resources(self):
        """æ¸…ç†GPUè³‡æº"""
        self.logger.info("é–‹å§‹æ¸…ç†GPUè³‡æº...")
        
        # è¨˜éŒ„æ¸…ç†å‰çš„è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³ï¼ˆç°¡åŒ–é¡¯ç¤ºï¼‰
        if torch.cuda.is_available():
            self.logger.info("ğŸ§¹ é–‹å§‹æ¸…ç†GPUè¨˜æ†¶é«”...")
            # ä¸é¡¯ç¤ºè©³ç´°ç‹€æ…‹ï¼Œé¿å…æ—¥èªŒå†—é¤˜
        
        # å¤šGPUç’°å¢ƒä¸‹çš„æ·±åº¦æ¸…ç†
        if torch.cuda.is_available():
            if self.use_multi_gpu:
                # æ¸…ç†æ‰€æœ‰GPUçš„è¨˜æ†¶é«”
                self.gpu_manager.clear_gpu_memory(self.gpu_manager.available_gpus)
                
                # é‡ç½®CUDAç’°å¢ƒ
                for gpu_id in self.gpu_manager.available_gpus:
                    torch.cuda.set_device(gpu_id)
                    torch.cuda.empty_cache()
                    torch.cuda.ipc_collect()
            else:
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
        
        # Pythonåƒåœ¾å›æ”¶
        gc.collect()
        
        # è¨˜éŒ„æ¸…ç†å¾Œçš„è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³ï¼ˆåƒ…åœ¨å¿…è¦æ™‚é¡¯ç¤ºï¼‰
        if torch.cuda.is_available():
            self.gpu_manager.log_gpu_status(reason="è¨˜æ†¶é«”æ¸…ç†å®Œæˆ")
        
        success_msg = "âœ… GPUè³‡æºå·²æ¸…ç†"
        if self.use_multi_gpu:
            success_msg += f" (å·²æ¸…ç† {len(self.gpu_manager.available_gpus)} å¼µGPU)"
        self.logger.info(success_msg)
